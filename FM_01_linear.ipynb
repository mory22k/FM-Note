{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gibbs Sampling for Factorization Machine\n",
    "\n",
    "This notebook series demonstrate how to implement the Alternating Least Squares (ALS) and Gibbs Sampler for the Factorization Machine (FM) using Python and Julia. FM is a generalization of matrix factorization and is used in recommendation systems. ALS is an optimization algorithm for FM, which is used to minimize the loss function of FM. The implementation is based on the paper by Rendle et al. [1, 2].\n",
    "\n",
    "- Written by Keisuke Morita (GSIS Tohoku Univ.)\n",
    "\n",
    "- References\n",
    "  1. S. Rendle, Z. Gantner, C. Freudenthaler, and L. Schmidt-Thieme, Fast Context-Aware Recommendations with Factorization Machines, in Proceedings of the 34th International ACM SIGIR Conference on Research and Development in Information Retrieval (Association for Computing Machinery, New York, NY, USA, 2011), pp. 635â€“644.\n",
    "  2. S. Rendle, Factorization Machines with libFM, ACM Trans. Intell. Syst. Technol. 3, 57:1 (2012)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01. Factorization Machine as a Linear Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import Optional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Factorization Machine\n",
    "\n",
    "A factorization machine (FM) is given by\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\n",
    "f(x; b, w, v)\n",
    "&= b + \\sum_{i=1}^N w_i x_i + \\sum_{i \\lt j} \\sum_{k=1}^K v_{ik} v_{jk} x_i x_j \\\\\n",
    "\n",
    "&= b + \\sum_{i=1}^N w_i x_i + \\frac{1}{2} \\sum_{k=1}^K \\left( \\sum_{i=1}^N \\sum_{j=1}^N v_{ik} v_{jk} x_i x_j - \\sum_{i=1}^N v_{ik}^2 x_i^2 \\right).\n",
    "\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FactorizationMachines:\n",
    "    def __init__(self,\n",
    "        num_features: int,\n",
    "        num_factors:  int,\n",
    "        sigma_b_init: float=0.,\n",
    "        sigma_w_init: float=1.,\n",
    "        sigma_v_init: float=1.,\n",
    "        seed: Optional[int]=None\n",
    "    ) -> None:\n",
    "        self.rng = np.random.default_rng(seed)\n",
    "        b = self.rng.normal(0, sigma_b_init)\n",
    "        w = self.rng.normal(0, sigma_w_init, num_features)\n",
    "        v = self.rng.normal(0, sigma_v_init, (num_features, num_factors))\n",
    "        self.params = {'b': b, 'w': w, 'v': v}\n",
    "\n",
    "    def predict(self, x: np.ndarray) -> float:\n",
    "        if x.ndim == 1:\n",
    "            x = x.reshape(1, -1) # x: (d, n)\n",
    "        b = self.params['b']     # b: (1)\n",
    "        w = self.params['w']     # w: (d)\n",
    "        v = self.params['v']     # v: (d, k)\n",
    "\n",
    "        bias   = b\n",
    "            # (1)\n",
    "        linear = x[:, :] @ w[:]\n",
    "            # (D, N) @ (N) = (D)\n",
    "        inter  = 0.5 * np.sum((x[:, :] @ v[:, :]) ** 2 - (x[:, :] ** 2) @ (v[:, :] ** 2), axis=1)\n",
    "            # (D, K) -> (D)\n",
    "\n",
    "        result = bias + linear + inter\n",
    "            # (D)\n",
    "\n",
    "        if result.shape[0] == 1:\n",
    "            return float(result[0])\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-52.43420000201252\n",
      "(5,)\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "\n",
    "N = 100\n",
    "K = 10\n",
    "D = 5\n",
    "\n",
    "fm = FactorizationMachines(N, K)\n",
    "\n",
    "# for x: (N,)\n",
    "x = np.random.randn(N)\n",
    "y = fm.predict(x)\n",
    "print(y)       # float\n",
    "\n",
    "# for x: (D, N)\n",
    "x = np.random.choice((0, 1), size=(D, N))\n",
    "y = fm.predict(x)\n",
    "print(y.shape) # (5,)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rewriting an FM as a linear function\n",
    "\n",
    "- **Lemma** [Rendle+ (2011)]\n",
    "\n",
    "    A factorization machine is a linear function with respect to every single model parameter $\\theta \\in \\Theta = \\{b, w_i, v_{ik}\\}$.\n",
    "\n",
    "    $$\n",
    "    \\begin{aligned}\n",
    "    f(x) = \\theta h_\\theta(x) + g_\\theta(x)\n",
    "    \\end{aligned}\n",
    "    $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Differentiating $f$ by $\\theta \\in \\Theta$ yields\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial f}{\\partial \\theta}\n",
    "&=\n",
    "\\left\\{\\begin{aligned}\n",
    "& 1,\n",
    "&& \\theta=b \\\\\n",
    "& x_i,\n",
    "&& \\theta=w_i\\\\\n",
    "& x_i \\left( \\sum_{j=1}^N v_{jk} x_j - v_{ik} x_i \\right).\n",
    "&& \\theta = v_{ik}\n",
    "\\end{aligned}\\right.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "By defining $h_\\theta(x) \\coloneqq \\frac{\\partial f}{\\partial \\theta}$ and $g_\\theta(x) \\coloneqq f(x) - \\theta h_\\theta(x)$ we obtain\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "f(x) = \\theta h_\\theta(x) + g_\\theta(x).\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Remark that the value of $h_\\theta(x)$ and $g_\\theta(x)$ is independent of the value of $\\theta$, that is,\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial g_\\theta(x)}{\\partial \\theta}\n",
    "=\n",
    "\\frac{\\partial h_\\theta(x)}{\\partial \\theta}\n",
    "=\n",
    "0.\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_h_b(x: np.ndarray) -> np.ndarray:\n",
    "    return np.ones(x.shape[0])\n",
    "\n",
    "def calc_h_w(x: np.ndarray, i: int) -> np.ndarray:\n",
    "    return x[:, i]\n",
    "\n",
    "def calc_h_v(x: np.ndarray, v: np.ndarray, i: int, k: int) -> np.ndarray:\n",
    "    return x[:, i] * (x[:, :] @ v[:, k] - x[:, i] * v[i, k])\n",
    "\n",
    "def calc_g(f: np.ndarray, h: np.ndarray, p: float) -> np.ndarray:\n",
    "    return f - h * p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm that the values of g and h are independent of parameter p.\n",
    "\n",
    "# for b\n",
    "h_1 = calc_h_b(x)\n",
    "g_1 = calc_g(fm.predict(x), h_1, fm.params['b'])\n",
    "\n",
    "fm.params['b'] = fm.params['b'] - 100\n",
    "\n",
    "h_2 = calc_h_b(x)\n",
    "g_2 = calc_g(fm.predict(x), h_2, fm.params['b'])\n",
    "\n",
    "fm.params['b'] = fm.params['b'] + 100\n",
    "\n",
    "assert np.isclose(h_1, h_2).all()\n",
    "assert np.isclose(g_1, g_2).all()\n",
    "\n",
    "# for w\n",
    "for i in range(N):\n",
    "    h_1 = calc_h_w(x, i)\n",
    "    g_1 = calc_g(fm.predict(x), h_1, fm.params['w'][i])\n",
    "\n",
    "    fm.params['w'][i] = fm.params['w'][i] - 100\n",
    "\n",
    "    h_2 = calc_h_w(x, i)\n",
    "    g_2 = calc_g(fm.predict(x), h_2, fm.params['w'][i])\n",
    "\n",
    "    fm.params['w'][i] = fm.params['w'][i] + 100\n",
    "\n",
    "    assert np.isclose(h_1, h_2).all()\n",
    "    assert np.isclose(g_1, g_2).all()\n",
    "\n",
    "\n",
    "# for v\n",
    "from itertools import product\n",
    "for i, k in product(range(N), range(K)):\n",
    "    h_1 = calc_h_v(x, fm.params['v'], i, k)\n",
    "    f_1 = fm.predict(x)\n",
    "    g_1 = calc_g(f_1, h_1, fm.params['v'][i, k])\n",
    "\n",
    "    fm.params['v'][i, k] = fm.params['v'][i, k] - 100\n",
    "\n",
    "    h_2 = calc_h_v(x, fm.params['v'], i, k)\n",
    "    f_2 = fm.predict(x)\n",
    "    g_2 = calc_g(f_2, h_2, fm.params['v'][i, k])\n",
    "\n",
    "    fm.params['v'][i, k] = fm.params['v'][i, k] + 100\n",
    "\n",
    "    assert np.isclose(h_1, h_2).all()\n",
    "    assert np.isclose(g_1, g_2).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Speeding up the inference\n",
    "\n",
    "When using MCMC or something for learning, $h_\\theta(x)$ and $g_\\theta(x)$ have to be evaluated so many times, which incurs a significant computational cost. Here we show how to reduce this complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make explicit that $f(x^{(d)})$, $g_\\theta(x^{(d)})$ and $h_\\theta(x^{(d)})$ are values, we rewrite them as\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "f^{(d)} &\\coloneqq f(x^{(d)}), \\\\\n",
    "g_\\theta^{(d)} &\\coloneqq g_\\theta(x^{(d)}), \\\\\n",
    "h_\\theta^{(d)} &\\coloneqq h_\\theta(x^{(d)}). \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Note that $g_\\theta^{(d)}$ is given by\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "g_\\theta^{(d)} \\coloneqq \\theta h_\\theta^{(d)} - f^{(d)}.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "The main idea is to calculate the new $f^{(d)}$ based on its previous value. We save the value of $f^{(d)}$, and every time we sample $\\theta^{\\rm new}$, we update $f^{(d)}$ using $f^{(d)\\rm new} = f^{(d)} + \\Delta f^{(d)}$, where\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\Delta f^{(d)\\rm new}\n",
    "&\\coloneqq\n",
    "f^{(d)\\rm new} - f^{(d)} \\\\\n",
    "\n",
    "&=\n",
    "(\\theta^{\\rm new} - \\theta) h_\\theta^{(d)}.\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update rule of f values\n",
    "def calc_df(h: np.ndarray, p_new: float, p_old: float) -> np.ndarray:\n",
    "    # h: (D)\n",
    "    return (p_new - p_old) * h # (D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since $h_\\theta^{(d)}$ is given by\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "h_\\theta^{(d)}\n",
    "&=\n",
    "\\left\\{\\begin{aligned}\n",
    "& 1,\n",
    "&& \\theta=b \\\\\n",
    "& x_i^{(d)},\n",
    "&& \\theta=w_i\\\\\n",
    "& x_i^{(d)} \\left( \\sum_{j=1}^N v_{jk} x_j^{(d)} - v_{ik} x_i^{(d)} \\right),\n",
    "&& \\theta = v_{ik}\n",
    "\\end{aligned}\\right.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "the computational cost of $h_\\theta(x)$ is $\\mathcal O(1)$ for $\\theta = b, w_i$ but for $\\theta = v_{ik}$.\n",
    "\n",
    "To reduce the complexity of $h_{v_{ik}}^{(d)}$, we precalculate the value of $q_k^{(d)}$, which is defined by\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "q_k^{(d)} \\coloneqq \\sum_{j=1}^N v_{jk} x_j^{(d)}.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Using this, we can calculate\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "h_{v_{ik}}^{(d)} = x_i^{(d)} (q_k^{(d)} - v_{ik} x_i^{(d)})\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "in constant time.\n",
    "\n",
    "$q_k^{(d)}$ can be updated using $q_k^{(d) \\rm new}(x; \\Theta) = q_k^{(d)} + \\Delta q_k^{(d)}$, where\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\Delta q_k^{(d)}\n",
    "&\\coloneqq q_k^{(d) \\rm new} - q_k^{(d)} \\\\\n",
    "&= x_i^{(d)} (v_{ik}^{\\rm new} - v_{ik}),\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "whose time complexity is also $\\mathcal O(1)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# definition of q and its update rules\n",
    "def calc_q_init(x: np.ndarray, v: np.ndarray) -> np.ndarray:\n",
    "    # x: (D, N)\n",
    "    # v: (N, K)\n",
    "    return x[:, :] @ v[:, :] # (D, K)\n",
    "\n",
    "def calc_dq(x: np.ndarray, v_ik_new: float, v: np.ndarray, i: int, k: int) -> np.ndarray:\n",
    "    # v_ik_new: float\n",
    "    # v: (N, K)\n",
    "    # x: (D, N)\n",
    "    return (v_ik_new - v[i, k]) * x[:, i] # (D)\n",
    "\n",
    "def calc_h_v_fast(x: np.ndarray, v: np.ndarray, q: np.ndarray, i: int, k: int) -> np.ndarray:\n",
    "    return x[:, i] * (q[:, k] - x[:, i] * v[i, k]) # (D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "naive: 0.46719 [s]\n",
      "fast:  0.00857 [s]\n",
      "----------------------------------\n",
      "Both returned the same value: True\n"
     ]
    }
   ],
   "source": [
    "# check how much faster the fast version is\n",
    "import time\n",
    "\n",
    "def naive_updates(iter, x):\n",
    "    v = fm.params['v']\n",
    "    for _ in range(iter):\n",
    "        h = calc_h_v(x, v, 0, 0)\n",
    "        v[0, 0] -= 100\n",
    "    return h\n",
    "\n",
    "def fast_updates(iter, x):\n",
    "    v = fm.params['v']\n",
    "    q = calc_q_init(x, v)\n",
    "    for _ in range(iter):\n",
    "        h = calc_h_v_fast(x, v, q, 0, 0)\n",
    "        v_ik_new = v[0, 0] - 100\n",
    "        q[:, 0] += calc_dq(x, v_ik_new, v, 0, 0)\n",
    "        v[0, 0] = v_ik_new\n",
    "    return h\n",
    "\n",
    "iter = 100\n",
    "\n",
    "seed = 0\n",
    "rng  = np.random.default_rng(seed)\n",
    "\n",
    "N = 100\n",
    "K = 10\n",
    "D = 10000\n",
    "x = rng.choice((0, 1), size=(D, N))\n",
    "\n",
    "fm = FactorizationMachines(N, K, seed=seed)\n",
    "\n",
    "time_start = time.time()\n",
    "h_naive    = naive_updates(iter, x)\n",
    "time_end   = time.time()\n",
    "time_naive = time_end - time_start\n",
    "\n",
    "fm = FactorizationMachines(N, K, seed=seed)\n",
    "\n",
    "time_start = time.time()\n",
    "h_fast     = fast_updates(iter, x)\n",
    "time_end   = time.time()\n",
    "time_fast  = time_end - time_start\n",
    "\n",
    "print(f'naive: {time_naive:.5f} [s]')\n",
    "print(f'fast:  {time_fast:.5f} [s]')\n",
    "print('-'*34)\n",
    "print(f'Both returned the same value: {np.isclose(h_fast, h_naive).all()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we obtain the whole update rule as follows:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "q_k^{(d)}\n",
    "&=\n",
    "\\sum_{j=1}^N v_{jk} x_j^{(d)},\n",
    "\\\\\n",
    "\n",
    "\\Delta f^{(d)}\n",
    "&= (\\theta^{\\rm new} - \\theta) h_\\theta^{(d)},\n",
    "\\\\\n",
    "\n",
    "\\Delta q_k^{(d)}\n",
    "&= x_i^{(d)} (v_{ik}^{\\rm new} - v_{ik}),\n",
    "\\\\\n",
    "\n",
    "h_\\theta^{(d)}\n",
    "&=\n",
    "\\left\\{\\begin{aligned}\n",
    "& 1,\n",
    "&& \\theta=b \\\\\n",
    "& x_i^{(d)},\n",
    "&& \\theta=w_i\\\\\n",
    "& x_i^{(d)} \\left( q_k^{(d)} - v_{ik} x_i^{(d)} \\right).\n",
    "&& \\theta = v_{ik}\n",
    "\\end{aligned}\\right.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "The time complexity is $\\mathcal O(1)$ for every $\\theta$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_q_init(x: np.ndarray, v: np.ndarray) -> np.ndarray:\n",
    "    # x: (D, N)\n",
    "    # v: (N, K)\n",
    "    return x[:, :] @ v[:, :] # (D, K)\n",
    "\n",
    "def calc_dq(x: np.ndarray, v_ik_new: float, v: np.ndarray, i: int, k: int) -> np.ndarray:\n",
    "    # v_ik_new: float\n",
    "    # v: (N, K)\n",
    "    # x: (D, N)\n",
    "    return (v_ik_new - v[i, k]) * x[:, i] # (D)\n",
    "\n",
    "def calc_df(h: np.ndarray, p_new: float, p_old: float) -> np.ndarray:\n",
    "    # h: (D)\n",
    "    return (p_new - p_old) * h # (D)\n",
    "\n",
    "def calc_h_b(x) -> np.ndarray:\n",
    "    return np.ones(x.shape[0])\n",
    "\n",
    "def calc_h_w(x: np.ndarray, i: int) -> np.ndarray:\n",
    "    # x: (D, N)\n",
    "    return x[:, i]\n",
    "\n",
    "def calc_h_v_fast(x: np.ndarray, v: np.ndarray, q: np.ndarray, i: int, k: int) -> np.ndarray:\n",
    "    # x: (D, N)\n",
    "    # v: (N, K)\n",
    "    # q: (D, K)\n",
    "    return x[:, i] * (q[:, k] - x[:, i] * v[i, k]) # (D)\n",
    "\n",
    "def calc_g(f: np.ndarray, h: np.ndarray, p: float) -> np.ndarray:\n",
    "    return f[:] - h[:] * p"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
