{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02. Alternating Least Squares for Factorization Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import Optional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Factorization Machine as a Linear Model\n",
    "\n",
    "As we noted in FM_01_linear.ipynb, an FM can be rewritten as a linear model. The FM model is defined as\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\n",
    "f(x; b, w, v)\n",
    "&= b + \\sum_{i=1}^N w_i x_i + \\sum_{i \\lt j} \\sum_{k=1}^K v_{ik} v_{jk} x_i x_j \\\\\n",
    "\n",
    "&= \\theta h_\\theta(x) + g_\\theta(x)\n",
    "\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "h_\\theta(x) &\\coloneqq\n",
    "\\left\\{\\begin{aligned}\n",
    "    & 1,\n",
    "    && \\theta=b \\\\\n",
    "    & x_i,\n",
    "    && \\theta=w_i\\\\\n",
    "    & x_i \\left( \\sum_{j=1}^N v_{jk} x_j - v_{ik} x_i \\right),\n",
    "    && \\theta = v_{ik}\n",
    "\\end{aligned}\\right.\n",
    "\\\\\n",
    "\n",
    "g_\\theta(x) &\\coloneqq f(x) - \\theta h_\\theta(x).\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FactorizationMachines:\n",
    "    def __init__(self,\n",
    "        num_features: int,\n",
    "        num_factors:  int,\n",
    "        sigma_b_init: float=0.,\n",
    "        sigma_w_init: float=1.,\n",
    "        sigma_v_init: float=1.,\n",
    "        seed: Optional[int]=None\n",
    "    ) -> None:\n",
    "        self.rng = np.random.default_rng(seed)\n",
    "        b = self.rng.normal(0, sigma_b_init)\n",
    "        w = self.rng.normal(0, sigma_w_init, num_features)\n",
    "        v = self.rng.normal(0, sigma_v_init, (num_features, num_factors))\n",
    "        self.params = {'b': b, 'w': w, 'v': v}\n",
    "\n",
    "    def predict(self, x: np.ndarray) -> float:\n",
    "        if x.ndim == 1:\n",
    "            x = x.reshape(1, -1) # x: (d, n)\n",
    "        b = self.params['b']     # b: (1)\n",
    "        w = self.params['w']     # w: (d)\n",
    "        v = self.params['v']     # v: (d, k)\n",
    "\n",
    "        bias   = b\n",
    "            # (1)\n",
    "        linear = x[:, :] @ w[:]\n",
    "            # (D, N) @ (N) = (D)\n",
    "        inter  = 0.5 * np.sum((x[:, :] @ v[:, :]) ** 2 - (x[:, :] ** 2) @ (v[:, :] ** 2), axis=1)\n",
    "            # (D, K) -> (D)\n",
    "\n",
    "        result = bias + linear + inter\n",
    "            # (D)\n",
    "\n",
    "        if result.shape[0] == 1:\n",
    "            return float(result[0])\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can speed up the computation of $h_\\theta$ and $g_\\theta$ by precomputation and updating techniques.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "q_k^{(d)}\n",
    "&=\n",
    "\\sum_{j=1}^N v_{jk} x_j^{(d)},\n",
    "\\\\\n",
    "\n",
    "\\Delta f^{(d)}\n",
    "&= (\\theta^{\\rm new} - \\theta) h_\\theta^{(d)},\n",
    "\\\\\n",
    "\n",
    "\\Delta q_k^{(d)}\n",
    "&= x_i^{(d)} (v_{ik}^{\\rm new} - v_{ik}),\n",
    "\\\\\n",
    "\n",
    "h_\\theta^{(d)}\n",
    "&=\n",
    "\\left\\{\\begin{aligned}\n",
    "& 1,\n",
    "&& \\theta=b \\\\\n",
    "& x_i^{(d)},\n",
    "&& \\theta=w_i\\\\\n",
    "& x_i^{(d)} \\left( q_k^{(d)} - v_{ik} x_i^{(d)} \\right).\n",
    "&& \\theta = v_{ik}\n",
    "\\end{aligned}\\right.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "The time complexity is $\\mathcal O(1)$ for every $\\theta$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_q_init(x: np.ndarray, v: np.ndarray) -> np.ndarray:\n",
    "    # x: (D, N)\n",
    "    # v: (N, K)\n",
    "    return x[:, :] @ v[:, :] # (D, K)\n",
    "\n",
    "def calc_dq(x: np.ndarray, v_ik_new: float, v: np.ndarray, i: int, k: int) -> np.ndarray:\n",
    "    # v_ik_new: float\n",
    "    # v: (N, K)\n",
    "    # x: (D, N)\n",
    "    return (v_ik_new - v[i, k]) * x[:, i] # (D)\n",
    "\n",
    "def calc_df(h: np.ndarray, p_new: float, p_old: float) -> np.ndarray:\n",
    "    # h: (D)\n",
    "    return (p_new - p_old) * h # (D)\n",
    "\n",
    "def calc_h_b(x) -> float:\n",
    "    return np.ones(x.shape[0])\n",
    "\n",
    "def calc_h_w(x: np.ndarray, i: int) -> np.ndarray:\n",
    "    # x: (D, N)\n",
    "    return x[:, i]\n",
    "\n",
    "def calc_h_v_fast(x: np.ndarray, v: np.ndarray, q: np.ndarray, i: int, k: int) -> np.ndarray:\n",
    "    # x: (D, N)\n",
    "    # v: (N, K)\n",
    "    # q: (D, K)\n",
    "    return x[:, i] * (q[:, k] - x[:, i] * v[i, k]) # (D)\n",
    "\n",
    "def calc_g(f: np.ndarray, h: np.ndarray, p: float) -> np.ndarray:\n",
    "    g = f - h * p\n",
    "    return g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternating Least Squares (ALS) for Factorization Machines\n",
    "\n",
    "Assume that a model is first-order for each parameter $\\theta \\in \\Theta$ and can be written as\n",
    "\n",
    "$$\n",
    "    y_\\theta^{(d)} = \\theta x_\\theta^{(d)} + \\varepsilon^{(d)},\n",
    "$$\n",
    "\n",
    "where\n",
    "- $y_\\theta^{(d)}$ is the output variable,\n",
    "- $x_\\theta^{(d)}$ is the input variable,\n",
    "- $\\theta$ is the model parameter, and\n",
    "- $\\varepsilon^{(d)}$ is the error term.\n",
    "\n",
    "We want to estimate $\\theta$ from the observed data $\\{x_\\theta^{(d)}, y_\\theta^{(d)}\\}_{d=1}^D$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ordinary Least Squares (OLS)\n",
    "\n",
    "If a model has a single parameter, $\\Theta = \\{ \\theta \\}$, we can estimate $\\theta$ by minimizing the sum of squared errors (SSE):\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\hat \\theta\n",
    "&= \\argmin_\\theta \\sum_{d=1}^D \\left( y_\\theta^{(d)} - \\theta x_\\theta^{(d)} \\right)^2,\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "whose result is given by\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\hat \\theta\n",
    "&= (x_\\theta^\\top x_\\theta)^{-1} x_\\theta^\\top y_\\theta \\\\\n",
    "&= \\left( \\sum_{d=1}^D x_\\theta^{(d)2} \\right)^{-1} \\sum_{d=1}^D x_\\theta^{(d)} y_\\theta^{(d)}.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "This is called the **ordinary least squares** (OLS) method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternating Least Squares (ALS)\n",
    "\n",
    "On the other hand, if we have an $M$-parameter models, i.e., $\\Theta = \\{ \\theta_m \\}_{m=1,\\dots,M}$, and the model is first-order with respect to each $\\theta_m$, we *alternatively* apply the least squares method to each parameter $\\theta_m$:\n",
    "\n",
    "1. repeat until convergence:\n",
    "   1. repeat for all $\\theta_m \\in \\Theta$:\n",
    "      1. fix all the other parameters $\\Theta \\setminus \\{\\theta_m\\}$\n",
    "      2. estimate $\\theta_m$ by minimizing the SSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, let our model given by\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "y^{(d)}\n",
    "&= f^{(d)} + \\varepsilon^{(d)},\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "f^{(d)} = \\sum_{i=1}^N \\sum_{j=1}^N \\sum_{k=1}^K v_{ik} v_{jk} x_i^{(d)} x_j^{(d)}.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "The model has $NK$ parameters $\\Theta = \\{ v_{ik} \\}_{(i,k) \\in [N] \\times [K]}$ and first-order with respect to each parameter.\n",
    "\n",
    "We can estimate $v_{ik}$ by minimizing the SSE:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\hat v_{ik}\n",
    "&= \\argmin_{v_{ik}} \\sum_{d=1}^D \\left( y^{(d)} - \\sum_{i=1}^N \\sum_{j=1}^N \\sum_{k=1}^K v_{ik} v_{jk} x_i^{(d)} x_j^{(d)} \\right)^2.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "If we write\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "x_{v_{ik}}^{(d)}\n",
    "&\\coloneqq\n",
    "\\frac{\\partial f^{(d)}}{\\partial v_{ik}},\n",
    "\\\\\n",
    "\n",
    "g_{v_{ik}}^{(d)}\n",
    "&\\coloneqq\n",
    "f^{(d)} - v_{ik} h_{v_{ik}}^{(d)},\n",
    "\\\\\n",
    "\n",
    "y_{v_{ik}}^{(d)}\n",
    "&\\coloneqq\n",
    "y^{(d)} - g_{v_{ik}}^{(d)},\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "the estimator of $v_{ik}$ is given by\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\hat v_{ik}\n",
    "&=\n",
    "\\argmin_{v_{ik}} \\sum_{d=1}^D \\left( y^{(d)} - (v_{ik} x_{v_{ik}}^{(d)} + g_{v_{ik}}^{(d)}) \\right)^2\n",
    "\\\\\n",
    "\n",
    "&=\n",
    "\\argmin_{v_{ik}} \\sum_{d=1}^D \\left( (y^{(d)} - g_{v_{ik}}^{(d)}) - v_{ik} x_{v_{ik}}^{(d)} \\right)^2\n",
    "\\\\\n",
    "\n",
    "&=\n",
    "\\argmin_{v_{ik}} \\sum_{d=1}^D \\left( y_{v_{ik}}^{(d)} - v_{ik} x_{v_{ik}}^{(d)} \\right)^2\n",
    "\\\\\n",
    "\n",
    "&=\n",
    "\\left( \\sum_{d=1}^D x_{v_{ik}}^{(d)2} \\right)^{-1} \\sum_{d=1}^D x_{v_{ik}}^{(d)} y_{v_ik}^{(d)}.\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is how the **alternating least squares (ALS)** method works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ALS for FM\n",
    "\n",
    "From here, we apply the ALS method to the FM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, the model of an FM is given by\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "f^{(d)} = \\theta h_\\theta^{(d)} + g_\\theta^{(d)}.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "By defining\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "x_\\theta^{(d)} &\\coloneqq h_\\theta^{(d)}, \\\\\n",
    "y_\\theta^{(d)} &\\coloneqq y^{(d)} - g_\\theta^{(d)},\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "the estimated $\\theta$ is\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\hat \\theta\n",
    "\n",
    "&=\n",
    "\\left( \\sum_{d=1}^D x_\\theta^{(d)2} \\right)^{-1} \\sum_{d=1}^D x_\\theta^{(d)} y_\\theta^{(d)}.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "By repeating this for all $\\theta \\in \\Theta$, we can estimate the parameters of the FM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the use of naive OLS may result in division by zero, it is common to use L2 regularization as an alternative:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\hat \\theta\n",
    "\n",
    "&=\n",
    "\\left( \\sum_{d=1}^D x_\\theta^{(d)2} + \\lambda \\right)^{-1} \\sum_{d=1}^D x_\\theta^{(d)} y_\\theta^{(d)}.\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_q_init(x: np.ndarray, v: np.ndarray) -> np.ndarray:\n",
    "    # x: (D, N)\n",
    "    # v: (N, K)\n",
    "    return x[:, :] @ v[:, :] # (D, K)\n",
    "\n",
    "def calc_dq(x: np.ndarray, v_ik_new: float, v: np.ndarray, i: int, k: int) -> np.ndarray:\n",
    "    # v_ik_new: float\n",
    "    # v: (N, K)\n",
    "    # x: (D, N)\n",
    "    return (v_ik_new - v[i, k]) * x[:, i] # (D)\n",
    "\n",
    "def calc_df(h: np.ndarray, p_new: float, p_old: float) -> np.ndarray:\n",
    "    # h: (D)\n",
    "    return (p_new - p_old) * h # (D)\n",
    "\n",
    "def calc_h_b(x) -> float:\n",
    "    return np.ones(x.shape[0])\n",
    "\n",
    "def calc_h_w(x: np.ndarray, i: int) -> np.ndarray:\n",
    "    # x: (D, N)\n",
    "    return x[:, i]\n",
    "\n",
    "def calc_h_v_fast(x: np.ndarray, v: np.ndarray, q: np.ndarray, i: int, k: int) -> np.ndarray:\n",
    "    # x: (D, N)\n",
    "    # v: (N, K)\n",
    "    # q: (D, K)\n",
    "    return x[:, i] * (q[:, k] - x[:, i] * v[i, k]) # (D)\n",
    "\n",
    "def calc_g(f: np.ndarray, h: np.ndarray, p: float) -> np.ndarray:\n",
    "    g = f - h * p\n",
    "    return g\n",
    "\n",
    "def sample_param_lstsq(y: np.ndarray, h: np.ndarray, g: np.ndarray, lamb: float=1e-8) -> float:\n",
    "    # h: (D)\n",
    "    # g: (D)\n",
    "    x_theta = h\n",
    "    y_theta = y - g\n",
    "    return np.sum(x_theta * y_theta) / (np.sum(x_theta ** 2) + lamb)\n",
    "\n",
    "def train_fm_als(\n",
    "    init_params: dict,\n",
    "    x_data: np.ndarray,\n",
    "    y_data: np.ndarray,\n",
    "    f_init: np.ndarray,\n",
    "    num_iter: int,\n",
    ") -> dict:\n",
    "    # get indices\n",
    "    N = x_data.shape[1]\n",
    "    K = init_params['v'].shape[1]\n",
    "\n",
    "    # get initial parameter\n",
    "    params = init_params\n",
    "\n",
    "    # precalculate\n",
    "    f = f_init\n",
    "    q = calc_q_init(x_data, params['v'])\n",
    "\n",
    "    # main loop\n",
    "    for iter in range(num_iter):\n",
    "        # sample b\n",
    "        h_b   = calc_h_b(x_data)\n",
    "        g_b   = calc_g(f, h_b, params['b'])\n",
    "        b_new = sample_param_lstsq(y_data, h_b, g_b)\n",
    "        f     = f + calc_df(h_b, b_new, params['b'])\n",
    "        params['b'] = b_new\n",
    "\n",
    "        # sample w\n",
    "        for i in range(N):\n",
    "            h_w   = calc_h_w(x_data, i)\n",
    "            g_w   = calc_g(f, h_w, params['w'][i])\n",
    "            w_i_new = sample_param_lstsq(y_data, h_w, g_w)\n",
    "            f     = f + calc_df(h_w, w_i_new, params['w'][i])\n",
    "            params['w'][i] = w_i_new\n",
    "\n",
    "        # sample v\n",
    "        for i in range(N):\n",
    "            for k in range(K):\n",
    "                h_v      = calc_h_v_fast(x_data, params['v'], q, i, k)\n",
    "                g_v      = calc_g(f, h_v, params['v'][i, k])\n",
    "                v_ik_new = sample_param_lstsq(y_data, h_v, g_v)\n",
    "                f        = f      + calc_df(h_v, v_ik_new, params['v'][i, k])\n",
    "                q[:,k]   = q[:,k] + calc_dq(x_data, v_ik_new, params['v'], i, k)\n",
    "                params['v'][i, k] = v_ik_new\n",
    "\n",
    "        if iter % 10 == 0:\n",
    "            print(f'iter: {iter}, loss: {np.mean((y_data - f) ** 2)}')\n",
    "\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter: 0, loss: 26.39329176790396\n",
      "iter: 10, loss: 2.3858678729327116\n",
      "iter: 20, loss: 1.2735758696485489\n",
      "iter: 30, loss: 0.877355504778103\n",
      "iter: 40, loss: 0.585963714328617\n",
      "iter: 50, loss: 0.3821961192027613\n",
      "iter: 60, loss: 0.31346318080660984\n",
      "iter: 70, loss: 0.2759510632942438\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter: 80, loss: 0.2478740205126534\n",
      "iter: 90, loss: 0.22491829439837246\n",
      "iter: 100, loss: 0.205683520927063\n",
      "iter: 110, loss: 0.18932825865828948\n",
      "iter: 120, loss: 0.17523883467637943\n",
      "iter: 130, loss: 0.16298095072785646\n",
      "iter: 140, loss: 0.1522543744330969\n",
      "iter: 150, loss: 0.14284317842872335\n",
      "iter: 160, loss: 0.13457849180403977\n",
      "iter: 170, loss: 0.12731826095125778\n",
      "iter: 180, loss: 0.12093892161998704\n",
      "iter: 190, loss: 0.11533219933686281\n",
      "iter: 200, loss: 0.11040298662542901\n",
      "iter: 210, loss: 0.10606716544653094\n",
      "iter: 220, loss: 0.10224961083738565\n",
      "iter: 230, loss: 0.09888274071974543\n",
      "iter: 240, loss: 0.09590567739768235\n",
      "iter: 250, loss: 0.09326384701702065\n",
      "iter: 260, loss: 0.09090878000112879\n",
      "iter: 270, loss: 0.08879792819306101\n",
      "iter: 280, loss: 0.08689440045664532\n",
      "iter: 290, loss: 0.085166589623591\n",
      "iter: 300, loss: 0.08358770613351095\n",
      "iter: 310, loss: 0.08213525175345\n",
      "iter: 320, loss: 0.08079046949114813\n",
      "iter: 330, loss: 0.0795378009635657\n",
      "iter: 340, loss: 0.07836437491224528\n",
      "iter: 350, loss: 0.07725954278329542\n",
      "iter: 360, loss: 0.07621447039634374\n",
      "iter: 370, loss: 0.07522178907913747\n",
      "iter: 380, loss: 0.07427530529755628\n",
      "iter: 390, loss: 0.07336976469831642\n",
      "iter: 400, loss: 0.07250066448576145\n",
      "iter: 410, loss: 0.07166410702504658\n",
      "iter: 420, loss: 0.07085668732652567\n",
      "iter: 430, loss: 0.07007540742817453\n",
      "iter: 440, loss: 0.06931761145957988\n",
      "iter: 450, loss: 0.06858093616082073\n",
      "iter: 460, loss: 0.06786327268873173\n",
      "iter: 470, loss: 0.06716273655466952\n",
      "iter: 480, loss: 0.06647764342488043\n",
      "iter: 490, loss: 0.06580648923648189\n",
      "iter: 500, loss: 0.06514793362819157\n",
      "iter: 510, loss: 0.06450078606575711\n",
      "iter: 520, loss: 0.06386399428061779\n",
      "iter: 530, loss: 0.06323663476543961\n",
      "iter: 540, loss: 0.06261790511121136\n",
      "iter: 550, loss: 0.0620071179546513\n",
      "iter: 560, loss: 0.061403696255144864\n",
      "iter: 570, loss: 0.06080716955668074\n",
      "iter: 580, loss: 0.06021717082827359\n",
      "iter: 590, loss: 0.05963343342899112\n",
      "iter: 600, loss: 0.059055787721454894\n",
      "iter: 610, loss: 0.058484156868457315\n",
      "iter: 620, loss: 0.057918551396457074\n",
      "iter: 630, loss: 0.0573590621989524\n",
      "iter: 640, loss: 0.05680585177978225\n",
      "iter: 650, loss: 0.056259143694058114\n",
      "iter: 660, loss: 0.05571921032055511\n",
      "iter: 670, loss: 0.0551863592775855\n",
      "iter: 680, loss: 0.0546609189555842\n",
      "iter: 690, loss: 0.05414322376469767\n",
      "iter: 700, loss: 0.05363359976827947\n",
      "iter: 710, loss: 0.05313235138288949\n",
      "iter: 720, loss: 0.05263974976955256\n",
      "iter: 730, loss: 0.05215602342583607\n",
      "iter: 740, loss: 0.05168135132783304\n",
      "iter: 750, loss: 0.0512158587849688\n",
      "iter: 760, loss: 0.05075961598072281\n",
      "iter: 770, loss: 0.05031263899955794\n",
      "iter: 780, loss: 0.049874893001081055\n",
      "iter: 790, loss: 0.049446297107048454\n",
      "iter: 800, loss: 0.04902673051872951\n",
      "iter: 810, loss: 0.04861603937873988\n",
      "iter: 820, loss: 0.0482140439252921\n",
      "iter: 830, loss: 0.04782054554788308\n",
      "iter: 840, loss: 0.04743533343072804\n",
      "iter: 850, loss: 0.04705819055365687\n",
      "iter: 860, loss: 0.0466888989013467\n",
      "iter: 870, loss: 0.04632724380459696\n",
      "iter: 880, loss: 0.045973017398299135\n",
      "iter: 890, loss: 0.04562602122814359\n",
      "iter: 900, loss: 0.045286068072104606\n",
      "iter: 910, loss: 0.04495298306451109\n",
      "iter: 920, loss: 0.0446266042219386\n",
      "iter: 930, loss: 0.044306782473340106\n",
      "iter: 940, loss: 0.04399338129394059\n",
      "iter: 950, loss: 0.043686276035327355\n",
      "iter: 960, loss: 0.043385353034496185\n",
      "iter: 970, loss: 0.043090508573690164\n",
      "iter: 980, loss: 0.04280164775161714\n",
      "iter: 990, loss: 0.042518683315761543\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "N = 16\n",
    "K = 8\n",
    "D = 128\n",
    "\n",
    "seed = 0\n",
    "rng  = np.random.default_rng(seed)\n",
    "fm   = FactorizationMachines(N, K, seed=seed)\n",
    "x    = rng.choice((0, 1), size=(D, N))\n",
    "\n",
    "Q    = rng.uniform(-1., 1., (N, N))\n",
    "y    = np.einsum('dn,nm,dm->d', x, Q, x)\n",
    "\n",
    "_ = train_fm_als(\n",
    "    fm.params, x, y, fm.predict(x), 1000\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems pretty good. Let's implement above in Julia to make it superfast!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PyCall.jlwrap train_fm_als>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from julia import Main\n",
    "\n",
    "julia_code = \"\"\"\n",
    "function calc_q_init(x::Array{Int64,2}, v::Array{Float64,2})::Array{Float64,2}\n",
    "    # x: (D, N)\n",
    "    # v: (N, K)\n",
    "    return x[:, :] * v[:, :] # (D, K)\n",
    "end\n",
    "\n",
    "function calc_dq(x::Array{Int64,2}, v_ik_new::Float64, v::Array{Float64,2}, i::Int, k::Int)::Array{Float64,1}\n",
    "    # v_ik_new: float\n",
    "    # v: (N, K)\n",
    "    # x: (D, N)\n",
    "    return (v_ik_new - v[i, k]) * x[:, i] # (D)\n",
    "end\n",
    "\n",
    "function calc_df(h::Array{Float64,1}, p_new::Float64, p_old::Float64)::Array{Float64,1}\n",
    "    # h: (D)\n",
    "    return (p_new - p_old) * h # (D)\n",
    "end\n",
    "\n",
    "function calc_h_b(x::Array{Int64,2})::Array{Float64,1}\n",
    "    return ones(size(x, 1))\n",
    "end\n",
    "\n",
    "function calc_h_w(x::Array{Int64,2}, i::Int)::Array{Float64,1}\n",
    "    # x: (D, N)\n",
    "    return x[:, i]\n",
    "end\n",
    "\n",
    "function calc_h_v_fast(x::Array{Int64,2}, v::Array{Float64,2}, q::Array{Float64,2}, i::Int, k::Int)::Array{Float64,1}\n",
    "    # x: (D, N)\n",
    "    # v: (N, K)\n",
    "    # q: (D, K)\n",
    "    return x[:, i] .* (q[:, k] - x[:, i] .* v[i, k]) # (D)\n",
    "end\n",
    "\n",
    "function calc_g(f::Array{Float64,1}, h::Array{Float64,1}, p::Float64)::Array{Float64,1}\n",
    "    g = f - h .* p\n",
    "    return g\n",
    "end\n",
    "\n",
    "function sample_param_lstsq(y::Array{Float64,1}, h::Array{Float64,1}, g::Array{Float64,1}, lamb::Float64=1e-8)::Float64\n",
    "    # h: (D)\n",
    "    # g: (D)\n",
    "    x_theta = h\n",
    "    y_theta = y - g\n",
    "    return sum(x_theta .* y_theta) / (sum(x_theta .^ 2) + lamb)\n",
    "end\n",
    "\n",
    "function train_fm_als(\n",
    "    init_params::Dict,\n",
    "    x_data::Array{Int64,2},\n",
    "    y_data::Array{Float64,1},\n",
    "    f_init::Array{Float64,1},\n",
    "    num_iter::Int\n",
    ")::Tuple{Dict, Array{Float64,1}}\n",
    "    # loss history\n",
    "    loss_hist = Float64[]\n",
    "    push!(loss_hist, sum((y_data - f_init) .^ 2) / size(y_data, 1))\n",
    "\n",
    "    # get indices\n",
    "    N = size(x_data, 2)\n",
    "    K = size(init_params[\"v\"], 2)\n",
    "\n",
    "    # get initial parameter\n",
    "    params = init_params\n",
    "\n",
    "    # precalculate\n",
    "    f = f_init\n",
    "    q = calc_q_init(x_data, params[\"v\"])\n",
    "\n",
    "    # main loop\n",
    "    for iter in 1:num_iter\n",
    "        # sample b\n",
    "        h_b   = calc_h_b(x_data)\n",
    "        g_b   = calc_g(f, h_b, params[\"b\"])\n",
    "        b_new = sample_param_lstsq(y_data, h_b, g_b)\n",
    "        f     = f + calc_df(h_b, b_new, params[\"b\"])\n",
    "        params[\"b\"] = b_new\n",
    "\n",
    "        # sample w\n",
    "        for i in 1:N\n",
    "            h_w   = calc_h_w(x_data, i)\n",
    "            g_w   = calc_g(f, h_w, params[\"w\"][i])\n",
    "            w_i_new = sample_param_lstsq(y_data, h_w, g_w)\n",
    "            f     = f + calc_df(h_w, w_i_new, params[\"w\"][i])\n",
    "            params[\"w\"][i] = w_i_new\n",
    "        end\n",
    "\n",
    "        # sample v\n",
    "        for i in 1:N\n",
    "            for k in 1:K\n",
    "                h_v      = calc_h_v_fast(x_data, params[\"v\"], q, i, k)\n",
    "                g_v      = calc_g(f, h_v, params[\"v\"][i, k])\n",
    "                v_ik_new = sample_param_lstsq(y_data, h_v, g_v)\n",
    "                f        = f      + calc_df(h_v, v_ik_new, params[\"v\"][i, k])\n",
    "                q[:,k]   = q[:,k] + calc_dq(x_data, v_ik_new, params[\"v\"], i, k)\n",
    "                params[\"v\"][i, k] = v_ik_new\n",
    "            end\n",
    "        end\n",
    "\n",
    "        if iter % 10 == 0\n",
    "            println(\"iter: $iter, loss: $(sum((y_data - f) .^ 2) / size(y_data, 1))\")\n",
    "        end\n",
    "\n",
    "        push!(loss_hist, sum((y_data - f) .^ 2) / size(y_data, 1))\n",
    "    end\n",
    "\n",
    "    return params, loss_hist\n",
    "end\n",
    "\"\"\"\n",
    "\n",
    "Main.eval(julia_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter: 10, loss: 2.6647681411167468\n",
      "iter: 20, loss: 1.32934990271368\n",
      "iter: 30, loss: 0.9091061582182377\n",
      "iter: 40, loss: 0.6147971031641509\n",
      "iter: 50, loss: 0.39377628835964895\n",
      "iter: 60, loss: 0.31820225501498256\n",
      "iter: 70, loss: 0.27914843037945875\n",
      "iter: 80, loss: 0.2504179447407509\n",
      "iter: 90, loss: 0.22702999530023021\n",
      "iter: 100, loss: 0.20746555786830556\n",
      "iter: 110, loss: 0.19085322387488063\n",
      "iter: 120, loss: 0.1765594138540084\n",
      "iter: 130, loss: 0.16413368938025383\n",
      "iter: 140, loss: 0.15326474004443702\n",
      "iter: 150, loss: 0.14373017132132043\n",
      "iter: 160, loss: 0.135357576095628\n",
      "iter: 170, loss: 0.12800273211593904\n",
      "iter: 180, loss: 0.12154041146253809\n",
      "iter: 190, loss: 0.11586090770753783\n",
      "iter: 200, loss: 0.11086789368331386\n",
      "iter: 210, loss: 0.1064762488603552\n",
      "iter: 220, loss: 0.10261002483105713\n",
      "iter: 230, loss: 0.09920093218900786\n",
      "iter: 240, loss: 0.09618744615122982\n",
      "iter: 250, loss: 0.09351437305598995\n",
      "iter: 260, loss: 0.09113264035387658\n",
      "iter: 270, loss: 0.088999117896592\n",
      "iter: 280, loss: 0.08707636386600197\n",
      "iter: 290, loss: 0.08533226237675036\n",
      "iter: 300, loss: 0.0837395650921808\n",
      "iter: 310, loss: 0.0822753692754754\n",
      "iter: 320, loss: 0.08092056856380282\n",
      "iter: 330, loss: 0.07965930839330701\n",
      "iter: 340, loss: 0.07847847056008529\n",
      "iter: 350, loss: 0.07736720358312008\n",
      "iter: 360, loss: 0.07631650852948385\n",
      "iter: 370, loss: 0.07531888418431257\n",
      "iter: 380, loss: 0.07436803097283377\n",
      "iter: 390, loss: 0.0734586098105289\n",
      "iter: 400, loss: 0.07258604995619121\n",
      "iter: 410, loss: 0.07174639882443738\n",
      "iter: 420, loss: 0.07093620640598725\n",
      "iter: 430, loss: 0.07015243725499246\n",
      "iter: 440, loss: 0.06939240373739299\n",
      "iter: 450, loss: 0.0686537152092706\n",
      "iter: 460, loss: 0.06793423885230003\n",
      "iter: 470, loss: 0.06723206891387963\n",
      "iter: 480, loss: 0.06654550200139081\n",
      "iter: 490, loss: 0.06587301681930631\n",
      "iter: 500, loss: 0.06521325730142852\n",
      "iter: 510, loss: 0.06456501848696376\n",
      "iter: 520, loss: 0.06392723474070683\n",
      "iter: 530, loss: 0.06329897005272675\n",
      "iter: 540, loss: 0.06267941020111303\n",
      "iter: 550, loss: 0.06206785655000502\n",
      "iter: 560, loss: 0.06146372120798817\n",
      "iter: 570, loss: 0.060866523208882806\n",
      "iter: 580, loss: 0.060275885314123055\n",
      "iter: 590, loss: 0.05969153098662404\n",
      "iter: 600, loss: 0.05911328106083737\n",
      "iter: 610, loss: 0.058541049640914646\n",
      "iter: 620, loss: 0.05797483880401323\n",
      "iter: 630, loss: 0.05741473177104759\n",
      "iter: 640, loss: 0.05686088433069456\n",
      "iter: 650, loss: 0.05631351445752236\n",
      "iter: 660, loss: 0.05577289024006475\n",
      "iter: 670, loss: 0.055239316413491474\n",
      "iter: 680, loss: 0.05471311995529602\n",
      "iter: 690, loss: 0.05419463533192022\n",
      "iter: 700, loss: 0.053684190062676225\n",
      "iter: 710, loss: 0.05318209128356144\n",
      "iter: 720, loss: 0.05268861394414314\n",
      "iter: 730, loss: 0.05220399116092961\n",
      "iter: 740, loss: 0.05172840709385733\n",
      "iter: 750, loss: 0.051261992527964415\n",
      "iter: 760, loss: 0.0508048231518715\n",
      "iter: 770, loss: 0.050356920349369456\n",
      "iter: 780, loss: 0.04991825417719981\n",
      "iter: 790, loss: 0.049488748102060325\n",
      "iter: 800, loss: 0.049068285017065685\n",
      "iter: 810, loss: 0.04865671405028112\n",
      "iter: 820, loss: 0.04825385770849843\n",
      "iter: 830, loss: 0.047859518958248526\n",
      "iter: 840, loss: 0.04747348792220537\n",
      "iter: 850, loss: 0.04709554795234081\n",
      "iter: 860, loss: 0.04672548092286677\n",
      "iter: 870, loss: 0.04636307165990478\n",
      "iter: 880, loss: 0.04600811148701638\n",
      "iter: 890, loss: 0.04566040091452327\n",
      "iter: 900, loss: 0.045319751535834764\n",
      "iter: 910, loss: 0.044985987216913804\n",
      "iter: 920, loss: 0.04465894467737213\n",
      "iter: 930, loss: 0.04433847356561452\n",
      "iter: 940, loss: 0.04402443612805636\n",
      "iter: 950, loss: 0.04371670656569351\n",
      "iter: 960, loss: 0.0434151701618382\n",
      "iter: 970, loss: 0.04311972225396621\n",
      "iter: 980, loss: 0.04283026711138444\n",
      "iter: 990, loss: 0.04254671676949299\n",
      "iter: 1000, loss: 0.04226898986126352\n",
      "iter: 1010, loss: 0.041997010477464534\n",
      "iter: 1020, loss: 0.041730707079247246\n",
      "iter: 1030, loss: 0.041470011479973146\n",
      "iter: 1040, loss: 0.04121485790758992\n",
      "iter: 1050, loss: 0.04096518215433292\n",
      "iter: 1060, loss: 0.04072092081691606\n",
      "iter: 1070, loss: 0.040482010627596474\n",
      "iter: 1080, loss: 0.04024838787435891\n",
      "iter: 1090, loss: 0.04001998790694072\n",
      "iter: 1100, loss: 0.03979674472429632\n",
      "iter: 1110, loss: 0.03957859063839946\n",
      "iter: 1120, loss: 0.03936545600882699\n",
      "iter: 1130, loss: 0.03915726904236835\n",
      "iter: 1140, loss: 0.03895395565185284\n",
      "iter: 1150, loss: 0.03875543936847511\n",
      "iter: 1160, loss: 0.038561641302065304\n",
      "iter: 1170, loss: 0.03837248014399178\n",
      "iter: 1180, loss: 0.038187872207657916\n",
      "iter: 1190, loss: 0.038007731501851834\n",
      "iter: 1200, loss: 0.037831969832530836\n",
      "iter: 1210, loss: 0.03766049692892593\n",
      "iter: 1220, loss: 0.03749322059017609\n",
      "iter: 1230, loss: 0.037330046848995345\n",
      "iter: 1240, loss: 0.037170880149179596\n",
      "iter: 1250, loss: 0.037015623534031095\n",
      "iter: 1260, loss: 0.036864178843047476\n",
      "iter: 1270, loss: 0.03671644691446436\n",
      "iter: 1280, loss: 0.03657232779149084\n",
      "iter: 1290, loss: 0.03643172093027313\n",
      "iter: 1300, loss: 0.03629452540784699\n",
      "iter: 1310, loss: 0.036160640128517586\n",
      "iter: 1320, loss: 0.03602996402728412\n",
      "iter: 1330, loss: 0.0359023962690912\n",
      "iter: 1340, loss: 0.035777836442846496\n",
      "iter: 1350, loss: 0.03565618474926583\n",
      "iter: 1360, loss: 0.035537342181767045\n",
      "iter: 1370, loss: 0.035421210699717855\n",
      "iter: 1380, loss: 0.03530769339348196\n",
      "iter: 1390, loss: 0.03519669464079252\n",
      "iter: 1400, loss: 0.03508812025407529\n",
      "iter: 1410, loss: 0.034981877618434944\n",
      "iter: 1420, loss: 0.03487787582008837\n",
      "iter: 1430, loss: 0.03477602576510076\n",
      "iter: 1440, loss: 0.034676240288341355\n",
      "iter: 1450, loss: 0.034578434252632004\n",
      "iter: 1460, loss: 0.03448252463811867\n",
      "iter: 1470, loss: 0.03438843062192999\n",
      "iter: 1480, loss: 0.03429607364823476\n",
      "iter: 1490, loss: 0.03420537748884587\n",
      "iter: 1500, loss: 0.03411626829454361\n",
      "iter: 1510, loss: 0.034028674637319976\n",
      "iter: 1520, loss: 0.03394252754377636\n",
      "iter: 1530, loss: 0.03385776051991183\n",
      "iter: 1540, loss: 0.033774309567569764\n",
      "iter: 1550, loss: 0.03369211319281684\n",
      "iter: 1560, loss: 0.033611112406544706\n",
      "iter: 1570, loss: 0.03353125071758693\n",
      "iter: 1580, loss: 0.03345247411866231\n",
      "iter: 1590, loss: 0.0333747310654542\n",
      "iter: 1600, loss: 0.033297972449142045\n",
      "iter: 1610, loss: 0.033222151562709135\n",
      "iter: 1620, loss: 0.03314722406134816\n",
      "iter: 1630, loss: 0.033073147917296306\n",
      "iter: 1640, loss: 0.03299988336942803\n",
      "iter: 1650, loss: 0.03292739286794251\n",
      "iter: 1660, loss: 0.03285564101447601\n",
      "iter: 1670, loss: 0.032784594497986735\n",
      "iter: 1680, loss: 0.032714222026744344\n",
      "iter: 1690, loss: 0.03264449425677521\n",
      "iter: 1700, loss: 0.032575383717108754\n",
      "iter: 1710, loss: 0.032506864732174394\n",
      "iter: 1720, loss: 0.032438913341707905\n",
      "iter: 1730, loss: 0.0323715072185186\n",
      "iter: 1740, loss: 0.03230462558448142\n",
      "iter: 1750, loss: 0.03223824912511934\n",
      "iter: 1760, loss: 0.03217235990313641\n",
      "iter: 1770, loss: 0.03210694127127513\n",
      "iter: 1780, loss: 0.032041977784860474\n",
      "iter: 1790, loss: 0.03197745511440439\n",
      "iter: 1800, loss: 0.031913359958632107\n",
      "iter: 1810, loss: 0.03184967995829996\n",
      "iter: 1820, loss: 0.03178640361115459\n",
      "iter: 1830, loss: 0.03172352018839622\n",
      "iter: 1840, loss: 0.03166101965297723\n",
      "iter: 1850, loss: 0.031598892580078475\n",
      "iter: 1860, loss: 0.031537130080074346\n",
      "iter: 1870, loss: 0.03147572372429297\n",
      "iter: 1880, loss: 0.03141466547385307\n",
      "iter: 1890, loss: 0.031353947611842126\n",
      "iter: 1900, loss: 0.03129356267907643\n",
      "iter: 1910, loss: 0.03123350341365443\n",
      "iter: 1920, loss: 0.031173762694495507\n",
      "iter: 1930, loss: 0.03111433348901318\n",
      "iter: 1940, loss: 0.031055208805058285\n",
      "iter: 1950, loss: 0.030996381647219344\n",
      "iter: 1960, loss: 0.030937844977542345\n",
      "iter: 1970, loss: 0.030879591680694537\n",
      "iter: 1980, loss: 0.030821614533567573\n",
      "iter: 1990, loss: 0.03076390617927234\n",
      "iter: 2000, loss: 0.030706459105454623\n",
      "iter: 2010, loss: 0.03064926562681899\n",
      "iter: 2020, loss: 0.03059231787172309\n",
      "iter: 2030, loss: 0.030535607772673674\n",
      "iter: 2040, loss: 0.030479127060526922\n",
      "iter: 2050, loss: 0.03042286726217705\n",
      "iter: 2060, loss: 0.030366819701483343\n",
      "iter: 2070, loss: 0.0303109755031832\n",
      "iter: 2080, loss: 0.030255325599511537\n",
      "iter: 2090, loss: 0.03019986073923712\n",
      "iter: 2100, loss: 0.030144571498823557\n",
      "iter: 2110, loss: 0.030089448295413376\n",
      "iter: 2120, loss: 0.030034481401330963\n",
      "iter: 2130, loss: 0.029979660959805327\n",
      "iter: 2140, loss: 0.029924977001621313\n",
      "iter: 2150, loss: 0.029870419462409106\n",
      "iter: 2160, loss: 0.02981597820030241\n",
      "iter: 2170, loss: 0.02976164301370012\n",
      "iter: 2180, loss: 0.02970740365889183\n",
      "iter: 2190, loss: 0.02965324986732297\n",
      "iter: 2200, loss: 0.0295991713622878\n",
      "iter: 2210, loss: 0.029545157874874547\n",
      "iter: 2220, loss: 0.029491199158997793\n",
      "iter: 2230, loss: 0.029437285005376884\n",
      "iter: 2240, loss: 0.02938340525435173\n",
      "iter: 2250, loss: 0.02932954980743609\n",
      "iter: 2260, loss: 0.029275708637544393\n",
      "iter: 2270, loss: 0.029221871797840568\n",
      "iter: 2280, loss: 0.029168029429184367\n",
      "iter: 2290, loss: 0.029114171766164522\n",
      "iter: 2300, loss: 0.029060289141732348\n",
      "iter: 2310, loss: 0.029006371990461705\n",
      "iter: 2320, loss: 0.028952410850478245\n",
      "iter: 2330, loss: 0.028898396364111952\n",
      "iter: 2340, loss: 0.028844319277341577\n",
      "iter: 2350, loss: 0.028790170438105465\n",
      "iter: 2360, loss: 0.028735940793563355\n",
      "iter: 2370, loss: 0.028681621386398605\n",
      "iter: 2380, loss: 0.028627203350254826\n",
      "iter: 2390, loss: 0.028572677904400864\n",
      "iter: 2400, loss: 0.028518036347723562\n",
      "iter: 2410, loss: 0.028463270052144506\n",
      "iter: 2420, loss: 0.0284083704555522\n",
      "iter: 2430, loss: 0.02835332905434776\n",
      "iter: 2440, loss: 0.028298137395685704\n",
      "iter: 2450, loss: 0.028242787069499373\n",
      "iter: 2460, loss: 0.028187269700385542\n",
      "iter: 2470, loss: 0.02813157693942294\n",
      "iter: 2480, loss: 0.02807570045599464\n",
      "iter: 2490, loss: 0.028019631929670236\n",
      "iter: 2500, loss: 0.02796336304220786\n",
      "iter: 2510, loss: 0.02790688546972471\n",
      "iter: 2520, loss: 0.02785019087507703\n",
      "iter: 2530, loss: 0.027793270900490517\n",
      "iter: 2540, loss: 0.027736117160473885\n",
      "iter: 2550, loss: 0.027678721235042822\n",
      "iter: 2560, loss: 0.02762107466327776\n",
      "iter: 2570, loss: 0.027563168937238387\n",
      "iter: 2580, loss: 0.02750499549624833\n",
      "iter: 2590, loss: 0.027446545721569807\n",
      "iter: 2600, loss: 0.027387810931471045\n",
      "iter: 2610, loss: 0.027328782376707515\n",
      "iter: 2620, loss: 0.027269451236419027\n",
      "iter: 2630, loss: 0.02720980861445384\n",
      "iter: 2640, loss: 0.027149845536128595\n",
      "iter: 2650, loss: 0.02708955294543129\n",
      "iter: 2660, loss: 0.02702892170268161\n",
      "iter: 2670, loss: 0.02696794258265066\n",
      "iter: 2680, loss: 0.02690660627316467\n",
      "iter: 2690, loss: 0.026844903374196098\n",
      "iter: 2700, loss: 0.026782824397463257\n",
      "iter: 2710, loss: 0.026720359766560862\n",
      "iter: 2720, loss: 0.026657499817636477\n",
      "iter: 2730, loss: 0.026594234800643365\n",
      "iter: 2740, loss: 0.026530554881195934\n",
      "iter: 2750, loss: 0.026466450143058033\n",
      "iter: 2760, loss: 0.026401910591303028\n",
      "iter: 2770, loss: 0.026336926156178227\n",
      "iter: 2780, loss: 0.026271486697724052\n",
      "iter: 2790, loss: 0.026205582011188073\n",
      "iter: 2800, loss: 0.02613920183328687\n",
      "iter: 2810, loss: 0.02607233584937137\n",
      "iter: 2820, loss: 0.026004973701554183\n",
      "iter: 2830, loss: 0.02593710499785651\n",
      "iter: 2840, loss: 0.025868719322452067\n",
      "iter: 2850, loss: 0.025799806247067823\n",
      "iter: 2860, loss: 0.02573035534362454\n",
      "iter: 2870, loss: 0.025660356198190383\n",
      "iter: 2880, loss: 0.025589798426334808\n",
      "iter: 2890, loss: 0.025518671689966944\n",
      "iter: 2900, loss: 0.02544696571574393\n",
      "iter: 2910, loss: 0.025374670315149082\n",
      "iter: 2920, loss: 0.025301775406328553\n",
      "iter: 2930, loss: 0.025228271037785903\n",
      "iter: 2940, loss: 0.025154147414031353\n",
      "iter: 2950, loss: 0.025079394923286535\n",
      "iter: 2960, loss: 0.025004004167342032\n",
      "iter: 2970, loss: 0.02492796599366619\n",
      "iter: 2980, loss: 0.024851271529860015\n",
      "iter: 2990, loss: 0.024773912220551957\n",
      "iter: 3000, loss: 0.024695879866816522\n",
      "iter: 3010, loss: 0.02461716666820085\n",
      "iter: 3020, loss: 0.02453776526742692\n",
      "iter: 3030, loss: 0.024457668797832472\n",
      "iter: 3040, loss: 0.024376870933597177\n",
      "iter: 3050, loss: 0.02429536594278752\n",
      "iter: 3060, loss: 0.024213148743228004\n",
      "iter: 3070, loss: 0.02413021496119482\n",
      "iter: 3080, loss: 0.024046560992893186\n",
      "iter: 3090, loss: 0.02396218406865372\n",
      "iter: 3100, loss: 0.023877082319751115\n",
      "iter: 3110, loss: 0.023791254847705376\n",
      "iter: 3120, loss: 0.02370470179589272\n",
      "iter: 3130, loss: 0.02361742442323272\n",
      "iter: 3140, loss: 0.02352942517968538\n",
      "iter: 3150, loss: 0.023440707783212764\n",
      "iter: 3160, loss: 0.02335127729782141\n",
      "iter: 3170, loss: 0.02326114021222109\n",
      "iter: 3180, loss: 0.023170304518573104\n",
      "iter: 3190, loss: 0.023078779790728438\n",
      "iter: 3200, loss: 0.02298657726128096\n",
      "iter: 3210, loss: 0.022893709896682192\n",
      "iter: 3220, loss: 0.022800192469592105\n",
      "iter: 3230, loss: 0.02270604162755651\n",
      "iter: 3240, loss: 0.022611275957039146\n",
      "iter: 3250, loss: 0.022515916041757095\n",
      "iter: 3260, loss: 0.02241998451421575\n",
      "iter: 3270, loss: 0.02232350609927957\n",
      "iter: 3280, loss: 0.02222650764858358\n",
      "iter: 3290, loss: 0.02212901816455503\n",
      "iter: 3300, loss: 0.022031068812819764\n",
      "iter: 3310, loss: 0.021932692921772252\n",
      "iter: 3320, loss: 0.021833925968131683\n",
      "iter: 3330, loss: 0.02173480554737039\n",
      "iter: 3340, loss: 0.021635371327996623\n",
      "iter: 3350, loss: 0.021535664988795656\n",
      "iter: 3360, loss: 0.021435730138293817\n",
      "iter: 3370, loss: 0.02133561221590773\n",
      "iter: 3380, loss: 0.021235358374450727\n",
      "iter: 3390, loss: 0.021135017343936406\n",
      "iter: 3400, loss: 0.021034639276901617\n",
      "iter: 3410, loss: 0.02093427557577024\n",
      "iter: 3420, loss: 0.020833978703120877\n",
      "iter: 3430, loss: 0.020733801976046316\n",
      "iter: 3440, loss: 0.02063379934614722\n",
      "iter: 3450, loss: 0.020534025167033818\n",
      "iter: 3460, loss: 0.020434533951535723\n",
      "iter: 3470, loss: 0.020335380121119673\n",
      "iter: 3480, loss: 0.02023661775026747\n",
      "iter: 3490, loss: 0.020138300308796233\n",
      "iter: 3500, loss: 0.020040480405244563\n",
      "iter: 3510, loss: 0.019943209534555598\n",
      "iter: 3520, loss: 0.01984653783330239\n",
      "iter: 3530, loss: 0.019750513845654384\n",
      "iter: 3540, loss: 0.01965518430314895\n",
      "iter: 3550, loss: 0.019560593921128588\n",
      "iter: 3560, loss: 0.019466785214424746\n",
      "iter: 3570, loss: 0.019373798334527723\n",
      "iter: 3580, loss: 0.019281670930079247\n",
      "iter: 3590, loss: 0.01919043803207813\n",
      "iter: 3600, loss: 0.01910013196471901\n",
      "iter: 3610, loss: 0.019010782282289834\n",
      "iter: 3620, loss: 0.018922415732058495\n",
      "iter: 3630, loss: 0.01883505624260739\n",
      "iter: 3640, loss: 0.01874872493662094\n",
      "iter: 3650, loss: 0.01866344016672614\n",
      "iter: 3660, loss: 0.0185792175726356\n",
      "iter: 3670, loss: 0.01849607015754414\n",
      "iter: 3680, loss: 0.018414008381508286\n",
      "iter: 3690, loss: 0.018333040269383497\n",
      "iter: 3700, loss: 0.018253171530806956\n",
      "iter: 3710, loss: 0.018174405689696842\n",
      "iter: 3720, loss: 0.018096744220780753\n",
      "iter: 3730, loss: 0.018020186690773946\n",
      "iter: 3740, loss: 0.01794473090196828\n",
      "iter: 3750, loss: 0.017870373036182486\n",
      "iter: 3760, loss: 0.017797107797241713\n",
      "iter: 3770, loss: 0.017724928550382994\n",
      "iter: 3780, loss: 0.017653827457233717\n",
      "iter: 3790, loss: 0.017583795605246918\n",
      "iter: 3800, loss: 0.017514823130729707\n",
      "iter: 3810, loss: 0.01744689933480987\n",
      "iter: 3820, loss: 0.017380012791914735\n",
      "iter: 3830, loss: 0.01731415145051261\n",
      "iter: 3840, loss: 0.0172493027260377\n",
      "iter: 3850, loss: 0.01718545358606378\n",
      "iter: 3860, loss: 0.01712259062790728\n",
      "iter: 3870, loss: 0.01706070014893426\n",
      "iter: 3880, loss: 0.016999768209921322\n",
      "iter: 3890, loss: 0.016939780691871156\n",
      "iter: 3900, loss: 0.016880723346715476\n",
      "iter: 3910, loss: 0.016822581842358617\n",
      "iter: 3920, loss: 0.016765341802521944\n",
      "iter: 3930, loss: 0.01670898884183935\n",
      "iter: 3940, loss: 0.016653508596647076\n",
      "iter: 3950, loss: 0.01659888675188072\n",
      "iter: 3960, loss: 0.016545109064480274\n",
      "iter: 3970, loss: 0.01649216138366741\n",
      "iter: 3980, loss: 0.01644002966843356\n",
      "iter: 3990, loss: 0.01638870000254654\n",
      "iter: 4000, loss: 0.016338158607360116\n",
      "iter: 4010, loss: 0.016288391852675006\n",
      "iter: 4020, loss: 0.01623938626587958\n",
      "iter: 4030, loss: 0.016191128539573798\n",
      "iter: 4040, loss: 0.01614360553785413\n",
      "iter: 4050, loss: 0.016096804301422723\n",
      "iter: 4060, loss: 0.016050712051660327\n",
      "iter: 4070, loss: 0.016005316193792585\n",
      "iter: 4080, loss: 0.015960604319256975\n",
      "iter: 4090, loss: 0.015916564207375326\n",
      "iter: 4100, loss: 0.015873183826415418\n",
      "iter: 4110, loss: 0.01583045133412639\n",
      "iter: 4120, loss: 0.01578835507781161\n",
      "iter: 4130, loss: 0.015746883594006455\n",
      "iter: 4140, loss: 0.015706025607816122\n",
      "iter: 4150, loss: 0.015665770031961547\n",
      "iter: 4160, loss: 0.015626105965581692\n",
      "iter: 4170, loss: 0.01558702269282937\n",
      "iter: 4180, loss: 0.015548509681294918\n",
      "iter: 4190, loss: 0.015510556580294302\n",
      "iter: 4200, loss: 0.015473153219044215\n",
      "iter: 4210, loss: 0.015436289604750545\n",
      "iter: 4220, loss: 0.015399955920632083\n",
      "iter: 4230, loss: 0.015364142523895886\n",
      "iter: 4240, loss: 0.01532883994368252\n",
      "iter: 4250, loss: 0.015294038878990592\n",
      "iter: 4260, loss: 0.015259730196595779\n",
      "iter: 4270, loss: 0.01522590492896853\n",
      "iter: 4280, loss: 0.015192554272200932\n",
      "iter: 4290, loss: 0.015159669583945731\n",
      "iter: 4300, loss: 0.015127242381371997\n",
      "iter: 4310, loss: 0.015095264339140217\n",
      "iter: 4320, loss: 0.015063727287395112\n",
      "iter: 4330, loss: 0.015032623209780781\n",
      "iter: 4340, loss: 0.015001944241473384\n",
      "iter: 4350, loss: 0.01497168266723182\n",
      "iter: 4360, loss: 0.01494183091946546\n",
      "iter: 4370, loss: 0.014912381576313066\n",
      "iter: 4380, loss: 0.014883327359733302\n",
      "iter: 4390, loss: 0.014854661133605026\n",
      "iter: 4400, loss: 0.014826375901829754\n",
      "iter: 4410, loss: 0.014798464806437333\n",
      "iter: 4420, loss: 0.014770921125690523\n",
      "iter: 4430, loss: 0.014743738272188018\n",
      "iter: 4440, loss: 0.014716909790958577\n",
      "iter: 4450, loss: 0.014690429357548404\n",
      "iter: 4460, loss: 0.014664290776099941\n",
      "iter: 4470, loss: 0.014638487977416157\n",
      "iter: 4480, loss: 0.014613015017013807\n",
      "iter: 4490, loss: 0.0145878660731621\n",
      "iter: 4500, loss: 0.01456303544490816\n",
      "iter: 4510, loss: 0.014538517550084937\n",
      "iter: 4520, loss: 0.014514306923308013\n",
      "iter: 4530, loss: 0.014490398213955332\n",
      "iter: 4540, loss: 0.01446678618413387\n",
      "iter: 4550, loss: 0.014443465706634795\n",
      "iter: 4560, loss: 0.014420431762876485\n",
      "iter: 4570, loss: 0.014397679440836744\n",
      "iter: 4580, loss: 0.014375203932978836\n",
      "iter: 4590, loss: 0.014353000534169201\n",
      "iter: 4600, loss: 0.014331064639591015\n",
      "iter: 4610, loss: 0.014309391742656722\n",
      "iter: 4620, loss: 0.014287977432918824\n",
      "iter: 4630, loss: 0.014266817393984737\n",
      "iter: 4640, loss: 0.014245907401434938\n",
      "iter: 4650, loss: 0.014225243320748138\n",
      "iter: 4660, loss: 0.014204821105237174\n",
      "iter: 4670, loss: 0.01418463679399577\n",
      "iter: 4680, loss: 0.014164686509860018\n",
      "iter: 4690, loss: 0.014144966457385761\n",
      "iter: 4700, loss: 0.014125472920845582\n",
      "iter: 4710, loss: 0.014106202262246475\n",
      "iter: 4720, loss: 0.014087150919370224\n",
      "iter: 4730, loss: 0.014068315403840395\n",
      "iter: 4740, loss: 0.014049692299213096\n",
      "iter: 4750, loss: 0.014031278259101136\n",
      "iter: 4760, loss: 0.014013070005324493\n",
      "iter: 4770, loss: 0.013995064326093872\n",
      "iter: 4780, loss: 0.013977258074228432\n",
      "iter: 4790, loss: 0.013959648165407753\n",
      "iter: 4800, loss: 0.013942231576459439\n",
      "iter: 4810, loss: 0.013925005343682413\n",
      "iter: 4820, loss: 0.013907966561209397\n",
      "iter: 4830, loss: 0.01389111237940699\n",
      "iter: 4840, loss: 0.013874440003313447\n",
      "iter: 4850, loss: 0.013857946691119097\n",
      "iter: 4860, loss: 0.013841629752682797\n",
      "iter: 4870, loss: 0.01382548654809113\n",
      "iter: 4880, loss: 0.013809514486256096\n",
      "iter: 4890, loss: 0.013793711023554393\n",
      "iter: 4900, loss: 0.013778073662506458\n",
      "iter: 4910, loss: 0.013762599950495195\n",
      "iter: 4920, loss: 0.013747287478525187\n",
      "iter: 4930, loss: 0.013732133880020016\n",
      "iter: 4940, loss: 0.013717136829660633\n",
      "iter: 4950, loss: 0.013702294042262756\n",
      "iter: 4960, loss: 0.013687603271688076\n",
      "iter: 4970, loss: 0.013673062309798666\n",
      "iter: 4980, loss: 0.013658668985444475\n",
      "iter: 4990, loss: 0.013644421163489562\n",
      "iter: 5000, loss: 0.013630316743871664\n",
      "iter: 5010, loss: 0.013616353660697434\n",
      "iter: 5020, loss: 0.01360252988137238\n",
      "iter: 5030, loss: 0.013588843405761782\n",
      "iter: 5040, loss: 0.013575292265386282\n",
      "iter: 5050, loss: 0.01356187452264734\n",
      "iter: 5060, loss: 0.013548588270083747\n",
      "iter: 5070, loss: 0.013535431629656623\n",
      "iter: 5080, loss: 0.013522402752064594\n",
      "iter: 5090, loss: 0.013509499816086108\n",
      "iter: 5100, loss: 0.0134967210279474\n",
      "iter: 5110, loss: 0.013484064620717684\n",
      "iter: 5120, loss: 0.013471528853728898\n",
      "iter: 5130, loss: 0.013459112012019062\n",
      "iter: 5140, loss: 0.013446812405799214\n",
      "iter: 5150, loss: 0.01343462836994219\n",
      "iter: 5160, loss: 0.013422558263492337\n",
      "iter: 5170, loss: 0.013410600469196407\n",
      "iter: 5180, loss: 0.013398753393054571\n",
      "iter: 5190, loss: 0.013387015463887115\n",
      "iter: 5200, loss: 0.013375385132922956\n",
      "iter: 5210, loss: 0.013363860873401944\n",
      "iter: 5220, loss: 0.013352441180195688\n",
      "iter: 5230, loss: 0.01334112456944262\n",
      "iter: 5240, loss: 0.013329909578197406\n",
      "iter: 5250, loss: 0.013318794764096064\n",
      "iter: 5260, loss: 0.013307778705033543\n",
      "iter: 5270, loss: 0.01329685999885113\n",
      "iter: 5280, loss: 0.013286037263041388\n",
      "iter: 5290, loss: 0.013275309134459075\n",
      "iter: 5300, loss: 0.013264674269045408\n",
      "iter: 5310, loss: 0.013254131341562236\n",
      "iter: 5320, loss: 0.013243679045335115\n",
      "iter: 5330, loss: 0.013233316092005\n",
      "iter: 5340, loss: 0.013223041211288932\n",
      "iter: 5350, loss: 0.0132128531507475\n",
      "iter: 5360, loss: 0.013202750675561996\n",
      "iter: 5370, loss: 0.01319273256831672\n",
      "iter: 5380, loss: 0.013182797628786337\n",
      "iter: 5390, loss: 0.013172944673732633\n",
      "iter: 5400, loss: 0.01316317253670403\n",
      "iter: 5410, loss: 0.013153480067843188\n",
      "iter: 5420, loss: 0.013143866133695512\n",
      "iter: 5430, loss: 0.01313432961702549\n",
      "iter: 5440, loss: 0.013124869416635723\n",
      "iter: 5450, loss: 0.013115484447190637\n",
      "iter: 5460, loss: 0.01310617363904298\n",
      "iter: 5470, loss: 0.013096935938063853\n",
      "iter: 5480, loss: 0.013087770305475773\n",
      "iter: 5490, loss: 0.013078675717688996\n",
      "iter: 5500, loss: 0.013069651166139309\n",
      "iter: 5510, loss: 0.013060695657129041\n",
      "iter: 5520, loss: 0.013051808211670159\n",
      "iter: 5530, loss: 0.013042987865328346\n",
      "iter: 5540, loss: 0.013034233668070884\n",
      "iter: 5550, loss: 0.013025544684114014\n",
      "iter: 5560, loss: 0.01301691999177203\n",
      "iter: 5570, loss: 0.013008358683308574\n",
      "iter: 5580, loss: 0.012999859864789375\n",
      "iter: 5590, loss: 0.012991422655934481\n",
      "iter: 5600, loss: 0.012983046189973433\n",
      "iter: 5610, loss: 0.012974729613498697\n",
      "iter: 5620, loss: 0.012966472086322114\n",
      "iter: 5630, loss: 0.012958272781331649\n",
      "iter: 5640, loss: 0.01295013088434858\n",
      "iter: 5650, loss: 0.01294204559398386\n",
      "iter: 5660, loss: 0.012934016121496791\n",
      "iter: 5670, loss: 0.012926041690653593\n",
      "iter: 5680, loss: 0.012918121537586404\n",
      "iter: 5690, loss: 0.01291025491065163\n",
      "iter: 5700, loss: 0.012902441070290644\n",
      "iter: 5710, loss: 0.012894679288888858\n",
      "iter: 5720, loss: 0.01288696885063645\n",
      "iter: 5730, loss: 0.012879309051388134\n",
      "iter: 5740, loss: 0.012871699198524074\n",
      "iter: 5750, loss: 0.012864138610810994\n",
      "iter: 5760, loss: 0.012856626618263223\n",
      "iter: 5770, loss: 0.012849162562004024\n",
      "iter: 5780, loss: 0.01284174579412744\n",
      "iter: 5790, loss: 0.012834375677559963\n",
      "iter: 5800, loss: 0.01282705158592283\n",
      "iter: 5810, loss: 0.012819772903394713\n",
      "iter: 5820, loss: 0.012812539024574196\n",
      "iter: 5830, loss: 0.012805349354342972\n",
      "iter: 5840, loss: 0.012798203307729463\n",
      "iter: 5850, loss: 0.012791100309772492\n",
      "iter: 5860, loss: 0.012784039795385364\n",
      "iter: 5870, loss: 0.012777021209221464\n",
      "iter: 5880, loss: 0.012770044005536753\n",
      "iter: 5890, loss: 0.012763107648058039\n",
      "iter: 5900, loss: 0.012756211609847891\n",
      "iter: 5910, loss: 0.012749355373170238\n",
      "iter: 5920, loss: 0.012742538429359069\n",
      "iter: 5930, loss: 0.012735760278684768\n",
      "iter: 5940, loss: 0.01272902043022249\n",
      "iter: 5950, loss: 0.012722318401721666\n",
      "iter: 5960, loss: 0.012715653719474232\n",
      "iter: 5970, loss: 0.012709025918185846\n",
      "iter: 5980, loss: 0.012702434540847272\n",
      "iter: 5990, loss: 0.012695879138604347\n",
      "iter: 6000, loss: 0.01268935927063141\n",
      "iter: 6010, loss: 0.012682874504004793\n",
      "iter: 6020, loss: 0.012676424413575879\n",
      "iter: 6030, loss: 0.01267000858184731\n",
      "iter: 6040, loss: 0.012663626598847457\n",
      "iter: 6050, loss: 0.01265727806200805\n",
      "iter: 6060, loss: 0.012650962576041288\n",
      "iter: 6070, loss: 0.012644679752819308\n",
      "iter: 6080, loss: 0.012638429211253204\n",
      "iter: 6090, loss: 0.012632210577173023\n",
      "iter: 6100, loss: 0.012626023483210468\n",
      "iter: 6110, loss: 0.012619867568681801\n",
      "iter: 6120, loss: 0.012613742479470511\n",
      "iter: 6130, loss: 0.012607647867912913\n",
      "iter: 6140, loss: 0.012601583392684532\n",
      "iter: 6150, loss: 0.01259554871868642\n",
      "iter: 6160, loss: 0.012589543516934575\n",
      "iter: 6170, loss: 0.012583567464448185\n",
      "iter: 6180, loss: 0.012577620244141904\n",
      "iter: 6190, loss: 0.01257170154471636\n",
      "iter: 6200, loss: 0.012565811060552063\n",
      "iter: 6210, loss: 0.012559948491603724\n",
      "iter: 6220, loss: 0.012554113543295567\n",
      "iter: 6230, loss: 0.012548305926418642\n",
      "iter: 6240, loss: 0.012542525357028553\n",
      "iter: 6250, loss: 0.012536771556345474\n",
      "iter: 6260, loss: 0.012531044250654415\n",
      "iter: 6270, loss: 0.012525343171206862\n",
      "iter: 6280, loss: 0.012519668054125717\n",
      "iter: 6290, loss: 0.012514018640307845\n",
      "iter: 6300, loss: 0.012508394675331895\n",
      "iter: 6310, loss: 0.012502795909363616\n",
      "iter: 6320, loss: 0.012497222097066536\n",
      "iter: 6330, loss: 0.01249167299751046\n",
      "iter: 6340, loss: 0.012486148374082883\n",
      "iter: 6350, loss: 0.012480647994402273\n",
      "iter: 6360, loss: 0.012475171630230946\n",
      "iter: 6370, loss: 0.012469719057390933\n",
      "iter: 6380, loss: 0.012464290055680855\n",
      "iter: 6390, loss: 0.01245888440879243\n",
      "iter: 6400, loss: 0.012453501904231244\n",
      "iter: 6410, loss: 0.012448142333235908\n",
      "iter: 6420, loss: 0.012442805490700957\n",
      "iter: 6430, loss: 0.012437491175099288\n",
      "iter: 6440, loss: 0.012432199188406217\n",
      "iter: 6450, loss: 0.012426929336026184\n",
      "iter: 6460, loss: 0.012421681426719337\n",
      "iter: 6470, loss: 0.012416455272529109\n",
      "iter: 6480, loss: 0.012411250688713156\n",
      "iter: 6490, loss: 0.012406067493672419\n",
      "iter: 6500, loss: 0.012400905508884792\n",
      "iter: 6510, loss: 0.012395764558837386\n",
      "iter: 6520, loss: 0.012390644470961663\n",
      "iter: 6530, loss: 0.012385545075568436\n",
      "iter: 6540, loss: 0.012380466205785881\n",
      "iter: 6550, loss: 0.012375407697496904\n",
      "iter: 6560, loss: 0.012370369389279226\n",
      "iter: 6570, loss: 0.012365351122345436\n",
      "iter: 6580, loss: 0.01236035274048487\n",
      "iter: 6590, loss: 0.012355374090006149\n",
      "iter: 6600, loss: 0.012350415019682193\n",
      "iter: 6610, loss: 0.012345475380693928\n",
      "iter: 6620, loss: 0.012340555026579072\n",
      "iter: 6630, loss: 0.012335653813176218\n",
      "iter: 6640, loss: 0.012330771598576049\n",
      "iter: 6650, loss: 0.012325908243069464\n",
      "iter: 6660, loss: 0.012321063609099044\n",
      "iter: 6670, loss: 0.012316237561210445\n",
      "iter: 6680, loss: 0.012311429966004853\n",
      "iter: 6690, loss: 0.012306640692093442\n",
      "iter: 6700, loss: 0.012301869610051334\n",
      "iter: 6710, loss: 0.012297116592374428\n",
      "iter: 6720, loss: 0.012292381513434755\n",
      "iter: 6730, loss: 0.012287664249439547\n",
      "iter: 6740, loss: 0.01228296467838891\n",
      "iter: 6750, loss: 0.012278282680036471\n",
      "iter: 6760, loss: 0.012273618135848841\n",
      "iter: 6770, loss: 0.012268970928968207\n",
      "iter: 6780, loss: 0.012264340944173457\n",
      "iter: 6790, loss: 0.012259728067844162\n",
      "iter: 6800, loss: 0.012255132187924513\n",
      "iter: 6810, loss: 0.012250553193888188\n",
      "iter: 6820, loss: 0.012245990976704306\n",
      "iter: 6830, loss: 0.012241445428803732\n",
      "iter: 6840, loss: 0.01223691644404686\n",
      "iter: 6850, loss: 0.012232403917690968\n",
      "iter: 6860, loss: 0.012227907746360435\n",
      "iter: 6870, loss: 0.01222342782801553\n",
      "iter: 6880, loss: 0.012218964061922792\n",
      "iter: 6890, loss: 0.012214516348627344\n",
      "iter: 6900, loss: 0.012210084589922905\n",
      "iter: 6910, loss: 0.012205668688826455\n",
      "iter: 6920, loss: 0.012201268549549995\n",
      "iter: 6930, loss: 0.012196884077475486\n",
      "iter: 6940, loss: 0.01219251517912923\n",
      "iter: 6950, loss: 0.01218816176215657\n",
      "iter: 6960, loss: 0.01218382373529875\n",
      "iter: 6970, loss: 0.012179501008368662\n",
      "iter: 6980, loss: 0.012175193492228589\n",
      "iter: 6990, loss: 0.01217090109876761\n",
      "iter: 7000, loss: 0.01216662374088007\n",
      "iter: 7010, loss: 0.012162361332444105\n",
      "iter: 7020, loss: 0.012158113788301641\n",
      "iter: 7030, loss: 0.012153881024238214\n",
      "iter: 7040, loss: 0.012149662956963117\n",
      "iter: 7050, loss: 0.01214545950409099\n",
      "iter: 7060, loss: 0.012141270584123261\n",
      "iter: 7070, loss: 0.01213709611642921\n",
      "iter: 7080, loss: 0.0121329360212302\n",
      "iter: 7090, loss: 0.012128790219581572\n",
      "iter: 7100, loss: 0.012124658633355476\n",
      "iter: 7110, loss: 0.012120541185226867\n",
      "iter: 7120, loss: 0.01211643779865564\n",
      "iter: 7130, loss: 0.012112348397872818\n",
      "iter: 7140, loss: 0.01210827290786441\n",
      "iter: 7150, loss: 0.012104211254358508\n",
      "iter: 7160, loss: 0.012100163363810246\n",
      "iter: 7170, loss: 0.012096129163388881\n",
      "iter: 7180, loss: 0.01209210858096315\n",
      "iter: 7190, loss: 0.012088101545090048\n",
      "iter: 7200, loss: 0.012084107985001579\n",
      "iter: 7210, loss: 0.012080127830592154\n",
      "iter: 7220, loss: 0.0120761610124068\n",
      "iter: 7230, loss: 0.012072207461630631\n",
      "iter: 7240, loss: 0.012068267110076239\n",
      "iter: 7250, loss: 0.012064339890174003\n",
      "iter: 7260, loss: 0.012060425734960247\n",
      "iter: 7270, loss: 0.01205652457806828\n",
      "iter: 7280, loss: 0.012052636353716829\n",
      "iter: 7290, loss: 0.012048760996701281\n",
      "iter: 7300, loss: 0.012044898442384686\n",
      "iter: 7310, loss: 0.012041048626687006\n",
      "iter: 7320, loss: 0.012037211486077514\n",
      "iter: 7330, loss: 0.012033386957565199\n",
      "iter: 7340, loss: 0.012029574978690253\n",
      "iter: 7350, loss: 0.012025775487517095\n",
      "iter: 7360, loss: 0.012021988422624147\n",
      "iter: 7370, loss: 0.0120182137230976\n",
      "iter: 7380, loss: 0.012014451328524148\n",
      "iter: 7390, loss: 0.012010701178982253\n",
      "iter: 7400, loss: 0.012006963215035067\n",
      "iter: 7410, loss: 0.01200323737772455\n",
      "iter: 7420, loss: 0.011999523608563526\n",
      "iter: 7430, loss: 0.011995821849529405\n",
      "iter: 7440, loss: 0.011992132043058068\n",
      "iter: 7450, loss: 0.011988454132037247\n",
      "iter: 7460, loss: 0.011984788059799937\n",
      "iter: 7470, loss: 0.011981133770119182\n",
      "iter: 7480, loss: 0.011977491207201787\n",
      "iter: 7490, loss: 0.011973860315682252\n",
      "iter: 7500, loss: 0.01197024104061852\n",
      "iter: 7510, loss: 0.011966633327485076\n",
      "iter: 7520, loss: 0.011963037122169153\n",
      "iter: 7530, loss: 0.011959452370963918\n",
      "iter: 7540, loss: 0.01195587902056442\n",
      "iter: 7550, loss: 0.011952317018063301\n",
      "iter: 7560, loss: 0.01194876631094441\n",
      "iter: 7570, loss: 0.011945226847080073\n",
      "iter: 7580, loss: 0.011941698574724456\n",
      "iter: 7590, loss: 0.01193818144251079\n",
      "iter: 7600, loss: 0.01193467539944624\n",
      "iter: 7610, loss: 0.011931180394907934\n",
      "iter: 7620, loss: 0.011927696378638528\n",
      "iter: 7630, loss: 0.011924223300742361\n",
      "iter: 7640, loss: 0.011920761111681897\n",
      "iter: 7650, loss: 0.011917309762272314\n",
      "iter: 7660, loss: 0.011913869203679877\n",
      "iter: 7670, loss: 0.011910439387416353\n",
      "iter: 7680, loss: 0.011907020265336195\n",
      "iter: 7690, loss: 0.01190361178963356\n",
      "iter: 7700, loss: 0.01190021391283759\n",
      "iter: 7710, loss: 0.011896826587809278\n",
      "iter: 7720, loss: 0.01189344976773862\n",
      "iter: 7730, loss: 0.011890083406141084\n",
      "iter: 7740, loss: 0.011886727456854672\n",
      "iter: 7750, loss: 0.011883381874035657\n",
      "iter: 7760, loss: 0.011880046612156645\n",
      "iter: 7770, loss: 0.011876721626002581\n",
      "iter: 7780, loss: 0.011873406870668773\n",
      "iter: 7790, loss: 0.01187010230155732\n",
      "iter: 7800, loss: 0.011866807874373842\n",
      "iter: 7810, loss: 0.011863523545125047\n",
      "iter: 7820, loss: 0.011860249270116162\n",
      "iter: 7830, loss: 0.011856985005948131\n",
      "iter: 7840, loss: 0.011853730709513441\n",
      "iter: 7850, loss: 0.011850486337995994\n",
      "iter: 7860, loss: 0.011847251848866326\n",
      "iter: 7870, loss: 0.011844027199879845\n",
      "iter: 7880, loss: 0.011840812349074135\n",
      "iter: 7890, loss: 0.011837607254766255\n",
      "iter: 7900, loss: 0.011834411875551084\n",
      "iter: 7910, loss: 0.011831226170297374\n",
      "iter: 7920, loss: 0.011828050098146626\n",
      "iter: 7930, loss: 0.011824883618510147\n",
      "iter: 7940, loss: 0.011821726691066237\n",
      "iter: 7950, loss: 0.011818579275758878\n",
      "iter: 7960, loss: 0.011815441332794836\n",
      "iter: 7970, loss: 0.011812312822640805\n",
      "iter: 7980, loss: 0.011809193706021957\n",
      "iter: 7990, loss: 0.01180608394391968\n",
      "iter: 8000, loss: 0.011802983497568574\n",
      "iter: 8010, loss: 0.011799892328455788\n",
      "iter: 8020, loss: 0.01179681039831723\n",
      "iter: 8030, loss: 0.01179373766913585\n",
      "iter: 8040, loss: 0.011790674103140069\n",
      "iter: 8050, loss: 0.011787619662801414\n",
      "iter: 8060, loss: 0.011784574310832172\n",
      "iter: 8070, loss: 0.011781538010183253\n",
      "iter: 8080, loss: 0.01177851072404281\n",
      "iter: 8090, loss: 0.011775492415834095\n",
      "iter: 8100, loss: 0.01177248304921249\n",
      "iter: 8110, loss: 0.01176948258806463\n",
      "iter: 8120, loss: 0.011766490996505731\n",
      "iter: 8130, loss: 0.01176350823887795\n",
      "iter: 8140, loss: 0.011760534279748608\n",
      "iter: 8150, loss: 0.011757569083907779\n",
      "iter: 8160, loss: 0.011754612616367177\n",
      "iter: 8170, loss: 0.011751664842356941\n",
      "iter: 8180, loss: 0.01174872572732526\n",
      "iter: 8190, loss: 0.011745795236935468\n",
      "iter: 8200, loss: 0.011742873337064622\n",
      "iter: 8210, loss: 0.011739959993801835\n",
      "iter: 8220, loss: 0.011737055173446163\n",
      "iter: 8230, loss: 0.011734158842504575\n",
      "iter: 8240, loss: 0.011731270967690471\n",
      "iter: 8250, loss: 0.011728391515922535\n",
      "iter: 8260, loss: 0.0117255204543216\n",
      "iter: 8270, loss: 0.01172265775020976\n",
      "iter: 8280, loss: 0.011719803371108698\n",
      "iter: 8290, loss: 0.011716957284738063\n",
      "iter: 8300, loss: 0.011714119459012773\n",
      "iter: 8310, loss: 0.011711289862042382\n",
      "iter: 8320, loss: 0.011708468462128896\n",
      "iter: 8330, loss: 0.011705655227765743\n",
      "iter: 8340, loss: 0.011702850127635226\n",
      "iter: 8350, loss: 0.011700053130606803\n",
      "iter: 8360, loss: 0.011697264205736486\n",
      "iter: 8370, loss: 0.01169448332226449\n",
      "iter: 8380, loss: 0.011691710449613747\n",
      "iter: 8390, loss: 0.011688945557388556\n",
      "iter: 8400, loss: 0.011686188615372024\n",
      "iter: 8410, loss: 0.011683439593525505\n",
      "iter: 8420, loss: 0.011680698461987077\n",
      "iter: 8430, loss: 0.0116779651910696\n",
      "iter: 8440, loss: 0.011675239751258934\n",
      "iter: 8450, loss: 0.011672522113212668\n",
      "iter: 8460, loss: 0.011669812247759074\n",
      "iter: 8470, loss: 0.011667110125894335\n",
      "iter: 8480, loss: 0.011664415718782533\n",
      "iter: 8490, loss: 0.011661728997753392\n",
      "iter: 8500, loss: 0.011659049934300858\n",
      "iter: 8510, loss: 0.01165637850008129\n",
      "iter: 8520, loss: 0.011653714666912774\n",
      "iter: 8530, loss: 0.011651058406773392\n",
      "iter: 8540, loss: 0.011648409691799238\n",
      "iter: 8550, loss: 0.011645768494283719\n",
      "iter: 8560, loss: 0.011643134786675731\n",
      "iter: 8570, loss: 0.011640508541578972\n",
      "iter: 8580, loss: 0.011637889731748776\n",
      "iter: 8590, loss: 0.011635278330093122\n",
      "iter: 8600, loss: 0.011632674309669173\n",
      "iter: 8610, loss: 0.011630077643683326\n",
      "iter: 8620, loss: 0.011627488305489514\n",
      "iter: 8630, loss: 0.011624906268587289\n",
      "iter: 8640, loss: 0.011622331506621524\n",
      "iter: 8650, loss: 0.011619763993379616\n",
      "iter: 8660, loss: 0.011617203702792263\n",
      "iter: 8670, loss: 0.011614650608929581\n",
      "iter: 8680, loss: 0.011612104686002649\n",
      "iter: 8690, loss: 0.01160956590836009\n",
      "iter: 8700, loss: 0.011607034250487454\n",
      "iter: 8710, loss: 0.011604509687006527\n",
      "iter: 8720, loss: 0.01160199219267311\n",
      "iter: 8730, loss: 0.011599481742376959\n",
      "iter: 8740, loss: 0.011596978311139419\n",
      "iter: 8750, loss: 0.011594481874113022\n",
      "iter: 8760, loss: 0.011591992406580086\n",
      "iter: 8770, loss: 0.011589509883951106\n",
      "iter: 8780, loss: 0.011587034281763983\n",
      "iter: 8790, loss: 0.011584565575683313\n",
      "iter: 8800, loss: 0.011582103741498255\n",
      "iter: 8810, loss: 0.01157964875512226\n",
      "iter: 8820, loss: 0.011577200592591036\n",
      "iter: 8830, loss: 0.011574759230062572\n",
      "iter: 8840, loss: 0.011572324643815546\n",
      "iter: 8850, loss: 0.011569896810247355\n",
      "iter: 8860, loss: 0.011567475705874136\n",
      "iter: 8870, loss: 0.01156506130732969\n",
      "iter: 8880, loss: 0.011562653591363162\n",
      "iter: 8890, loss: 0.011560252534839938\n",
      "iter: 8900, loss: 0.011557858114738731\n",
      "iter: 8910, loss: 0.011555470308151817\n",
      "iter: 8920, loss: 0.01155308909228372\n",
      "iter: 8930, loss: 0.011550714444449875\n",
      "iter: 8940, loss: 0.011548346342075603\n",
      "iter: 8950, loss: 0.01154598476269577\n",
      "iter: 8960, loss: 0.011543629683952452\n",
      "iter: 8970, loss: 0.011541281083595822\n",
      "iter: 8980, loss: 0.011538938939481923\n",
      "iter: 8990, loss: 0.011536603229571877\n",
      "iter: 9000, loss: 0.011534273931931373\n",
      "iter: 9010, loss: 0.011531951024729073\n",
      "iter: 9020, loss: 0.011529634486236331\n",
      "iter: 9030, loss: 0.011527324294826254\n",
      "iter: 9040, loss: 0.011525020428971862\n",
      "iter: 9050, loss: 0.011522722867246748\n",
      "iter: 9060, loss: 0.011520431588322914\n",
      "iter: 9070, loss: 0.011518146570970455\n",
      "iter: 9080, loss: 0.011515867794056472\n",
      "iter: 9090, loss: 0.011513595236544535\n",
      "iter: 9100, loss: 0.011511328877492812\n",
      "iter: 9110, loss: 0.011509068696055374\n",
      "iter: 9120, loss: 0.011506814671479475\n",
      "iter: 9130, loss: 0.011504566783105057\n",
      "iter: 9140, loss: 0.011502325010364559\n",
      "iter: 9150, loss: 0.011500089332781646\n",
      "iter: 9160, loss: 0.011497859729970129\n",
      "iter: 9170, loss: 0.011495636181634362\n",
      "iter: 9180, loss: 0.011493418667567467\n",
      "iter: 9190, loss: 0.011491207167650513\n",
      "iter: 9200, loss: 0.011489001661852004\n",
      "iter: 9210, loss: 0.011486802130228142\n",
      "iter: 9220, loss: 0.011484608552919757\n",
      "iter: 9230, loss: 0.011482420910154199\n",
      "iter: 9240, loss: 0.011480239182242733\n",
      "iter: 9250, loss: 0.011478063349580554\n",
      "iter: 9260, loss: 0.011475893392646428\n",
      "iter: 9270, loss: 0.01147372929200138\n",
      "iter: 9280, loss: 0.011471571028287865\n",
      "iter: 9290, loss: 0.01146941858222993\n",
      "iter: 9300, loss: 0.011467271934632076\n",
      "iter: 9310, loss: 0.011465131066378524\n",
      "iter: 9320, loss: 0.011462995958432469\n",
      "iter: 9330, loss: 0.011460866591835622\n",
      "iter: 9340, loss: 0.011458742947708095\n",
      "iter: 9350, loss: 0.01145662500724691\n",
      "iter: 9360, loss: 0.011454512751725548\n",
      "iter: 9370, loss: 0.011452406162493657\n",
      "iter: 9380, loss: 0.011450305220976504\n",
      "iter: 9390, loss: 0.011448209908674215\n",
      "iter: 9400, loss: 0.011446120207160761\n",
      "iter: 9410, loss: 0.011444036098083903\n",
      "iter: 9420, loss: 0.011441957563164781\n",
      "iter: 9430, loss: 0.01143988458419674\n",
      "iter: 9440, loss: 0.011437817143045351\n",
      "iter: 9450, loss: 0.011435755221647206\n",
      "iter: 9460, loss: 0.011433698802010455\n",
      "iter: 9470, loss: 0.011431647866212896\n",
      "iter: 9480, loss: 0.011429602396402625\n",
      "iter: 9490, loss: 0.0114275623747967\n",
      "iter: 9500, loss: 0.011425527783681012\n",
      "iter: 9510, loss: 0.01142349860540983\n",
      "iter: 9520, loss: 0.011421474822405082\n",
      "iter: 9530, loss: 0.011419456417156222\n",
      "iter: 9540, loss: 0.011417443372218988\n",
      "iter: 9550, loss: 0.011415435670215544\n",
      "iter: 9560, loss: 0.011413433293834247\n",
      "iter: 9570, loss: 0.011411436225828667\n",
      "iter: 9580, loss: 0.01140944444901709\n",
      "iter: 9590, loss: 0.01140745794628197\n",
      "iter: 9600, loss: 0.011405476700570068\n",
      "iter: 9610, loss: 0.011403500694892078\n",
      "iter: 9620, loss: 0.011401529912320837\n",
      "iter: 9630, loss: 0.011399564335992027\n",
      "iter: 9640, loss: 0.011397603949104117\n",
      "iter: 9650, loss: 0.011395648734917177\n",
      "iter: 9660, loss: 0.011393698676752445\n",
      "iter: 9670, loss: 0.011391753757992024\n",
      "iter: 9680, loss: 0.011389813962078978\n",
      "iter: 9690, loss: 0.011387879272515963\n",
      "iter: 9700, loss: 0.011385949672866321\n",
      "iter: 9710, loss: 0.011384025146752028\n",
      "iter: 9720, loss: 0.011382105677853836\n",
      "iter: 9730, loss: 0.011380191249911445\n",
      "iter: 9740, loss: 0.011378281846723312\n",
      "iter: 9750, loss: 0.011376377452144924\n",
      "iter: 9760, loss: 0.01137447805008986\n",
      "iter: 9770, loss: 0.01137258362452818\n",
      "iter: 9780, loss: 0.011370694159487498\n",
      "iter: 9790, loss: 0.0113688096390512\n",
      "iter: 9800, loss: 0.011366930047359221\n",
      "iter: 9810, loss: 0.011365055368607125\n",
      "iter: 9820, loss: 0.011363185587045492\n",
      "iter: 9830, loss: 0.011361320686980696\n",
      "iter: 9840, loss: 0.011359460652773353\n",
      "iter: 9850, loss: 0.01135760546883847\n",
      "iter: 9860, loss: 0.01135575511964563\n",
      "iter: 9870, loss: 0.01135390958971791\n",
      "iter: 9880, loss: 0.011352068863631512\n",
      "iter: 9890, loss: 0.011350232926016767\n",
      "iter: 9900, loss: 0.011348401761555842\n",
      "iter: 9910, loss: 0.011346575354984184\n",
      "iter: 9920, loss: 0.011344753691089091\n",
      "iter: 9930, loss: 0.011342936754710417\n",
      "iter: 9940, loss: 0.011341124530739016\n",
      "iter: 9950, loss: 0.011339317004118029\n",
      "iter: 9960, loss: 0.01133751415984091\n",
      "iter: 9970, loss: 0.011335715982952499\n",
      "iter: 9980, loss: 0.011333922458548099\n",
      "iter: 9990, loss: 0.011332133571773378\n",
      "iter: 10000, loss: 0.011330349307824209\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "N = 16\n",
    "K = 8\n",
    "D = 128\n",
    "\n",
    "seed = 0\n",
    "rng  = np.random.default_rng(seed)\n",
    "fm   = FactorizationMachines(N, K, seed=seed)\n",
    "x    = rng.choice((0, 1), size=(D, N))\n",
    "\n",
    "Q    = rng.uniform(-1., 1., (N, N))\n",
    "y    = np.einsum('dn,nm,dm->d', x, Q, x)\n",
    "\n",
    "_, loss_hist = Main.train_fm_als(\n",
    "    fm.params, x, y, fm.predict(x), 10000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.scatter(range(len(loss_hist))[::200], loss_hist[::200], s=50, marker='o', color='none', edgecolors='black')\n",
    "plt.plot(loss_hist, color='black', linestyle='dotted')\n",
    "plt.yscale('log')\n",
    "plt.xlabel(r'Iteration $t$')\n",
    "plt.ylabel(r'$\\dfrac{1}{D} \\sum_{d} (y^{(d)} - f^{(d)})^2$')\n",
    "plt.title('Loss History of FM (ALS)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
