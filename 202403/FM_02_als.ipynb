{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02. Alternating Least Squares for Factorization Machine\n",
    "\n",
    "$\\DeclareMathOperator*{\\argmin}{arg~min}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import Optional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ordinary Least Squares (OLS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume that we have a model that is first-order for each parameter $\\theta \\in \\Theta$ and can be written as\n",
    "\n",
    "$$\n",
    "    y_\\theta^{(d)} = \\theta x_\\theta^{(d)} + \\varepsilon^{(d)},\n",
    "$$\n",
    "\n",
    "where\n",
    "- $y_\\theta^{(d)}$ is the output variable,\n",
    "- $x_\\theta^{(d)}$ is the input variable,\n",
    "- $\\theta$ is the model parameter, and\n",
    "- $\\varepsilon^{(d)}$ is the error term.\n",
    "\n",
    "We want to estimate $\\theta$ from the observed data $\\{(x_\\theta^{(d)}, y_\\theta^{(d)})\\}_{d=1, \\dots, D}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If a model has a single parameter, $\\Theta = \\{ \\theta \\}$, we can estimate $\\theta$ by minimizing the sum of squared errors (SSE):\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\hat \\theta\n",
    "&= \\argmin_\\theta \\sum_{d=1}^D \\left( y_\\theta^{(d)} - \\theta x_\\theta^{(d)} \\right)^2,\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "whose result is given by\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\hat \\theta\n",
    "&= (x_\\theta^\\top x_\\theta)^{-1} x_\\theta^\\top y_\\theta \\\\\n",
    "&= \\left( \\sum_{d=1}^D x_\\theta^{(d)2} \\right)^{-1} \\sum_{d=1}^D x_\\theta^{(d)} y_\\theta^{(d)}.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "This is called the **ordinary least squares** (OLS) method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternating Least Squares (ALS)\n",
    "\n",
    "On the other hand, if we have an $M$-parameter models, i.e., $\\Theta = \\{ \\theta_m \\}_{m=1,\\dots,M}$, and the model is first-order with respect to each $\\theta_m$, we *alternatively* apply the least squares method to each parameter $\\theta_m$:\n",
    "\n",
    "1. repeat until convergence:\n",
    "   1. repeat for all $\\theta_m \\in \\Theta$:\n",
    "      1. fix all the other parameters $\\Theta \\setminus \\{\\theta_m\\}$\n",
    "      2. estimate $\\theta_m$ by minimizing the SSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, let our model given by"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "y^{(d)}\n",
    "&= f^{(d)} + \\varepsilon^{(d)},\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "f^{(d)} = \\sum_{i=1}^N \\sum_{j=1}^N \\sum_{k=1}^K v_{ik} v_{jk} x_i^{(d)} x_j^{(d)}.\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model has $NK$ parameters $\\Theta = \\{ v_{ik} \\}_{(i,k) \\in [N] \\times [K]}$ and first-order with respect to each parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can estimate $v_{ik}$ by minimizing the SSE:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "\\hat v_{ik}\n",
    "&= \\argmin_{v_{ik}} \\sum_{d=1}^D \\left( y^{(d)} - \\sum_{i=1}^N \\sum_{j=1}^N \\sum_{k=1}^K v_{ik} v_{jk} x_i^{(d)} x_j^{(d)} \\right)^2.\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we write"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "x_{v_{ik}}^{(d)}\n",
    "&\\coloneqq\n",
    "\\frac{\\partial f^{(d)}}{\\partial v_{ik}},\n",
    "\\\\\n",
    "\n",
    "g_{v_{ik}}^{(d)}\n",
    "&\\coloneqq\n",
    "f^{(d)} - v_{ik} h_{v_{ik}}^{(d)},\n",
    "\\\\\n",
    "\n",
    "y_{v_{ik}}^{(d)}\n",
    "&\\coloneqq\n",
    "y^{(d)} - g_{v_{ik}}^{(d)},\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the estimator of $v_{ik}$ is given by"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "\\hat v_{ik}\n",
    "&=\n",
    "\\argmin_{v_{ik}} \\sum_{d=1}^D \\left( y^{(d)} - (v_{ik} x_{v_{ik}}^{(d)} + g_{v_{ik}}^{(d)}) \\right)^2\n",
    "\\\\\n",
    "\n",
    "&=\n",
    "\\argmin_{v_{ik}} \\sum_{d=1}^D \\left( (y^{(d)} - g_{v_{ik}}^{(d)}) - v_{ik} x_{v_{ik}}^{(d)} \\right)^2\n",
    "\\\\\n",
    "\n",
    "&=\n",
    "\\argmin_{v_{ik}} \\sum_{d=1}^D \\left( y_{v_{ik}}^{(d)} - v_{ik} x_{v_{ik}}^{(d)} \\right)^2\n",
    "\\\\\n",
    "\n",
    "&=\n",
    "\\left( \\sum_{d=1}^D x_{v_{ik}}^{(d)2} \\right)^{-1} \\sum_{d=1}^D x_{v_{ik}}^{(d)} y_{v_ik}^{(d)}.\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By simply applying above repeatedly for all $v_{ik}$ until convergence, we can estimate all the parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is how the **alternating least squares (ALS)** method works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ALS for FM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, if a model is given by"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "y_\\theta^{(d)}\n",
    "&= \\theta x_\\theta^{(d)} + \\varepsilon^{(d)},\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The OLS-estimated $\\theta$ is given as"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "\\hat \\theta\n",
    "\n",
    "&=\n",
    "\\left( \\sum_{d=1}^D x_\\theta^{(d)2} \\right)^{-1} \\sum_{d=1}^D x_\\theta^{(d)} y_\\theta^{(d)}.\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have written in the previous notebook, by defining $x_\\theta^{(d)}$ and $y_\\theta^{(d)}$ as"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "x_\\theta^{(d)} &\\coloneqq \\frac{\\partial f(x^{(d)})}{\\partial \\theta}, \\\\\n",
    "y_\\theta^{(d)} &\\coloneqq y^{(d)} - (f^{(d)} - \\theta x_\\theta^{(d)}),\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "an FM can be written as the form given above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeating above for all $\\theta \\in \\Theta$ gives us the ALS method for the FM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we can speed up the computation by using the update rules written in `FM_01_linear.ipynb`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the use of naive OLS may result in division by zero, it is common to use L2 regularization as an alternative:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\hat \\theta\n",
    "\n",
    "&=\n",
    "\\left( \\sum_{d=1}^D x_\\theta^{(d)2} + \\lambda \\right)^{-1} \\sum_{d=1}^D x_\\theta^{(d)} y_\\theta^{(d)}.\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Factorization Machine\n",
    "\n",
    "class FactorizationMachines:\n",
    "    def __init__(self,\n",
    "        num_features: int,\n",
    "        num_factors:  int,\n",
    "        sigma_b_init: float=0.,\n",
    "        sigma_w_init: float=1.,\n",
    "        sigma_v_init: float=1.,\n",
    "        seed: Optional[int]=None\n",
    "    ) -> None:\n",
    "        self.rng = np.random.default_rng(seed)\n",
    "        b = self.rng.normal(0, sigma_b_init)\n",
    "        w = self.rng.normal(0, sigma_w_init, num_features)\n",
    "        v = self.rng.normal(0, sigma_v_init, (num_features, num_factors))\n",
    "        self.params = {'b': b, 'w': w, 'v': v}\n",
    "\n",
    "    def predict(self, x: np.ndarray) -> float:\n",
    "        if x.ndim == 1:\n",
    "            x = x.reshape(1, -1) # x: (d, n)\n",
    "        b = self.params['b']     # b: (1)\n",
    "        w = self.params['w']     # w: (d)\n",
    "        v = self.params['v']     # v: (d, k)\n",
    "\n",
    "        bias   = b\n",
    "            # (1)\n",
    "        linear = x[:, :] @ w[:]\n",
    "            # (D, N) @ (N) = (D)\n",
    "        inter  = 0.5 * np.sum((x[:, :] @ v[:, :]) ** 2 - (x[:, :] ** 2) @ (v[:, :] ** 2), axis=1)\n",
    "            # (D, K) -> (D)\n",
    "\n",
    "        result = bias + linear + inter\n",
    "            # (D)\n",
    "\n",
    "        if result.shape[0] == 1:\n",
    "            return float(result[0])\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update rules\n",
    "\n",
    "def calc_q_init(\n",
    "    x: np.ndarray,\n",
    "    v: np.ndarray\n",
    ") -> np.ndarray:\n",
    "    # x: (D, N)\n",
    "    # v: (N, K)\n",
    "    return x[:, :] @ v[:, :] # (D, K)\n",
    "\n",
    "def calc_dq(\n",
    "    i: int,\n",
    "    x: np.ndarray,\n",
    "    v_ik_new: float,\n",
    "    v_ik_old: np.ndarray,\n",
    ") -> np.ndarray:\n",
    "    # v_ik_new: float\n",
    "    # v: (N, K)\n",
    "    # x: (D, N)\n",
    "    return (v_ik_new - v_ik_old) * x[:, i] # (D)\n",
    "\n",
    "def calc_df(\n",
    "    x_theta: np.ndarray,\n",
    "    param_new: float,\n",
    "    param_old: float,\n",
    "):\n",
    "    return (param_new - param_old) * x_theta\n",
    "\n",
    "def calc_xy_b(\n",
    "    f: np.ndarray,\n",
    "    b: float,\n",
    "    x_data: np.ndarray,\n",
    "    y_data: np.ndarray,\n",
    "):\n",
    "    # x_data: (D, N)\n",
    "    # y_data: (D)\n",
    "    x_b = np.ones(x_data.shape[0])\n",
    "    y_b = y_data - (f - b * x_b)\n",
    "    return x_b, y_b\n",
    "\n",
    "def calc_xy_w(\n",
    "    f: np.ndarray,\n",
    "    w: np.ndarray,\n",
    "    x_data: np.ndarray,\n",
    "    y_data: np.ndarray,\n",
    "    i: int\n",
    "):\n",
    "    # x_data: (D, N)\n",
    "    # y_data: (D)\n",
    "    x_w = x_data[:, i]\n",
    "    y_w = y_data - (f - x_w * w[i])\n",
    "    return x_w, y_w\n",
    "\n",
    "def calc_xy_v(\n",
    "    f: np.ndarray,\n",
    "    q: np.ndarray,\n",
    "    v: np.ndarray,\n",
    "    x_data: np.ndarray,\n",
    "    y_data: np.ndarray,\n",
    "    i: int,\n",
    "    k: int\n",
    "):\n",
    "    # x_data: (D, N)\n",
    "    # y_data: (D)\n",
    "    x_v = x_data[:, i] * (q[:, k] - x_data[:, i] * v[i, k])\n",
    "    y_v = y_data - (f - x_v * v[i, k])\n",
    "    return x_v, y_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# least squares\n",
    "\n",
    "def sample_param_lstsq(\n",
    "    x_theta: np.ndarray,\n",
    "    y_theta: np.ndarray,\n",
    "    lamb: float=1e-8\n",
    ") -> float:\n",
    "    return np.sum(x_theta * y_theta) / (np.sum(x_theta ** 2) + lamb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training loop\n",
    "\n",
    "def train_fm_als(\n",
    "    init_params: dict,\n",
    "    x_data: np.ndarray,\n",
    "    y_data: np.ndarray,\n",
    "    f_init: np.ndarray,\n",
    "    num_iter: int,\n",
    ") -> dict:\n",
    "    # get indices\n",
    "    N = x_data.shape[1]\n",
    "    K = init_params['v'].shape[1]\n",
    "\n",
    "    # get initial parameter\n",
    "    params = init_params\n",
    "\n",
    "    # precalculate\n",
    "    f = f_init\n",
    "    q = calc_q_init(x_data, params['v'])\n",
    "\n",
    "    # main loop\n",
    "    for iter in range(num_iter):\n",
    "        # sample b\n",
    "        x_b, y_b = calc_xy_b(f, params['b'], x_data, y_data)\n",
    "        b_new    = sample_param_lstsq(x_b, y_b)\n",
    "        f        = f + calc_df(x_b, b_new, params['b'])\n",
    "        params['b'] = b_new\n",
    "\n",
    "        # sample w\n",
    "        for i in range(N):\n",
    "            x_w, y_w = calc_xy_w(f, params['w'], x_data, y_data, i)\n",
    "            w_i_new  = sample_param_lstsq(x_w, y_w)\n",
    "            f        = f + calc_df(x_w, w_i_new, params['w'][i])\n",
    "            params['w'][i] = w_i_new\n",
    "\n",
    "        # sample v\n",
    "        # for i in range(N):\n",
    "            for k in range(K):\n",
    "                x_v, y_v = calc_xy_v(f, q, params['v'], x_data, y_data, i, k)\n",
    "                v_ik_new = sample_param_lstsq(x_v, y_v)\n",
    "                f        = f      + calc_df(x_v, v_ik_new, params['v'][i, k])\n",
    "                q[:,k]   = q[:,k] + calc_dq(i, x_data, v_ik_new, params['v'][i, k])\n",
    "                params['v'][i, k] = v_ik_new\n",
    "\n",
    "        if iter % 10 == 0:\n",
    "            print(f'iter: {iter}, loss: {np.sum((y_data - f) ** 2) / N}')\n",
    "\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "N = 16\n",
    "K = 8\n",
    "D = 128\n",
    "\n",
    "seed = 0\n",
    "rng  = np.random.default_rng(seed)\n",
    "fm   = FactorizationMachines(N, K, seed=seed)\n",
    "x    = rng.choice((0, 1), size=(D, N))\n",
    "\n",
    "Q    = rng.uniform(-1., 1., (N, N))\n",
    "y    = np.einsum('dn,nm,dm->d', x, Q, x)\n",
    "\n",
    "_ = train_fm_als(\n",
    "    fm.params, x, y, fm.predict(x), 1000\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems pretty good. Now implement above in Julia to make it superfast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import julia\n",
    "# julia.install()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from julia import Main\n",
    "\n",
    "julia_code = \"\"\"\n",
    "function calc_q_init(\n",
    "    x::Array{Float64},\n",
    "    v::Array{Float64}\n",
    ") :: Array{Float64}\n",
    "    # x: (D, N)\n",
    "    # v: (N, K)\n",
    "    return x[:, :] * v[:, :] # (D, K)\n",
    "end\n",
    "\n",
    "function calc_dq(\n",
    "    i::Int,\n",
    "    x::Array{Float64},\n",
    "    v_ik_new::Float64,\n",
    "    v_ik_old::Float64\n",
    ") :: Array{Float64, 1}\n",
    "    # v_ik_new: float\n",
    "    # v: (N, K)\n",
    "    # x: (D, N)\n",
    "    return (v_ik_new - v_ik_old) .* x[:, i] # (D)\n",
    "end\n",
    "\n",
    "function calc_df(\n",
    "    x_theta::Array{Float64},\n",
    "    param_new::Float64,\n",
    "    param_old::Float64\n",
    ")\n",
    "    return (param_new - param_old) .* x_theta\n",
    "end\n",
    "\n",
    "function calc_xy_b(\n",
    "    f::Array{Float64, 1},\n",
    "    b::Float64,\n",
    "    x_data::Array{Float64},\n",
    "    y_data::Array{Float64, 1}\n",
    ")\n",
    "    # x_data: (D, N)\n",
    "    # y_data: (D)\n",
    "    x_b = ones(size(x_data, 1))\n",
    "    y_b = y_data - (f - b .* x_b)\n",
    "    return x_b, y_b\n",
    "end\n",
    "\n",
    "function calc_xy_w(\n",
    "    f::Array{Float64, 1},\n",
    "    w::Array{Float64, 1},\n",
    "    x_data::Array{Float64},\n",
    "    y_data::Array{Float64, 1},\n",
    "    i::Int\n",
    ")\n",
    "    # x_data: (D, N)\n",
    "    # y_data: (D)\n",
    "    x_w = x_data[:, i]\n",
    "    y_w = y_data - (f - x_w .* w[i])\n",
    "    return x_w, y_w\n",
    "end\n",
    "\n",
    "function calc_xy_v(\n",
    "    f::Array{Float64, 1},\n",
    "    q::Array{Float64},\n",
    "    v::Array{Float64},\n",
    "    x_data::Array{Float64},\n",
    "    y_data::Array{Float64, 1},\n",
    "    i::Int,\n",
    "    k::Int\n",
    ")\n",
    "    # x_data: (D, N)\n",
    "    # y_data: (D)\n",
    "    x_v = x_data[:, i] .* (q[:, k] - x_data[:, i] .* v[i, k])\n",
    "    y_v = y_data - (f - x_v .* v[i, k])\n",
    "    return x_v, y_v\n",
    "end\n",
    "\n",
    "function sample_param_lstsq(\n",
    "    x_theta::Array{Float64},\n",
    "    y_theta::Array{Float64},\n",
    "    lamb::Float64=1e-8\n",
    ") :: Float64\n",
    "    return sum(x_theta .* y_theta) / (sum(x_theta .^ 2) + lamb)\n",
    "end\n",
    "\n",
    "function train_fm_als(\n",
    "    init_params::Dict{Any, Any},\n",
    "    x_data::Array{Float64},\n",
    "    y_data::Array{Float64, 1},\n",
    "    f_init::Array{Float64, 1},\n",
    "    num_iter::Int\n",
    ") :: Tuple{Dict{Any, Any}, Array{Float64}}\n",
    "    # get indices\n",
    "    N = size(x_data, 2)\n",
    "    K = size(init_params[\"v\"], 2)\n",
    "\n",
    "    # get initial parameter\n",
    "    params = init_params\n",
    "\n",
    "    # precalculate\n",
    "    f = f_init\n",
    "    q = calc_q_init(x_data, params[\"v\"])\n",
    "\n",
    "    # main loop\n",
    "    loss_hist = Float64[]\n",
    "    for iter in 1:num_iter\n",
    "        # sample b\n",
    "        x_b, y_b = calc_xy_b(f, params[\"b\"], x_data, y_data)\n",
    "        b_new    = sample_param_lstsq(x_b, y_b)\n",
    "        f        = f + calc_df(x_b, b_new, params[\"b\"])\n",
    "        params[\"b\"] = b_new\n",
    "\n",
    "        # sample w\n",
    "        for i in 1:N\n",
    "            x_w, y_w = calc_xy_w(f, params[\"w\"], x_data, y_data, i)\n",
    "            w_i_new  = sample_param_lstsq(x_w, y_w)\n",
    "            f        = f + calc_df(x_w, w_i_new, params[\"w\"][i])\n",
    "            params[\"w\"][i] = w_i_new\n",
    "\n",
    "            # sample v\n",
    "            for k in 1:K\n",
    "                x_v, y_v = calc_xy_v(f, q, params[\"v\"], x_data, y_data, i, k)\n",
    "                v_ik_new = sample_param_lstsq(x_v, y_v)\n",
    "                f        = f      + calc_df(x_v, v_ik_new, params[\"v\"][i, k])\n",
    "                q[:,k]   = q[:,k] + calc_dq(i, x_data, v_ik_new, params[\"v\"][i, k])\n",
    "                params[\"v\"][i, k] = v_ik_new\n",
    "            end\n",
    "        end\n",
    "\n",
    "        if iter % 10 == 0\n",
    "            println(\"iter: $iter, loss: $(sum((y_data - f) .^ 2) / N)\")\n",
    "        end\n",
    "\n",
    "        push!(loss_hist, sum((y_data - f) .^ 2) / N)\n",
    "    end\n",
    "\n",
    "    return params, loss_hist\n",
    "end\n",
    "\"\"\"\n",
    "\n",
    "Main.eval(julia_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "N = 16\n",
    "K = 8\n",
    "D = 128\n",
    "\n",
    "seed = 0\n",
    "rng  = np.random.default_rng(seed)\n",
    "fm   = FactorizationMachines(N, K, seed=seed)\n",
    "x    = rng.choice((0, 1), size=(D, N))\n",
    "\n",
    "Q    = rng.uniform(-1., 1., (N, N))\n",
    "y    = np.einsum('dn,nm,dm->d', x, Q, x)\n",
    "\n",
    "_, loss_hist = Main.train_fm_als(\n",
    "    fm.params, x.astype(float), y, fm.predict(x), 10000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "# plt.scatter(range(len(loss_hist))[::200], loss_hist[::200], s=50, marker='o', color='none', edgecolors='black')\n",
    "plt.plot(loss_hist, color='black')\n",
    "plt.yscale('log')\n",
    "plt.xlabel(r'Iteration $t$')\n",
    "plt.ylabel(r'$\\dfrac{1}{D} \\sum_{d} (y^{(d)} - f^{(d)})^2$')\n",
    "plt.title('Loss History of FM (ALS)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
