{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\DeclareMathOperator*{\\argmin}{arg~min}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02. Alternating Least Squares for Factorization Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import Optional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ordinary Least Squares (OLS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume that we have a model that is first-order for each parameter $\\theta \\in \\Theta$ and can be written as\n",
    "\n",
    "$$\n",
    "    y_\\theta^{(d)} = \\theta x_\\theta^{(d)} + \\varepsilon^{(d)},\n",
    "$$\n",
    "\n",
    "where\n",
    "- $y_\\theta^{(d)}$ is the output variable,\n",
    "- $x_\\theta^{(d)}$ is the input variable,\n",
    "- $\\theta$ is the model parameter, and\n",
    "- $\\varepsilon^{(d)}$ is the error term.\n",
    "\n",
    "We want to estimate $\\theta$ from the observed data $\\{(x_\\theta^{(d)}, y_\\theta^{(d)})\\}_{d=1, \\dots, D}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If a model has a single parameter, $\\Theta = \\{ \\theta \\}$, we can estimate $\\theta$ by minimizing the sum of squared errors (SSE):\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\hat \\theta\n",
    "&= \\argmin_\\theta \\sum_{d=1}^D \\left( y_\\theta^{(d)} - \\theta x_\\theta^{(d)} \\right)^2,\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "whose result is given by\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\hat \\theta\n",
    "&= (x_\\theta^\\top x_\\theta)^{-1} x_\\theta^\\top y_\\theta \\\\\n",
    "&= \\left( \\sum_{d=1}^D x_\\theta^{(d)2} \\right)^{-1} \\sum_{d=1}^D x_\\theta^{(d)} y_\\theta^{(d)}.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "This is called the **ordinary least squares** (OLS) method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternating Least Squares (ALS)\n",
    "\n",
    "On the other hand, if we have an $M$-parameter models, i.e., $\\Theta = \\{ \\theta_m \\}_{m=1,\\dots,M}$, and the model is first-order with respect to each $\\theta_m$, we *alternatively* apply the least squares method to each parameter $\\theta_m$:\n",
    "\n",
    "1. repeat until convergence:\n",
    "   1. repeat for all $\\theta_m \\in \\Theta$:\n",
    "      1. fix all the other parameters $\\Theta \\setminus \\{\\theta_m\\}$\n",
    "      2. estimate $\\theta_m$ by minimizing the SSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, let our model given by"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "y^{(d)}\n",
    "&= f^{(d)} + \\varepsilon^{(d)},\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "f^{(d)} = \\sum_{i=1}^N \\sum_{j=1}^N \\sum_{k=1}^K v_{ik} v_{jk} x_i^{(d)} x_j^{(d)}.\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model has $NK$ parameters $\\Theta = \\{ v_{ik} \\}_{(i,k) \\in [N] \\times [K]}$ and first-order with respect to each parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can estimate $v_{ik}$ by minimizing the SSE:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "\\hat v_{ik}\n",
    "&= \\argmin_{v_{ik}} \\sum_{d=1}^D \\left( y^{(d)} - \\sum_{i=1}^N \\sum_{j=1}^N \\sum_{k=1}^K v_{ik} v_{jk} x_i^{(d)} x_j^{(d)} \\right)^2.\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we write"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "x_{v_{ik}}^{(d)}\n",
    "&\\coloneqq\n",
    "\\frac{\\partial f^{(d)}}{\\partial v_{ik}},\n",
    "\\\\\n",
    "\n",
    "g_{v_{ik}}^{(d)}\n",
    "&\\coloneqq\n",
    "f^{(d)} - v_{ik} h_{v_{ik}}^{(d)},\n",
    "\\\\\n",
    "\n",
    "y_{v_{ik}}^{(d)}\n",
    "&\\coloneqq\n",
    "y^{(d)} - g_{v_{ik}}^{(d)},\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the estimator of $v_{ik}$ is given by"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "\\hat v_{ik}\n",
    "&=\n",
    "\\argmin_{v_{ik}} \\sum_{d=1}^D \\left( y^{(d)} - (v_{ik} x_{v_{ik}}^{(d)} + g_{v_{ik}}^{(d)}) \\right)^2\n",
    "\\\\\n",
    "\n",
    "&=\n",
    "\\argmin_{v_{ik}} \\sum_{d=1}^D \\left( (y^{(d)} - g_{v_{ik}}^{(d)}) - v_{ik} x_{v_{ik}}^{(d)} \\right)^2\n",
    "\\\\\n",
    "\n",
    "&=\n",
    "\\argmin_{v_{ik}} \\sum_{d=1}^D \\left( y_{v_{ik}}^{(d)} - v_{ik} x_{v_{ik}}^{(d)} \\right)^2\n",
    "\\\\\n",
    "\n",
    "&=\n",
    "\\left( \\sum_{d=1}^D x_{v_{ik}}^{(d)2} \\right)^{-1} \\sum_{d=1}^D x_{v_{ik}}^{(d)} y_{v_ik}^{(d)}.\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By simply applying above repeatedly for all $v_{ik}$ until convergence, we can estimate all the parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is how the **alternating least squares (ALS)** method works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ALS for FM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, if a model is given by"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "y_\\theta^{(d)}\n",
    "&= \\theta x_\\theta^{(d)} + \\varepsilon^{(d)},\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The OLS-estimated $\\theta$ is given as"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "\\hat \\theta\n",
    "\n",
    "&=\n",
    "\\left( \\sum_{d=1}^D x_\\theta^{(d)2} \\right)^{-1} \\sum_{d=1}^D x_\\theta^{(d)} y_\\theta^{(d)}.\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have written in the previous notebook, by defining $x_\\theta^{(d)}$ and $y_\\theta^{(d)}$ as"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "x_\\theta^{(d)} &\\coloneqq \\frac{\\partial f(x^{(d)})}{\\partial \\theta}, \\\\\n",
    "y_\\theta^{(d)} &\\coloneqq y^{(d)} - (f^{(d)} - \\theta x_\\theta^{(d)}),\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "an FM can be written as the form given above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeating above for all $\\theta \\in \\Theta$ gives us the ALS method for the FM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we can speed up the computation by using the update rules written in `FM_01_linear.ipynb`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the use of naive OLS may result in division by zero, it is common to use L2 regularization as an alternative:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\hat \\theta\n",
    "\n",
    "&=\n",
    "\\left( \\sum_{d=1}^D x_\\theta^{(d)2} + \\lambda \\right)^{-1} \\sum_{d=1}^D x_\\theta^{(d)} y_\\theta^{(d)}.\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Factorization Machine\n",
    "\n",
    "class FactorizationMachines:\n",
    "    def __init__(self,\n",
    "        num_features: int,\n",
    "        num_factors:  int,\n",
    "        sigma_b_init: float=0.,\n",
    "        sigma_w_init: float=1.,\n",
    "        sigma_v_init: float=1.,\n",
    "        seed: Optional[int]=None\n",
    "    ) -> None:\n",
    "        self.rng = np.random.default_rng(seed)\n",
    "        b = self.rng.normal(0, sigma_b_init)\n",
    "        w = self.rng.normal(0, sigma_w_init, num_features)\n",
    "        v = self.rng.normal(0, sigma_v_init, (num_features, num_factors))\n",
    "        self.params = {'b': b, 'w': w, 'v': v}\n",
    "\n",
    "    def predict(self, x: np.ndarray) -> float:\n",
    "        if x.ndim == 1:\n",
    "            x = x.reshape(1, -1) # x: (d, n)\n",
    "        b = self.params['b']     # b: (1)\n",
    "        w = self.params['w']     # w: (d)\n",
    "        v = self.params['v']     # v: (d, k)\n",
    "\n",
    "        bias   = b\n",
    "            # (1)\n",
    "        linear = x[:, :] @ w[:]\n",
    "            # (D, N) @ (N) = (D)\n",
    "        inter  = 0.5 * np.sum((x[:, :] @ v[:, :]) ** 2 - (x[:, :] ** 2) @ (v[:, :] ** 2), axis=1)\n",
    "            # (D, K) -> (D)\n",
    "\n",
    "        result = bias + linear + inter\n",
    "            # (D)\n",
    "\n",
    "        if result.shape[0] == 1:\n",
    "            return float(result[0])\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update rules\n",
    "\n",
    "def calc_q_init(\n",
    "    x: np.ndarray,\n",
    "    v: np.ndarray\n",
    ") -> np.ndarray:\n",
    "    # x: (D, N)\n",
    "    # v: (N, K)\n",
    "    return x[:, :] @ v[:, :] # (D, K)\n",
    "\n",
    "def calc_dq(\n",
    "    i: int,\n",
    "    x: np.ndarray,\n",
    "    v_ik_new: float,\n",
    "    v_ik_old: np.ndarray,\n",
    ") -> np.ndarray:\n",
    "    # v_ik_new: float\n",
    "    # v: (N, K)\n",
    "    # x: (D, N)\n",
    "    return (v_ik_new - v_ik_old) * x[:, i] # (D)\n",
    "\n",
    "def calc_df(\n",
    "    x_theta: np.ndarray,\n",
    "    param_new: float,\n",
    "    param_old: float,\n",
    "):\n",
    "    return (param_new - param_old) * x_theta\n",
    "\n",
    "def calc_xy_b(\n",
    "    f: np.ndarray,\n",
    "    b: float,\n",
    "    x_data: np.ndarray,\n",
    "    y_data: np.ndarray,\n",
    "):\n",
    "    # x_data: (D, N)\n",
    "    # y_data: (D)\n",
    "    x_b = np.ones(x_data.shape[0])\n",
    "    y_b = y_data - (f - b * x_b)\n",
    "    return x_b, y_b\n",
    "\n",
    "def calc_xy_w(\n",
    "    f: np.ndarray,\n",
    "    w: np.ndarray,\n",
    "    x_data: np.ndarray,\n",
    "    y_data: np.ndarray,\n",
    "    i: int\n",
    "):\n",
    "    # x_data: (D, N)\n",
    "    # y_data: (D)\n",
    "    x_w = x_data[:, i]\n",
    "    y_w = y_data - (f - x_w * w[i])\n",
    "    return x_w, y_w\n",
    "\n",
    "def calc_xy_v(\n",
    "    f: np.ndarray,\n",
    "    q: np.ndarray,\n",
    "    v: np.ndarray,\n",
    "    x_data: np.ndarray,\n",
    "    y_data: np.ndarray,\n",
    "    i: int,\n",
    "    k: int\n",
    "):\n",
    "    # x_data: (D, N)\n",
    "    # y_data: (D)\n",
    "    x_v = x_data[:, i] * (q[:, k] - x_data[:, i] * v[i, k])\n",
    "    y_v = y_data - (f - x_v * v[i, k])\n",
    "    return x_v, y_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# least squares\n",
    "\n",
    "def sample_param_lstsq(\n",
    "    x_theta: np.ndarray,\n",
    "    y_theta: np.ndarray,\n",
    "    lamb: float=1e-8\n",
    ") -> float:\n",
    "    return np.sum(x_theta * y_theta) / (np.sum(x_theta ** 2) + lamb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training loop\n",
    "\n",
    "def train_fm_als(\n",
    "    init_params: dict,\n",
    "    x_data: np.ndarray,\n",
    "    y_data: np.ndarray,\n",
    "    f_init: np.ndarray,\n",
    "    num_iter: int,\n",
    ") -> dict:\n",
    "    # get indices\n",
    "    N = x_data.shape[1]\n",
    "    K = init_params['v'].shape[1]\n",
    "\n",
    "    # get initial parameter\n",
    "    params = init_params\n",
    "\n",
    "    # precalculate\n",
    "    f = f_init\n",
    "    q = calc_q_init(x_data, params['v'])\n",
    "\n",
    "    # main loop\n",
    "    for iter in range(num_iter):\n",
    "        # sample b\n",
    "        x_b, y_b = calc_xy_b(f, params['b'], x_data, y_data)\n",
    "        b_new    = sample_param_lstsq(x_b, y_b)\n",
    "        f        = f + calc_df(x_b, b_new, params['b'])\n",
    "        params['b'] = b_new\n",
    "\n",
    "        # sample w\n",
    "        for i in range(N):\n",
    "            x_w, y_w = calc_xy_w(f, params['w'], x_data, y_data, i)\n",
    "            w_i_new  = sample_param_lstsq(x_w, y_w)\n",
    "            f        = f + calc_df(x_w, w_i_new, params['w'][i])\n",
    "            params['w'][i] = w_i_new\n",
    "\n",
    "        # sample v\n",
    "        # for i in range(N):\n",
    "            for k in range(K):\n",
    "                x_v, y_v = calc_xy_v(f, q, params['v'], x_data, y_data, i, k)\n",
    "                v_ik_new = sample_param_lstsq(x_v, y_v)\n",
    "                f        = f      + calc_df(x_v, v_ik_new, params['v'][i, k])\n",
    "                q[:,k]   = q[:,k] + calc_dq(i, x_data, v_ik_new, params['v'][i, k])\n",
    "                params['v'][i, k] = v_ik_new\n",
    "\n",
    "        if iter % 10 == 0:\n",
    "            print(f'iter: {iter}, loss: {np.sum((y_data - f) ** 2) / N}')\n",
    "\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter: 0, loss: 179.06365484724637\n",
      "iter: 10, loss: 8.98418157723689\n",
      "iter: 20, loss: 4.15654310358396\n",
      "iter: 30, loss: 2.8908957246632117\n",
      "iter: 40, loss: 2.2471760504729454\n",
      "iter: 50, loss: 1.834554991649325\n",
      "iter: 60, loss: 1.5472759216427807\n",
      "iter: 70, loss: 1.3392619820764988\n",
      "iter: 80, loss: 1.182538092147513\n",
      "iter: 90, loss: 1.0583190812594754\n",
      "iter: 100, loss: 0.95468503683955\n",
      "iter: 110, loss: 0.8647007765165866\n",
      "iter: 120, loss: 0.7846202500640986\n",
      "iter: 130, loss: 0.7125412368283995\n",
      "iter: 140, loss: 0.647554803094041\n",
      "iter: 150, loss: 0.589246270073279\n",
      "iter: 160, loss: 0.5373976855327314\n",
      "iter: 170, loss: 0.4918017361028244\n",
      "iter: 180, loss: 0.45215391210418965\n",
      "iter: 190, loss: 0.4180148893243939\n",
      "iter: 200, loss: 0.38883055517474474\n",
      "iter: 210, loss: 0.36398436688561087\n",
      "iter: 220, loss: 0.34285440647111287\n",
      "iter: 230, loss: 0.3248571845956359\n",
      "iter: 240, loss: 0.30947280662154464\n",
      "iter: 250, loss: 0.2962542809598825\n",
      "iter: 260, loss: 0.28482643017630194\n",
      "iter: 270, loss: 0.2748792929468249\n",
      "iter: 280, loss: 0.26615933526829094\n",
      "iter: 290, loss: 0.25846037556349616\n",
      "iter: 300, loss: 0.2516151732638253\n",
      "iter: 310, loss: 0.24548807234059677\n",
      "iter: 320, loss: 0.23996879335433102\n",
      "iter: 330, loss: 0.23496732035019963\n",
      "iter: 340, loss: 0.23040976376627376\n",
      "iter: 350, loss: 0.2262350586446748\n",
      "iter: 360, loss: 0.2223923577345705\n",
      "iter: 370, loss: 0.21883899017048664\n",
      "iter: 380, loss: 0.2155388721607841\n",
      "iter: 390, loss: 0.21246127321652197\n",
      "iter: 400, loss: 0.20957985811174795\n",
      "iter: 410, loss: 0.20687194005750512\n",
      "iter: 420, loss: 0.20431789403704825\n",
      "iter: 430, loss: 0.20190069071835826\n",
      "iter: 440, loss: 0.1996055208426597\n",
      "iter: 450, loss: 0.19741948761263406\n",
      "iter: 460, loss: 0.19533135057261536\n",
      "iter: 470, loss: 0.19333130902405538\n",
      "iter: 480, loss: 0.19141081640166135\n",
      "iter: 490, loss: 0.1895624194867559\n",
      "iter: 500, loss: 0.18777961806801471\n",
      "iter: 510, loss: 0.18605674185775484\n",
      "iter: 520, loss: 0.18438884228248276\n",
      "iter: 530, loss: 0.18277159730555714\n",
      "iter: 540, loss: 0.18120122779476266\n",
      "iter: 550, loss: 0.1796744241824604\n",
      "iter: 560, loss: 0.17818828232485348\n",
      "iter: 570, loss: 0.17674024758046578\n",
      "iter: 580, loss: 0.17532806621544855\n",
      "iter: 590, loss: 0.17394974331689925\n",
      "iter: 600, loss: 0.1726035064619309\n",
      "iter: 610, loss: 0.17128777445316307\n",
      "iter: 620, loss: 0.17000113049236265\n",
      "iter: 630, loss: 0.16874229922322936\n",
      "iter: 640, loss: 0.16751012713161254\n",
      "iter: 650, loss: 0.16630356584632983\n",
      "iter: 660, loss: 0.16512165793545114\n",
      "iter: 670, loss: 0.16396352484123172\n",
      "iter: 680, loss: 0.16282835664140088\n",
      "iter: 690, loss: 0.16171540336495557\n",
      "iter: 700, loss: 0.16062396762717301\n",
      "iter: 710, loss: 0.15955339838117527\n",
      "iter: 720, loss: 0.15850308561224524\n",
      "iter: 730, loss: 0.15747245582647199\n",
      "iter: 740, loss: 0.1564609682074535\n",
      "iter: 750, loss: 0.15546811133401436\n",
      "iter: 760, loss: 0.1544934003684192\n",
      "iter: 770, loss: 0.15353637463886588\n",
      "iter: 780, loss: 0.1525965955521219\n",
      "iter: 790, loss: 0.15167364478253326\n",
      "iter: 800, loss: 0.1507671226924696\n",
      "iter: 810, loss: 0.14987664694656688\n",
      "iter: 820, loss: 0.1490018512885025\n",
      "iter: 830, loss: 0.14814238445420555\n",
      "iter: 840, loss: 0.14729790919983707\n",
      "iter: 850, loss: 0.14646810142656985\n",
      "iter: 860, loss: 0.14565264938729647\n",
      "iter: 870, loss: 0.14485125296288556\n",
      "iter: 880, loss: 0.14406362299784337\n",
      "iter: 890, loss: 0.1432894806869402\n",
      "iter: 900, loss: 0.1425285570058561\n",
      "iter: 910, loss: 0.14178059218016736\n",
      "iter: 920, loss: 0.14104533518793164\n",
      "iter: 930, loss: 0.14032254329201893\n",
      "iter: 940, loss: 0.13961198159900523\n",
      "iter: 950, loss: 0.13891342264206802\n",
      "iter: 960, loss: 0.1382266459856698\n",
      "iter: 970, loss: 0.13755143785035903\n",
      "iter: 980, loss: 0.13688759075620313\n",
      "iter: 990, loss: 0.13623490318368348\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "N = 16\n",
    "K = 8\n",
    "D = 128\n",
    "\n",
    "seed = 0\n",
    "rng  = np.random.default_rng(seed)\n",
    "fm   = FactorizationMachines(N, K, seed=seed)\n",
    "x    = rng.choice((0, 1), size=(D, N))\n",
    "\n",
    "Q    = rng.uniform(-1., 1., (N, N))\n",
    "y    = np.einsum('dn,nm,dm->d', x, Q, x)\n",
    "\n",
    "_ = train_fm_als(\n",
    "    fm.params, x, y, fm.predict(x), 1000\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems pretty good. Now implement above in Julia to make it superfast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import julia\n",
    "# julia.install()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PyCall.jlwrap train_fm_als>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from julia import Main\n",
    "\n",
    "julia_code = \"\"\"\n",
    "function calc_q_init(\n",
    "    x::Array{Float64},\n",
    "    v::Array{Float64}\n",
    ") :: Array{Float64}\n",
    "    # x: (D, N)\n",
    "    # v: (N, K)\n",
    "    return x[:, :] * v[:, :] # (D, K)\n",
    "end\n",
    "\n",
    "function calc_dq(\n",
    "    i::Int,\n",
    "    x::Array{Float64},\n",
    "    v_ik_new::Float64,\n",
    "    v_ik_old::Float64\n",
    ") :: Array{Float64, 1}\n",
    "    # v_ik_new: float\n",
    "    # v: (N, K)\n",
    "    # x: (D, N)\n",
    "    return (v_ik_new - v_ik_old) .* x[:, i] # (D)\n",
    "end\n",
    "\n",
    "function calc_df(\n",
    "    x_theta::Array{Float64},\n",
    "    param_new::Float64,\n",
    "    param_old::Float64\n",
    ")\n",
    "    return (param_new - param_old) .* x_theta\n",
    "end\n",
    "\n",
    "function calc_xy_b(\n",
    "    f::Array{Float64, 1},\n",
    "    b::Float64,\n",
    "    x_data::Array{Float64},\n",
    "    y_data::Array{Float64, 1}\n",
    ")\n",
    "    # x_data: (D, N)\n",
    "    # y_data: (D)\n",
    "    x_b = ones(size(x_data, 1))\n",
    "    y_b = y_data - (f - b .* x_b)\n",
    "    return x_b, y_b\n",
    "end\n",
    "\n",
    "function calc_xy_w(\n",
    "    f::Array{Float64, 1},\n",
    "    w::Array{Float64, 1},\n",
    "    x_data::Array{Float64},\n",
    "    y_data::Array{Float64, 1},\n",
    "    i::Int\n",
    ")\n",
    "    # x_data: (D, N)\n",
    "    # y_data: (D)\n",
    "    x_w = x_data[:, i]\n",
    "    y_w = y_data - (f - x_w .* w[i])\n",
    "    return x_w, y_w\n",
    "end\n",
    "\n",
    "function calc_xy_v(\n",
    "    f::Array{Float64, 1},\n",
    "    q::Array{Float64},\n",
    "    v::Array{Float64},\n",
    "    x_data::Array{Float64},\n",
    "    y_data::Array{Float64, 1},\n",
    "    i::Int,\n",
    "    k::Int\n",
    ")\n",
    "    # x_data: (D, N)\n",
    "    # y_data: (D)\n",
    "    x_v = x_data[:, i] .* (q[:, k] - x_data[:, i] .* v[i, k])\n",
    "    y_v = y_data - (f - x_v .* v[i, k])\n",
    "    return x_v, y_v\n",
    "end\n",
    "\n",
    "function sample_param_lstsq(\n",
    "    x_theta::Array{Float64},\n",
    "    y_theta::Array{Float64},\n",
    "    lamb::Float64=1e-8\n",
    ") :: Float64\n",
    "    return sum(x_theta .* y_theta) / (sum(x_theta .^ 2) + lamb)\n",
    "end\n",
    "\n",
    "function train_fm_als(\n",
    "    init_params::Dict{Any, Any},\n",
    "    x_data::Array{Float64},\n",
    "    y_data::Array{Float64, 1},\n",
    "    f_init::Array{Float64, 1},\n",
    "    num_iter::Int\n",
    ") :: Tuple{Dict{Any, Any}, Array{Float64}}\n",
    "    # get indices\n",
    "    N = size(x_data, 2)\n",
    "    K = size(init_params[\"v\"], 2)\n",
    "\n",
    "    # get initial parameter\n",
    "    params = init_params\n",
    "\n",
    "    # precalculate\n",
    "    f = f_init\n",
    "    q = calc_q_init(x_data, params[\"v\"])\n",
    "\n",
    "    # main loop\n",
    "    loss_hist = Float64[]\n",
    "    for iter in 1:num_iter\n",
    "        # sample b\n",
    "        x_b, y_b = calc_xy_b(f, params[\"b\"], x_data, y_data)\n",
    "        b_new    = sample_param_lstsq(x_b, y_b)\n",
    "        f        = f + calc_df(x_b, b_new, params[\"b\"])\n",
    "        params[\"b\"] = b_new\n",
    "\n",
    "        # sample w\n",
    "        for i in 1:N\n",
    "            x_w, y_w = calc_xy_w(f, params[\"w\"], x_data, y_data, i)\n",
    "            w_i_new  = sample_param_lstsq(x_w, y_w)\n",
    "            f        = f + calc_df(x_w, w_i_new, params[\"w\"][i])\n",
    "            params[\"w\"][i] = w_i_new\n",
    "\n",
    "            # sample v\n",
    "            for k in 1:K\n",
    "                x_v, y_v = calc_xy_v(f, q, params[\"v\"], x_data, y_data, i, k)\n",
    "                v_ik_new = sample_param_lstsq(x_v, y_v)\n",
    "                f        = f      + calc_df(x_v, v_ik_new, params[\"v\"][i, k])\n",
    "                q[:,k]   = q[:,k] + calc_dq(i, x_data, v_ik_new, params[\"v\"][i, k])\n",
    "                params[\"v\"][i, k] = v_ik_new\n",
    "            end\n",
    "        end\n",
    "\n",
    "        if iter % 10 == 0\n",
    "            println(\"iter: $iter, loss: $(sum((y_data - f) .^ 2) / N)\")\n",
    "        end\n",
    "\n",
    "        push!(loss_hist, sum((y_data - f) .^ 2) / N)\n",
    "    end\n",
    "\n",
    "    return params, loss_hist\n",
    "end\n",
    "\"\"\"\n",
    "\n",
    "Main.eval(julia_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter: 10, loss: 10.215673503848897\n",
      "iter: 20, loss: 4.367602251928187\n",
      "iter: 30, loss: 2.977136059413109\n",
      "iter: 40, loss: 2.298381312389868\n",
      "iter: 50, loss: 1.8691419814416792\n",
      "iter: 60, loss: 1.5718730451402598\n",
      "iter: 70, loss: 1.357376367257618\n",
      "iter: 80, loss: 1.196497493302403\n",
      "iter: 90, loss: 1.0696634516616756\n",
      "iter: 100, loss: 0.9643501106736445\n",
      "iter: 110, loss: 0.873209745285615\n",
      "iter: 120, loss: 0.7922467517635035\n",
      "iter: 130, loss: 0.7194200820927555\n",
      "iter: 140, loss: 0.6537484025906307\n",
      "iter: 150, loss: 0.5947833919922167\n",
      "iter: 160, loss: 0.5422972323416191\n",
      "iter: 170, loss: 0.4960876025935615\n",
      "iter: 180, loss: 0.45586251155752455\n",
      "iter: 190, loss: 0.42119600119178563\n",
      "iter: 200, loss: 0.39154338222196544\n",
      "iter: 210, loss: 0.36629168043772103\n",
      "iter: 220, loss: 0.34481724612710857\n",
      "iter: 230, loss: 0.32653131177558914\n",
      "iter: 240, loss: 0.310906983829082\n",
      "iter: 250, loss: 0.29748990084902593\n",
      "iter: 260, loss: 0.2858979413586817\n",
      "iter: 270, loss: 0.2758150006462119\n",
      "iter: 280, loss: 0.2669823154840698\n",
      "iter: 290, loss: 0.2591893629118966\n",
      "iter: 300, loss: 0.25226535812025447\n",
      "iter: 310, loss: 0.246071784832336\n",
      "iter: 320, loss: 0.24049607329901493\n",
      "iter: 330, loss: 0.23544638241363997\n",
      "iter: 340, loss: 0.23084737121364388\n",
      "iter: 350, loss: 0.226636819996288\n",
      "iter: 360, loss: 0.22276295998578471\n",
      "iter: 370, loss: 0.21918238084275804\n",
      "iter: 380, loss: 0.2158584007756813\n",
      "iter: 390, loss: 0.21275980107340559\n",
      "iter: 400, loss: 0.20985984363456694\n",
      "iter: 410, loss: 0.20713550552430404\n",
      "iter: 420, loss: 0.2045668782480594\n",
      "iter: 430, loss: 0.20213669110224475\n",
      "iter: 440, loss: 0.1998299276386905\n",
      "iter: 450, loss: 0.1976335120822859\n",
      "iter: 460, loss: 0.19553604866514382\n",
      "iter: 470, loss: 0.1935276015221461\n",
      "iter: 480, loss: 0.191599506280749\n",
      "iter: 490, loss: 0.18974420701200886\n",
      "iter: 500, loss: 0.18795511400652024\n",
      "iter: 510, loss: 0.18622647908321888\n",
      "iter: 520, loss: 0.18455328598260826\n",
      "iter: 530, loss: 0.18293115395786944\n",
      "iter: 540, loss: 0.18135625304746114\n",
      "iter: 550, loss: 0.17982522975740628\n",
      "iter: 560, loss: 0.1783351420463569\n",
      "iter: 570, loss: 0.1768834026236112\n",
      "iter: 580, loss: 0.17546772965973395\n",
      "iter: 590, loss: 0.1740861040839829\n",
      "iter: 600, loss: 0.17273673270984588\n",
      "iter: 610, loss: 0.17141801649315191\n",
      "iter: 620, loss: 0.17012852328847913\n",
      "iter: 630, loss: 0.16886696452896088\n",
      "iter: 640, loss: 0.1676321753122199\n",
      "iter: 650, loss: 0.16642309743016656\n",
      "iter: 660, loss: 0.16523876493256773\n",
      "iter: 670, loss: 0.16407829186288717\n",
      "iter: 680, loss: 0.1629408618497856\n",
      "iter: 690, loss: 0.16182571927863762\n",
      "iter: 700, loss: 0.16073216180423824\n",
      "iter: 710, loss: 0.15965953399899846\n",
      "iter: 720, loss: 0.15860722196004406\n",
      "iter: 730, loss: 0.15757464872445398\n",
      "iter: 740, loss: 0.1565612703642799\n",
      "iter: 750, loss: 0.1555665726524705\n",
      "iter: 760, loss: 0.15459006820770668\n",
      "iter: 770, loss: 0.15363129404051534\n",
      "iter: 780, loss: 0.15268980943548233\n",
      "iter: 790, loss: 0.1517651941147802\n",
      "iter: 800, loss: 0.15085704663726443\n",
      "iter: 810, loss: 0.14996498299485267\n",
      "iter: 820, loss: 0.14908863537429576\n",
      "iter: 830, loss: 0.14822765105777563\n",
      "iter: 840, loss: 0.14738169144028918\n",
      "iter: 850, loss: 0.14655043114545252\n",
      "iter: 860, loss: 0.14573355722461254\n",
      "iter: 870, loss: 0.1449307684266495\n",
      "iter: 880, loss: 0.14414177452808963\n",
      "iter: 890, loss: 0.14336629571497098\n",
      "iter: 900, loss: 0.1426040620093747\n",
      "iter: 910, loss: 0.1418548127347944\n",
      "iter: 920, loss: 0.14111829601554346\n",
      "iter: 930, loss: 0.14039426830627844\n",
      "iter: 940, loss: 0.13968249394835766\n",
      "iter: 950, loss: 0.138982744750423\n",
      "iter: 960, loss: 0.13829479959101665\n",
      "iter: 970, loss: 0.1376184440414238\n",
      "iter: 980, loss: 0.13695347000729613\n",
      "iter: 990, loss: 0.13629967538785498\n",
      "iter: 1000, loss: 0.1356568637516905\n",
      "iter: 1010, loss: 0.13502484402831647\n",
      "iter: 1020, loss: 0.13440343021481793\n",
      "iter: 1030, loss: 0.13379244109702915\n",
      "iter: 1040, loss: 0.1331916999847305\n",
      "iter: 1050, loss: 0.13260103446047747\n",
      "iter: 1060, loss: 0.1320202761416385\n",
      "iter: 1070, loss: 0.13144926045536368\n",
      "iter: 1080, loss: 0.13088782642612654\n",
      "iter: 1090, loss: 0.13033581647554066\n",
      "iter: 1100, loss: 0.1297930762341829\n",
      "iter: 1110, loss: 0.12925945436509875\n",
      "iter: 1120, loss: 0.12873480239870175\n",
      "iter: 1130, loss: 0.12821897457876608\n",
      "iter: 1140, loss: 0.12771182771917106\n",
      "iter: 1150, loss: 0.12721322107108035\n",
      "iter: 1160, loss: 0.12672301620020807\n",
      "iter: 1170, loss: 0.1262410768738026\n",
      "iter: 1180, loss: 0.1257672689569724\n",
      "iter: 1190, loss: 0.12530146031796316\n",
      "iter: 1200, loss: 0.12484352074200174\n",
      "iter: 1210, loss: 0.12439332185324502\n",
      "iter: 1220, loss: 0.12395073704447174\n",
      "iter: 1230, loss: 0.12351564141403817\n",
      "iter: 1240, loss: 0.12308791170968456\n",
      "iter: 1250, loss: 0.12266742627874855\n",
      "iter: 1260, loss: 0.1222540650243585\n",
      "iter: 1270, loss: 0.1218477093671287\n",
      "iter: 1280, loss: 0.12144824221198687\n",
      "iter: 1290, loss: 0.12105554791965137\n",
      "iter: 1300, loss: 0.12066951228238158\n",
      "iter: 1310, loss: 0.12029002250355748\n",
      "iter: 1320, loss: 0.11991696718073935\n",
      "iter: 1330, loss: 0.11955023629177312\n",
      "iter: 1340, loss: 0.11918972118364579\n",
      "iter: 1350, loss: 0.11883531456367111\n",
      "iter: 1360, loss: 0.11848691049273884\n",
      "iter: 1370, loss: 0.11814440438027467\n",
      "iter: 1380, loss: 0.11780769298065814\n",
      "iter: 1390, loss: 0.11747667439079339\n",
      "iter: 1400, loss: 0.11715124804860001\n",
      "iter: 1410, loss: 0.11683131473219872\n",
      "iter: 1420, loss: 0.11651677655958578\n",
      "iter: 1430, loss: 0.11620753698857393\n",
      "iter: 1440, loss: 0.11590350081687487\n",
      "iter: 1450, loss: 0.11560457418213155\n",
      "iter: 1460, loss: 0.11531066456179807\n",
      "iter: 1470, loss: 0.1150216807727213\n",
      "iter: 1480, loss: 0.11473753297034707\n",
      "iter: 1490, loss: 0.11445813264744721\n",
      "iter: 1500, loss: 0.11418339263230554\n",
      "iter: 1510, loss: 0.11391322708630318\n",
      "iter: 1520, loss: 0.11364755150085804\n",
      "iter: 1530, loss: 0.11338628269367673\n",
      "iter: 1540, loss: 0.11312933880430728\n",
      "iter: 1550, loss: 0.11287663928896428\n",
      "iter: 1560, loss: 0.1126281049146266\n",
      "iter: 1570, loss: 0.11238365775240888\n",
      "iter: 1580, loss: 0.11214322117022155\n",
      "iter: 1590, loss: 0.11190671982470483\n",
      "iter: 1600, loss: 0.1116740796524896\n",
      "iter: 1610, loss: 0.11144522786078484\n",
      "iter: 1620, loss: 0.11122009291732926\n",
      "iter: 1630, loss: 0.11099860453972303\n",
      "iter: 1640, loss: 0.1107806936841737\n",
      "iter: 1650, loss: 0.11056629253371872\n",
      "iter: 1660, loss: 0.11035533448590015\n",
      "iter: 1670, loss: 0.11014775413998569\n",
      "iter: 1680, loss: 0.10994348728372551\n",
      "iter: 1690, loss: 0.10974247087971978\n",
      "iter: 1700, loss: 0.10954464305139426\n",
      "iter: 1710, loss: 0.10934994306864723\n",
      "iter: 1720, loss: 0.1091583113331981\n",
      "iter: 1730, loss: 0.10896968936365764\n",
      "iter: 1740, loss: 0.10878401978036702\n",
      "iter: 1750, loss: 0.10860124629002295\n",
      "iter: 1760, loss: 0.10842131367014689\n",
      "iter: 1770, loss: 0.10824416775338053\n",
      "iter: 1780, loss: 0.10806975541169572\n",
      "iter: 1790, loss: 0.10789802454049552\n",
      "iter: 1800, loss: 0.1077289240426501\n",
      "iter: 1810, loss: 0.10756240381250301\n",
      "iter: 1820, loss: 0.107398414719846\n",
      "iter: 1830, loss: 0.1072369085939051\n",
      "iter: 1840, loss: 0.10707783820735092\n",
      "iter: 1850, loss: 0.10692115726033663\n",
      "iter: 1860, loss: 0.1067668203646161\n",
      "iter: 1870, loss: 0.1066147830277173\n",
      "iter: 1880, loss: 0.10646500163721481\n",
      "iter: 1890, loss: 0.1063174334451039\n",
      "iter: 1900, loss: 0.10617203655228229\n",
      "iter: 1910, loss: 0.10602876989317588\n",
      "iter: 1920, loss: 0.10588759322048881\n",
      "iter: 1930, loss: 0.10574846709009558\n",
      "iter: 1940, loss: 0.10561135284610082\n",
      "iter: 1950, loss: 0.10547621260606092\n",
      "iter: 1960, loss: 0.10534300924636827\n",
      "iter: 1970, loss: 0.10521170638781424\n",
      "iter: 1980, loss: 0.1050822683813346\n",
      "iter: 1990, loss: 0.10495466029394536\n",
      "iter: 2000, loss: 0.10482884789486155\n",
      "iter: 2010, loss: 0.10470479764181101\n",
      "iter: 2020, loss: 0.10458247666755523\n",
      "iter: 2030, loss: 0.10446185276658826\n",
      "iter: 2040, loss: 0.10434289438206214\n",
      "iter: 2050, loss: 0.10422557059289521\n",
      "iter: 2060, loss: 0.10410985110109247\n",
      "iter: 2070, loss: 0.1039957062192732\n",
      "iter: 2080, loss: 0.10388310685839697\n",
      "iter: 2090, loss: 0.10377202451569678\n",
      "iter: 2100, loss: 0.10366243126282325\n",
      "iter: 2110, loss: 0.10355429973418251\n",
      "iter: 2120, loss: 0.10344760311548794\n",
      "iter: 2130, loss: 0.10334231513250577\n",
      "iter: 2140, loss: 0.103238410040009\n",
      "iter: 2150, loss: 0.10313586261093277\n",
      "iter: 2160, loss: 0.10303464812571492\n",
      "iter: 2170, loss: 0.10293474236185846\n",
      "iter: 2180, loss: 0.102836121583661\n",
      "iter: 2190, loss: 0.10273876253215536\n",
      "iter: 2200, loss: 0.1026426424152417\n",
      "iter: 2210, loss: 0.10254773889799963\n",
      "iter: 2220, loss: 0.10245403009319676\n",
      "iter: 2230, loss: 0.10236149455196997\n",
      "iter: 2240, loss: 0.10227011125470113\n",
      "iter: 2250, loss: 0.10217985960207313\n",
      "iter: 2260, loss: 0.10209071940628917\n",
      "iter: 2270, loss: 0.10200267088247947\n",
      "iter: 2280, loss: 0.10191569464027983\n",
      "iter: 2290, loss: 0.1018297716755753\n",
      "iter: 2300, loss: 0.10174488336241255\n",
      "iter: 2310, loss: 0.10166101144507841\n",
      "iter: 2320, loss: 0.10157813803033607\n",
      "iter: 2330, loss: 0.10149624557983022\n",
      "iter: 2340, loss: 0.10141531690263503\n",
      "iter: 2350, loss: 0.10133533514797044\n",
      "iter: 2360, loss: 0.10125628379805633\n",
      "iter: 2370, loss: 0.10117814666112983\n",
      "iter: 2380, loss: 0.10110090786459065\n",
      "iter: 2390, loss: 0.10102455184830773\n",
      "iter: 2400, loss: 0.10094906335805674\n",
      "iter: 2410, loss: 0.10087442743910123\n",
      "iter: 2420, loss: 0.1008006294298997\n",
      "iter: 2430, loss: 0.10072765495595838\n",
      "iter: 2440, loss: 0.10065548992380643\n",
      "iter: 2450, loss: 0.10058412051510549\n",
      "iter: 2460, loss: 0.10051353318087441\n",
      "iter: 2470, loss: 0.10044371463584811\n",
      "iter: 2480, loss: 0.10037465185295197\n",
      "iter: 2490, loss: 0.10030633205789749\n",
      "iter: 2500, loss: 0.10023874272388712\n",
      "iter: 2510, loss: 0.10017187156644615\n",
      "iter: 2520, loss: 0.10010570653834917\n",
      "iter: 2530, loss: 0.10004023582466845\n",
      "iter: 2540, loss: 0.09997544783792642\n",
      "iter: 2550, loss: 0.09991133121334661\n",
      "iter: 2560, loss: 0.09984787480421349\n",
      "iter: 2570, loss: 0.09978506767733564\n",
      "iter: 2580, loss: 0.09972289910859496\n",
      "iter: 2590, loss: 0.09966135857860356\n",
      "iter: 2600, loss: 0.09960043576845586\n",
      "iter: 2610, loss: 0.09954012055555979\n",
      "iter: 2620, loss: 0.09948040300957302\n",
      "iter: 2630, loss: 0.09942127338841593\n",
      "iter: 2640, loss: 0.09936272213437636\n",
      "iter: 2650, loss: 0.09930473987030654\n",
      "iter: 2660, loss: 0.09924731739588687\n",
      "iter: 2670, loss: 0.09919044568397914\n",
      "iter: 2680, loss: 0.09913411587706651\n",
      "iter: 2690, loss: 0.09907831928374777\n",
      "iter: 2700, loss: 0.0990230473753381\n",
      "iter: 2710, loss: 0.09896829178251731\n",
      "iter: 2720, loss: 0.09891404429206917\n",
      "iter: 2730, loss: 0.09886029684368035\n",
      "iter: 2740, loss: 0.09880704152681095\n",
      "iter: 2750, loss: 0.09875427057763737\n",
      "iter: 2760, loss: 0.0987019763760591\n",
      "iter: 2770, loss: 0.098650151442761\n",
      "iter: 2780, loss: 0.0985987884363615\n",
      "iter: 2790, loss: 0.09854788015059518\n",
      "iter: 2800, loss: 0.09849741951158031\n",
      "iter: 2810, loss: 0.09844739957512426\n",
      "iter: 2820, loss: 0.09839781352411095\n",
      "iter: 2830, loss: 0.09834865466591973\n",
      "iter: 2840, loss: 0.09829991642991624\n",
      "iter: 2850, loss: 0.0982515923649932\n",
      "iter: 2860, loss: 0.0982036761371573\n",
      "iter: 2870, loss: 0.09815616152718382\n",
      "iter: 2880, loss: 0.0981090424283083\n",
      "iter: 2890, loss: 0.09806231284396025\n",
      "iter: 2900, loss: 0.09801596688557747\n",
      "iter: 2910, loss: 0.09796999877042836\n",
      "iter: 2920, loss: 0.09792440281950557\n",
      "iter: 2930, loss: 0.09787917345546003\n",
      "iter: 2940, loss: 0.097834305200573\n",
      "iter: 2950, loss: 0.09778979267477325\n",
      "iter: 2960, loss: 0.09774563059370818\n",
      "iter: 2970, loss: 0.09770181376683121\n",
      "iter: 2980, loss: 0.09765833709555843\n",
      "iter: 2990, loss: 0.09761519557144462\n",
      "iter: 3000, loss: 0.09757238427440507\n",
      "iter: 3010, loss: 0.09752989837097206\n",
      "iter: 3020, loss: 0.09748773311259434\n",
      "iter: 3030, loss: 0.09744588383396442\n",
      "iter: 3040, loss: 0.097404345951387\n",
      "iter: 3050, loss: 0.09736311496118263\n",
      "iter: 3060, loss: 0.09732218643811993\n",
      "iter: 3070, loss: 0.09728155603387802\n",
      "iter: 3080, loss: 0.0972412194755573\n",
      "iter: 3090, loss: 0.09720117256420441\n",
      "iter: 3100, loss: 0.09716141117337164\n",
      "iter: 3110, loss: 0.09712193124771658\n",
      "iter: 3120, loss: 0.09708272880161549\n",
      "iter: 3130, loss: 0.09704379991782164\n",
      "iter: 3140, loss: 0.09700514074613648\n",
      "iter: 3150, loss: 0.09696674750212055\n",
      "iter: 3160, loss: 0.09692861646582746\n",
      "iter: 3170, loss: 0.09689074398055796\n",
      "iter: 3180, loss: 0.09685312645164817\n",
      "iter: 3190, loss: 0.09681576034527922\n",
      "iter: 3200, loss: 0.0967786421873156\n",
      "iter: 3210, loss: 0.09674176856215515\n",
      "iter: 3220, loss: 0.09670513611162267\n",
      "iter: 3230, loss: 0.09666874153386676\n",
      "iter: 3240, loss: 0.09663258158229486\n",
      "iter: 3250, loss: 0.0965966530645153\n",
      "iter: 3260, loss: 0.0965609528413147\n",
      "iter: 3270, loss: 0.09652547782565067\n",
      "iter: 3280, loss: 0.09649022498166485\n",
      "iter: 3290, loss: 0.09645519132371028\n",
      "iter: 3300, loss: 0.09642037391541626\n",
      "iter: 3310, loss: 0.09638576986875311\n",
      "iter: 3320, loss: 0.09635137634312314\n",
      "iter: 3330, loss: 0.09631719054447783\n",
      "iter: 3340, loss: 0.09628320972443834\n",
      "iter: 3350, loss: 0.09624943117944333\n",
      "iter: 3360, loss: 0.09621585224991965\n",
      "iter: 3370, loss: 0.09618247031945162\n",
      "iter: 3380, loss: 0.09614928281398108\n",
      "iter: 3390, loss: 0.09611628720102659\n",
      "iter: 3400, loss: 0.09608348098890723\n",
      "iter: 3410, loss: 0.09605086172598466\n",
      "iter: 3420, loss: 0.09601842699993075\n",
      "iter: 3430, loss: 0.09598617443699571\n",
      "iter: 3440, loss: 0.0959541017012977\n",
      "iter: 3450, loss: 0.09592220649412876\n",
      "iter: 3460, loss: 0.09589048655327384\n",
      "iter: 3470, loss: 0.09585893965233633\n",
      "iter: 3480, loss: 0.09582756360008841\n",
      "iter: 3490, loss: 0.09579635623982496\n",
      "iter: 3500, loss: 0.09576531544873562\n",
      "iter: 3510, loss: 0.09573443913728664\n",
      "iter: 3520, loss: 0.09570372524861788\n",
      "iter: 3530, loss: 0.09567317175794925\n",
      "iter: 3540, loss: 0.09564277667199991\n",
      "iter: 3550, loss: 0.09561253802841765\n",
      "iter: 3560, loss: 0.09558245389522628\n",
      "iter: 3570, loss: 0.09555252237026944\n",
      "iter: 3580, loss: 0.09552274158068123\n",
      "iter: 3590, loss: 0.09549310968235829\n",
      "iter: 3600, loss: 0.09546362485944826\n",
      "iter: 3610, loss: 0.09543428532383756\n",
      "iter: 3620, loss: 0.09540508931466309\n",
      "iter: 3630, loss: 0.09537603509782286\n",
      "iter: 3640, loss: 0.0953471209655007\n",
      "iter: 3650, loss: 0.09531834523569994\n",
      "iter: 3660, loss: 0.09528970625179238\n",
      "iter: 3670, loss: 0.09526120238205553\n",
      "iter: 3680, loss: 0.09523283201924675\n",
      "iter: 3690, loss: 0.09520459358016574\n",
      "iter: 3700, loss: 0.09517648550523218\n",
      "iter: 3710, loss: 0.09514850625807321\n",
      "iter: 3720, loss: 0.09512065432511538\n",
      "iter: 3730, loss: 0.09509292821518778\n",
      "iter: 3740, loss: 0.09506532645913017\n",
      "iter: 3750, loss: 0.0950378476094109\n",
      "iter: 3760, loss: 0.0950104902397464\n",
      "iter: 3770, loss: 0.09498325294474477\n",
      "iter: 3780, loss: 0.0949561343395271\n",
      "iter: 3790, loss: 0.09492913305938915\n",
      "iter: 3800, loss: 0.09490224775944374\n",
      "iter: 3810, loss: 0.09487547711428337\n",
      "iter: 3820, loss: 0.0948488198176482\n",
      "iter: 3830, loss: 0.09482227458209376\n",
      "iter: 3840, loss: 0.09479584013867166\n",
      "iter: 3850, loss: 0.09476951523661452\n",
      "iter: 3860, loss: 0.09474329864302654\n",
      "iter: 3870, loss: 0.09471718914257617\n",
      "iter: 3880, loss: 0.0946911855372069\n",
      "iter: 3890, loss: 0.09466528664583661\n",
      "iter: 3900, loss: 0.09463949130407495\n",
      "iter: 3910, loss: 0.09461379836394383\n",
      "iter: 3920, loss: 0.09458820669359885\n",
      "iter: 3930, loss: 0.09456271517705783\n",
      "iter: 3940, loss: 0.09453732271394094\n",
      "iter: 3950, loss: 0.09451202821920496\n",
      "iter: 3960, loss: 0.09448683062289136\n",
      "iter: 3970, loss: 0.0944617288698727\n",
      "iter: 3980, loss: 0.09443672191960845\n",
      "iter: 3990, loss: 0.09441180874590444\n",
      "iter: 4000, loss: 0.09438698833667618\n",
      "iter: 4010, loss: 0.09436225969371795\n",
      "iter: 4020, loss: 0.09433762183246457\n",
      "iter: 4030, loss: 0.09431307378178468\n",
      "iter: 4040, loss: 0.09428861458374609\n",
      "iter: 4050, loss: 0.09426424329341229\n",
      "iter: 4060, loss: 0.09423995897862072\n",
      "iter: 4070, loss: 0.09421576071978524\n",
      "iter: 4080, loss: 0.09419164760968211\n",
      "iter: 4090, loss: 0.09416761875325974\n",
      "iter: 4100, loss: 0.09414367326743496\n",
      "iter: 4110, loss: 0.0941198102809081\n",
      "iter: 4120, loss: 0.09409602893396729\n",
      "iter: 4130, loss: 0.0940723283783042\n",
      "iter: 4140, loss: 0.09404870777683674\n",
      "iter: 4150, loss: 0.09402516630352405\n",
      "iter: 4160, loss: 0.09400170314319671\n",
      "iter: 4170, loss: 0.09397831749138227\n",
      "iter: 4180, loss: 0.09395500855413069\n",
      "iter: 4190, loss: 0.09393177554785817\n",
      "iter: 4200, loss: 0.09390861769917817\n",
      "iter: 4210, loss: 0.09388553424474255\n",
      "iter: 4220, loss: 0.09386252443108434\n",
      "iter: 4230, loss: 0.0938395875144647\n",
      "iter: 4240, loss: 0.09381672276072119\n",
      "iter: 4250, loss: 0.0937939294451161\n",
      "iter: 4260, loss: 0.09377120685219506\n",
      "iter: 4270, loss: 0.09374855427563865\n",
      "iter: 4280, loss: 0.09372597101812827\n",
      "iter: 4290, loss: 0.09370345639120284\n",
      "iter: 4300, loss: 0.0936810097151224\n",
      "iter: 4310, loss: 0.09365863031873653\n",
      "iter: 4320, loss: 0.09363631753935467\n",
      "iter: 4330, loss: 0.09361407072261443\n",
      "iter: 4340, loss: 0.09359188922235931\n",
      "iter: 4350, loss: 0.0935697724005099\n",
      "iter: 4360, loss: 0.09354771962694307\n",
      "iter: 4370, loss: 0.09352573027937827\n",
      "iter: 4380, loss: 0.09350380374324926\n",
      "iter: 4390, loss: 0.09348193941159892\n",
      "iter: 4400, loss: 0.09346013668495828\n",
      "iter: 4410, loss: 0.09343839497123567\n",
      "iter: 4420, loss: 0.09341671368561573\n",
      "iter: 4430, loss: 0.0933950922504368\n",
      "iter: 4440, loss: 0.09337353009510026\n",
      "iter: 4450, loss: 0.09335202665595707\n",
      "iter: 4460, loss: 0.09333058137620939\n",
      "iter: 4470, loss: 0.09330919370580687\n",
      "iter: 4480, loss: 0.09328786310135506\n",
      "iter: 4490, loss: 0.09326658902600801\n",
      "iter: 4500, loss: 0.09324537094938244\n",
      "iter: 4510, loss: 0.09322420834746012\n",
      "iter: 4520, loss: 0.0932031007024971\n",
      "iter: 4530, loss: 0.09318204750292902\n",
      "iter: 4540, loss: 0.09316104824329066\n",
      "iter: 4550, loss: 0.09314010242411969\n",
      "iter: 4560, loss: 0.09311920955187562\n",
      "iter: 4570, loss: 0.09309836913885554\n",
      "iter: 4580, loss: 0.09307758070310987\n",
      "iter: 4590, loss: 0.09305684376836051\n",
      "iter: 4600, loss: 0.09303615786392162\n",
      "iter: 4610, loss: 0.09301552252462082\n",
      "iter: 4620, loss: 0.09299493729072156\n",
      "iter: 4630, loss: 0.09297440170784516\n",
      "iter: 4640, loss: 0.09295391532689776\n",
      "iter: 4650, loss: 0.092933477704\n",
      "iter: 4660, loss: 0.09291308840040334\n",
      "iter: 4670, loss: 0.09289274698243408\n",
      "iter: 4680, loss: 0.09287245302141316\n",
      "iter: 4690, loss: 0.09285220609358638\n",
      "iter: 4700, loss: 0.09283200578006343\n",
      "iter: 4710, loss: 0.09281185166674762\n",
      "iter: 4720, loss: 0.09279174334427046\n",
      "iter: 4730, loss: 0.0927716804079249\n",
      "iter: 4740, loss: 0.09275166245760766\n",
      "iter: 4750, loss: 0.09273168909775292\n",
      "iter: 4760, loss: 0.09271175993727017\n",
      "iter: 4770, loss: 0.09269187458948637\n",
      "iter: 4780, loss: 0.09267203267208592\n",
      "iter: 4790, loss: 0.09265223380705136\n",
      "iter: 4800, loss: 0.09263247762060779\n",
      "iter: 4810, loss: 0.09261276374316328\n",
      "iter: 4820, loss: 0.09259309180925382\n",
      "iter: 4830, loss: 0.09257346145749314\n",
      "iter: 4840, loss: 0.09255387233051367\n",
      "iter: 4850, loss: 0.09253432407491478\n",
      "iter: 4860, loss: 0.09251481634121485\n",
      "iter: 4870, loss: 0.09249534878379027\n",
      "iter: 4880, loss: 0.09247592106083746\n",
      "iter: 4890, loss: 0.09245653283431182\n",
      "iter: 4900, loss: 0.09243718376988622\n",
      "iter: 4910, loss: 0.09241787353689972\n",
      "iter: 4920, loss: 0.09239860180831279\n",
      "iter: 4930, loss: 0.09237936826065678\n",
      "iter: 4940, loss: 0.0923601725739883\n",
      "iter: 4950, loss: 0.09234101443184811\n",
      "iter: 4960, loss: 0.09232189352121359\n",
      "iter: 4970, loss: 0.09230280953245523\n",
      "iter: 4980, loss: 0.09228376215929503\n",
      "iter: 4990, loss: 0.09226475109876246\n",
      "iter: 5000, loss: 0.09224577605114904\n",
      "iter: 5010, loss: 0.09222683671997817\n",
      "iter: 5020, loss: 0.09220793281195527\n",
      "iter: 5030, loss: 0.09218906403692714\n",
      "iter: 5040, loss: 0.09217023010784667\n",
      "iter: 5050, loss: 0.09215143074073426\n",
      "iter: 5060, loss: 0.09213266565464034\n",
      "iter: 5070, loss: 0.09211393457160144\n",
      "iter: 5080, loss: 0.09209523721661343\n",
      "iter: 5090, loss: 0.09207657331758408\n",
      "iter: 5100, loss: 0.09205794260530803\n",
      "iter: 5110, loss: 0.09203934481341981\n",
      "iter: 5120, loss: 0.09202077967837272\n",
      "iter: 5130, loss: 0.09200224693939107\n",
      "iter: 5140, loss: 0.09198374633844647\n",
      "iter: 5150, loss: 0.0919652776202161\n",
      "iter: 5160, loss: 0.09194684053205734\n",
      "iter: 5170, loss: 0.09192843482397305\n",
      "iter: 5180, loss: 0.09191006024857862\n",
      "iter: 5190, loss: 0.09189171656107105\n",
      "iter: 5200, loss: 0.09187340351919733\n",
      "iter: 5210, loss: 0.09185512088322569\n",
      "iter: 5220, loss: 0.09183686841591528\n",
      "iter: 5230, loss: 0.09181864588248419\n",
      "iter: 5240, loss: 0.09180045305058193\n",
      "iter: 5250, loss: 0.09178228969026761\n",
      "iter: 5260, loss: 0.09176415557396761\n",
      "iter: 5270, loss: 0.09174605047645688\n",
      "iter: 5280, loss: 0.09172797417483002\n",
      "iter: 5290, loss: 0.09170992644847256\n",
      "iter: 5300, loss: 0.09169190707903976\n",
      "iter: 5310, loss: 0.09167391585042003\n",
      "iter: 5320, loss: 0.0916559525487205\n",
      "iter: 5330, loss: 0.09163801696223381\n",
      "iter: 5340, loss: 0.09162010888141295\n",
      "iter: 5350, loss: 0.09160222809885267\n",
      "iter: 5360, loss: 0.09158437440925422\n",
      "iter: 5370, loss: 0.09156654760941516\n",
      "iter: 5380, loss: 0.09154874749819226\n",
      "iter: 5390, loss: 0.09153097387648645\n",
      "iter: 5400, loss: 0.09151322654721393\n",
      "iter: 5410, loss: 0.0914955053152901\n",
      "iter: 5420, loss: 0.09147780998760108\n",
      "iter: 5430, loss: 0.09146014037298382\n",
      "iter: 5440, loss: 0.09144249628220434\n",
      "iter: 5450, loss: 0.09142487752793486\n",
      "iter: 5460, loss: 0.09140728392473245\n",
      "iter: 5470, loss: 0.09138971528902226\n",
      "iter: 5480, loss: 0.09137217143906862\n",
      "iter: 5490, loss: 0.09135465219496176\n",
      "iter: 5500, loss: 0.09133715737859821\n",
      "iter: 5510, loss: 0.09131968681365349\n",
      "iter: 5520, loss: 0.09130224032556794\n",
      "iter: 5530, loss: 0.09128481774152304\n",
      "iter: 5540, loss: 0.09126741889043112\n",
      "iter: 5550, loss: 0.09125004360290719\n",
      "iter: 5560, loss: 0.09123269171125498\n",
      "iter: 5570, loss: 0.09121536304944734\n",
      "iter: 5580, loss: 0.09119805745310741\n",
      "iter: 5590, loss: 0.09118077475949478\n",
      "iter: 5600, loss: 0.0911635148074791\n",
      "iter: 5610, loss: 0.09114627743753186\n",
      "iter: 5620, loss: 0.09112906249170644\n",
      "iter: 5630, loss: 0.09111186981361809\n",
      "iter: 5640, loss: 0.0910946992484278\n",
      "iter: 5650, loss: 0.09107755064283236\n",
      "iter: 5660, loss: 0.09106042384503853\n",
      "iter: 5670, loss: 0.09104331870475287\n",
      "iter: 5680, loss: 0.09102623507316526\n",
      "iter: 5690, loss: 0.09100917280293351\n",
      "iter: 5700, loss: 0.09099213174816141\n",
      "iter: 5710, loss: 0.09097511176439455\n",
      "iter: 5720, loss: 0.09095811270859587\n",
      "iter: 5730, loss: 0.0909411344391363\n",
      "iter: 5740, loss: 0.0909241768157791\n",
      "iter: 5750, loss: 0.09090723969966108\n",
      "iter: 5760, loss: 0.09089032295328509\n",
      "iter: 5770, loss: 0.09087342644050009\n",
      "iter: 5780, loss: 0.09085655002648918\n",
      "iter: 5790, loss: 0.09083969357775862\n",
      "iter: 5800, loss: 0.09082285696211943\n",
      "iter: 5810, loss: 0.090806040048677\n",
      "iter: 5820, loss: 0.09078924270781831\n",
      "iter: 5830, loss: 0.09077246481119661\n",
      "iter: 5840, loss: 0.09075570623171815\n",
      "iter: 5850, loss: 0.09073896684353118\n",
      "iter: 5860, loss: 0.09072224652201573\n",
      "iter: 5870, loss: 0.09070554514376407\n",
      "iter: 5880, loss: 0.09068886258657692\n",
      "iter: 5890, loss: 0.09067219872944177\n",
      "iter: 5900, loss: 0.09065555345253071\n",
      "iter: 5910, loss: 0.09063892663718387\n",
      "iter: 5920, loss: 0.09062231816589462\n",
      "iter: 5930, loss: 0.0906057279223011\n",
      "iter: 5940, loss: 0.09058915579117834\n",
      "iter: 5950, loss: 0.09057260165842104\n",
      "iter: 5960, loss: 0.09055606541103628\n",
      "iter: 5970, loss: 0.09053954693712975\n",
      "iter: 5980, loss: 0.09052304612589838\n",
      "iter: 5990, loss: 0.09050656286761484\n",
      "iter: 6000, loss: 0.09049009705361906\n",
      "iter: 6010, loss: 0.09047364857631239\n",
      "iter: 6020, loss: 0.09045721732913785\n",
      "iter: 6030, loss: 0.09044080320657959\n",
      "iter: 6040, loss: 0.09042440610414607\n",
      "iter: 6050, loss: 0.0904080259183621\n",
      "iter: 6060, loss: 0.09039166254675941\n",
      "iter: 6070, loss: 0.09037531588786817\n",
      "iter: 6080, loss: 0.0903589858412033\n",
      "iter: 6090, loss: 0.09034267230725868\n",
      "iter: 6100, loss: 0.09032637518749798\n",
      "iter: 6110, loss: 0.09031009438434098\n",
      "iter: 6120, loss: 0.09029382980115755\n",
      "iter: 6130, loss: 0.09027758134226073\n",
      "iter: 6140, loss: 0.09026134891289446\n",
      "iter: 6150, loss: 0.09024513241922626\n",
      "iter: 6160, loss: 0.09022893176833459\n",
      "iter: 6170, loss: 0.09021274686820803\n",
      "iter: 6180, loss: 0.09019657762772779\n",
      "iter: 6190, loss: 0.09018042395666447\n",
      "iter: 6200, loss: 0.09016428576567138\n",
      "iter: 6210, loss: 0.09014816296626914\n",
      "iter: 6220, loss: 0.09013205547084918\n",
      "iter: 6230, loss: 0.09011596319265136\n",
      "iter: 6240, loss: 0.09009988604576544\n",
      "iter: 6250, loss: 0.09008382394512168\n",
      "iter: 6260, loss: 0.09006777680648191\n",
      "iter: 6270, loss: 0.09005174454643144\n",
      "iter: 6280, loss: 0.09003572708237224\n",
      "iter: 6290, loss: 0.09001972433251743\n",
      "iter: 6300, loss: 0.09000373621588001\n",
      "iter: 6310, loss: 0.08998776265226498\n",
      "iter: 6320, loss: 0.08997180356226897\n",
      "iter: 6330, loss: 0.08995585886726365\n",
      "iter: 6340, loss: 0.08993992848939716\n",
      "iter: 6350, loss: 0.08992401235158108\n",
      "iter: 6360, loss: 0.08990811037748878\n",
      "iter: 6370, loss: 0.08989222249153886\n",
      "iter: 6380, loss: 0.0898763486189006\n",
      "iter: 6390, loss: 0.08986048868548005\n",
      "iter: 6400, loss: 0.08984464261791437\n",
      "iter: 6410, loss: 0.08982881034356602\n",
      "iter: 6420, loss: 0.08981299179051642\n",
      "iter: 6430, loss: 0.08979718688755628\n",
      "iter: 6440, loss: 0.08978139556418668\n",
      "iter: 6450, loss: 0.08976561775060617\n",
      "iter: 6460, loss: 0.08974985337770525\n",
      "iter: 6470, loss: 0.08973410237705982\n",
      "iter: 6480, loss: 0.08971836468093392\n",
      "iter: 6490, loss: 0.08970264022225932\n",
      "iter: 6500, loss: 0.0896869289346406\n",
      "iter: 6510, loss: 0.08967123075234619\n",
      "iter: 6520, loss: 0.08965554561029883\n",
      "iter: 6530, loss: 0.08963987344407598\n",
      "iter: 6540, loss: 0.08962421418989734\n",
      "iter: 6550, loss: 0.08960856778462589\n",
      "iter: 6560, loss: 0.08959293416576072\n",
      "iter: 6570, loss: 0.0895773132714257\n",
      "iter: 6580, loss: 0.08956170504037375\n",
      "iter: 6590, loss: 0.08954610941197011\n",
      "iter: 6600, loss: 0.08953052632619996\n",
      "iter: 6610, loss: 0.08951495572365242\n",
      "iter: 6620, loss: 0.08949939754551965\n",
      "iter: 6630, loss: 0.08948385173359219\n",
      "iter: 6640, loss: 0.08946831823025247\n",
      "iter: 6650, loss: 0.08945279697847061\n",
      "iter: 6660, loss: 0.08943728792179798\n",
      "iter: 6670, loss: 0.08942179100436486\n",
      "iter: 6680, loss: 0.08940630617087357\n",
      "iter: 6690, loss: 0.0893908333665951\n",
      "iter: 6700, loss: 0.08937537253736112\n",
      "iter: 6710, loss: 0.08935992362956528\n",
      "iter: 6720, loss: 0.08934448659015219\n",
      "iter: 6730, loss: 0.08932906136661353\n",
      "iter: 6740, loss: 0.08931364790699174\n",
      "iter: 6750, loss: 0.08929824615986513\n",
      "iter: 6760, loss: 0.08928285607434609\n",
      "iter: 6770, loss: 0.08926747760008039\n",
      "iter: 6780, loss: 0.08925211068723837\n",
      "iter: 6790, loss: 0.08923675528651522\n",
      "iter: 6800, loss: 0.08922141134912316\n",
      "iter: 6810, loss: 0.08920607882678644\n",
      "iter: 6820, loss: 0.08919075767174316\n",
      "iter: 6830, loss: 0.0891754478367312\n",
      "iter: 6840, loss: 0.08916014927499304\n",
      "iter: 6850, loss: 0.08914486194026983\n",
      "iter: 6860, loss: 0.08912958578679196\n",
      "iter: 6870, loss: 0.08911432076928144\n",
      "iter: 6880, loss: 0.0890990668429455\n",
      "iter: 6890, loss: 0.08908382396347267\n",
      "iter: 6900, loss: 0.08906859208702711\n",
      "iter: 6910, loss: 0.08905337117025065\n",
      "iter: 6920, loss: 0.08903816117025119\n",
      "iter: 6930, loss: 0.08902296204460529\n",
      "iter: 6940, loss: 0.08900777375135083\n",
      "iter: 6950, loss: 0.08899259624898266\n",
      "iter: 6960, loss: 0.08897742949645383\n",
      "iter: 6970, loss: 0.08896227345316945\n",
      "iter: 6980, loss: 0.0889471280789787\n",
      "iter: 6990, loss: 0.08893199333417946\n",
      "iter: 7000, loss: 0.08891686917950652\n",
      "iter: 7010, loss: 0.08890175557613618\n",
      "iter: 7020, loss: 0.08888665248567706\n",
      "iter: 7030, loss: 0.08887155987016886\n",
      "iter: 7040, loss: 0.08885647769207732\n",
      "iter: 7050, loss: 0.08884140591429483\n",
      "iter: 7060, loss: 0.08882634450013474\n",
      "iter: 7070, loss: 0.08881129341332483\n",
      "iter: 7080, loss: 0.08879625261800762\n",
      "iter: 7090, loss: 0.08878122207873862\n",
      "iter: 7100, loss: 0.08876620176048208\n",
      "iter: 7110, loss: 0.08875119162860665\n",
      "iter: 7120, loss: 0.08873619164888268\n",
      "iter: 7130, loss: 0.08872120178747808\n",
      "iter: 7140, loss: 0.08870622201095744\n",
      "iter: 7150, loss: 0.08869125228627545\n",
      "iter: 7160, loss: 0.0886762925807827\n",
      "iter: 7170, loss: 0.08866134286221226\n",
      "iter: 7180, loss: 0.08864640309868248\n",
      "iter: 7190, loss: 0.08863147325869165\n",
      "iter: 7200, loss: 0.08861655331111715\n",
      "iter: 7210, loss: 0.08860164322521226\n",
      "iter: 7220, loss: 0.088586742970603\n",
      "iter: 7230, loss: 0.08857185251728286\n",
      "iter: 7240, loss: 0.08855697183561724\n",
      "iter: 7250, loss: 0.08854210089633079\n",
      "iter: 7260, loss: 0.08852723967051132\n",
      "iter: 7270, loss: 0.08851238812960775\n",
      "iter: 7280, loss: 0.08849754624542415\n",
      "iter: 7290, loss: 0.08848271399011623\n",
      "iter: 7300, loss: 0.08846789133619352\n",
      "iter: 7310, loss: 0.08845307825651505\n",
      "iter: 7320, loss: 0.08843827472428219\n",
      "iter: 7330, loss: 0.08842348071304307\n",
      "iter: 7340, loss: 0.08840869619668608\n",
      "iter: 7350, loss: 0.08839392114943896\n",
      "iter: 7360, loss: 0.0883791555458616\n",
      "iter: 7370, loss: 0.08836439936085233\n",
      "iter: 7380, loss: 0.08834965256964106\n",
      "iter: 7390, loss: 0.08833491514778201\n",
      "iter: 7400, loss: 0.0883201870711612\n",
      "iter: 7410, loss: 0.08830546831598454\n",
      "iter: 7420, loss: 0.08829075885878304\n",
      "iter: 7430, loss: 0.08827605867640509\n",
      "iter: 7440, loss: 0.0882613677460192\n",
      "iter: 7450, loss: 0.08824668604510454\n",
      "iter: 7460, loss: 0.08823201355145797\n",
      "iter: 7470, loss: 0.08821735024318335\n",
      "iter: 7480, loss: 0.08820269609869702\n",
      "iter: 7490, loss: 0.0881880510967178\n",
      "iter: 7500, loss: 0.08817341521627038\n",
      "iter: 7510, loss: 0.08815878843668125\n",
      "iter: 7520, loss: 0.08814417073757747\n",
      "iter: 7530, loss: 0.08812956209888075\n",
      "iter: 7540, loss: 0.08811496250081366\n",
      "iter: 7550, loss: 0.08810037192388814\n",
      "iter: 7560, loss: 0.08808579034890973\n",
      "iter: 7570, loss: 0.08807121775697406\n",
      "iter: 7580, loss: 0.08805665412946227\n",
      "iter: 7590, loss: 0.0880420994480426\n",
      "iter: 7600, loss: 0.08802755369466594\n",
      "iter: 7610, loss: 0.088013016851566\n",
      "iter: 7620, loss: 0.08799848890125844\n",
      "iter: 7630, loss: 0.08798396982653017\n",
      "iter: 7640, loss: 0.08796945961044693\n",
      "iter: 7650, loss: 0.08795495823635414\n",
      "iter: 7660, loss: 0.08794046568786182\n",
      "iter: 7670, loss: 0.08792598194885258\n",
      "iter: 7680, loss: 0.08791150700347901\n",
      "iter: 7690, loss: 0.08789704083615754\n",
      "iter: 7700, loss: 0.08788258343157344\n",
      "iter: 7710, loss: 0.08786813477466968\n",
      "iter: 7720, loss: 0.08785369485065406\n",
      "iter: 7730, loss: 0.0878392636449922\n",
      "iter: 7740, loss: 0.08782484114340919\n",
      "iter: 7750, loss: 0.08781042733188331\n",
      "iter: 7760, loss: 0.08779602219665109\n",
      "iter: 7770, loss: 0.08778162572419522\n",
      "iter: 7780, loss: 0.08776723790125546\n",
      "iter: 7790, loss: 0.08775285871481617\n",
      "iter: 7800, loss: 0.08773848815211091\n",
      "iter: 7810, loss: 0.08772412620061906\n",
      "iter: 7820, loss: 0.08770977284806324\n",
      "iter: 7830, loss: 0.08769542808241035\n",
      "iter: 7840, loss: 0.08768109189186597\n",
      "iter: 7850, loss: 0.0876667642648744\n",
      "iter: 7860, loss: 0.08765244519012\n",
      "iter: 7870, loss: 0.08763813465652215\n",
      "iter: 7880, loss: 0.08762383265323344\n",
      "iter: 7890, loss: 0.08760953916964012\n",
      "iter: 7900, loss: 0.08759525419535932\n",
      "iter: 7910, loss: 0.0875809777202372\n",
      "iter: 7920, loss: 0.08756670973435055\n",
      "iter: 7930, loss: 0.08755245022799876\n",
      "iter: 7940, loss: 0.08753819919170999\n",
      "iter: 7950, loss: 0.0875239566162361\n",
      "iter: 7960, loss: 0.08750972249254854\n",
      "iter: 7970, loss: 0.08749549681183998\n",
      "iter: 7980, loss: 0.08748127956552129\n",
      "iter: 7990, loss: 0.08746707074522442\n",
      "iter: 8000, loss: 0.087452870342797\n",
      "iter: 8010, loss: 0.08743867835029671\n",
      "iter: 8020, loss: 0.08742449475999978\n",
      "iter: 8030, loss: 0.08741031956439224\n",
      "iter: 8040, loss: 0.08739615275617221\n",
      "iter: 8050, loss: 0.0873819943282445\n",
      "iter: 8060, loss: 0.0873678442737207\n",
      "iter: 8070, loss: 0.08735370258592363\n",
      "iter: 8080, loss: 0.08733956925837355\n",
      "iter: 8090, loss: 0.08732544428480284\n",
      "iter: 8100, loss: 0.08731132765914315\n",
      "iter: 8110, loss: 0.08729721937552082\n",
      "iter: 8120, loss: 0.08728311942826997\n",
      "iter: 8130, loss: 0.08726902781191906\n",
      "iter: 8140, loss: 0.08725494452119407\n",
      "iter: 8150, loss: 0.08724086955101464\n",
      "iter: 8160, loss: 0.08722680289649579\n",
      "iter: 8170, loss: 0.0872127445529475\n",
      "iter: 8180, loss: 0.08719869451586987\n",
      "iter: 8190, loss: 0.08718465278095114\n",
      "iter: 8200, loss: 0.08717061934406965\n",
      "iter: 8210, loss: 0.08715659420129487\n",
      "iter: 8220, loss: 0.08714257734887776\n",
      "iter: 8230, loss: 0.08712856878325774\n",
      "iter: 8240, loss: 0.08711456850105465\n",
      "iter: 8250, loss: 0.08710057649907596\n",
      "iter: 8260, loss: 0.08708659277430557\n",
      "iter: 8270, loss: 0.0870726173239118\n",
      "iter: 8280, loss: 0.08705865014523856\n",
      "iter: 8290, loss: 0.08704469123580964\n",
      "iter: 8300, loss: 0.08703074059332301\n",
      "iter: 8310, loss: 0.08701679821565364\n",
      "iter: 8320, loss: 0.08700286410085215\n",
      "iter: 8330, loss: 0.08698893824713745\n",
      "iter: 8340, loss: 0.08697502065290276\n",
      "iter: 8350, loss: 0.08696111131671354\n",
      "iter: 8360, loss: 0.08694721023730166\n",
      "iter: 8370, loss: 0.0869333174135696\n",
      "iter: 8380, loss: 0.08691943284458208\n",
      "iter: 8390, loss: 0.08690555652957611\n",
      "iter: 8400, loss: 0.08689168846794823\n",
      "iter: 8410, loss: 0.08687782865926014\n",
      "iter: 8420, loss: 0.08686397710323536\n",
      "iter: 8430, loss: 0.08685013379976063\n",
      "iter: 8440, loss: 0.08683629874888085\n",
      "iter: 8450, loss: 0.08682247195079958\n",
      "iter: 8460, loss: 0.08680865340587583\n",
      "iter: 8470, loss: 0.08679484311463263\n",
      "iter: 8480, loss: 0.08678104107774164\n",
      "iter: 8490, loss: 0.08676724729603007\n",
      "iter: 8500, loss: 0.08675346177047961\n",
      "iter: 8510, loss: 0.0867396845022258\n",
      "iter: 8520, loss: 0.08672591549255051\n",
      "iter: 8530, loss: 0.08671215474288904\n",
      "iter: 8540, loss: 0.0866984022548256\n",
      "iter: 8550, loss: 0.08668465803009025\n",
      "iter: 8560, loss: 0.08667092207056182\n",
      "iter: 8570, loss: 0.08665719437826275\n",
      "iter: 8580, loss: 0.08664347495536068\n",
      "iter: 8590, loss: 0.08662976380416405\n",
      "iter: 8600, loss: 0.08661606092712792\n",
      "iter: 8610, loss: 0.086602366326851\n",
      "iter: 8620, loss: 0.08658868000606233\n",
      "iter: 8630, loss: 0.08657500196764036\n",
      "iter: 8640, loss: 0.08656133221459601\n",
      "iter: 8650, loss: 0.08654767075007808\n",
      "iter: 8660, loss: 0.08653401757737231\n",
      "iter: 8670, loss: 0.08652037269989743\n",
      "iter: 8680, loss: 0.08650673612120727\n",
      "iter: 8690, loss: 0.08649310784499041\n",
      "iter: 8700, loss: 0.08647948787506429\n",
      "iter: 8710, loss: 0.08646587621537864\n",
      "iter: 8720, loss: 0.08645227287001157\n",
      "iter: 8730, loss: 0.08643867784317324\n",
      "iter: 8740, loss: 0.0864250911391954\n",
      "iter: 8750, loss: 0.08641151276254391\n",
      "iter: 8760, loss: 0.08639794271780359\n",
      "iter: 8770, loss: 0.0863843810096892\n",
      "iter: 8780, loss: 0.0863708276430347\n",
      "iter: 8790, loss: 0.08635728262280334\n",
      "iter: 8800, loss: 0.08634374595407185\n",
      "iter: 8810, loss: 0.08633021764204105\n",
      "iter: 8820, loss: 0.08631669769203289\n",
      "iter: 8830, loss: 0.08630318610948493\n",
      "iter: 8840, loss: 0.08628968289995503\n",
      "iter: 8850, loss: 0.08627618806911641\n",
      "iter: 8860, loss: 0.08626270162275854\n",
      "iter: 8870, loss: 0.08624922356678529\n",
      "iter: 8880, loss: 0.08623575390721369\n",
      "iter: 8890, loss: 0.08622229265017307\n",
      "iter: 8900, loss: 0.08620883980190618\n",
      "iter: 8910, loss: 0.08619539536876432\n",
      "iter: 8920, loss: 0.08618195935721093\n",
      "iter: 8930, loss: 0.0861685317738161\n",
      "iter: 8940, loss: 0.08615511262526103\n",
      "iter: 8950, loss: 0.08614170191832804\n",
      "iter: 8960, loss: 0.08612829965991137\n",
      "iter: 8970, loss: 0.08611490585700592\n",
      "iter: 8980, loss: 0.08610152051671059\n",
      "iter: 8990, loss: 0.0860881436462304\n",
      "iter: 9000, loss: 0.08607477525286916\n",
      "iter: 9010, loss: 0.08606141534403422\n",
      "iter: 9020, loss: 0.08604806392723312\n",
      "iter: 9030, loss: 0.08603472101007066\n",
      "iter: 9040, loss: 0.08602138660025004\n",
      "iter: 9050, loss: 0.08600806070557482\n",
      "iter: 9060, loss: 0.08599474333393878\n",
      "iter: 9070, loss: 0.0859814344933374\n",
      "iter: 9080, loss: 0.08596813419185628\n",
      "iter: 9090, loss: 0.08595484243767974\n",
      "iter: 9100, loss: 0.0859415592390787\n",
      "iter: 9110, loss: 0.0859282846044173\n",
      "iter: 9120, loss: 0.0859150185421526\n",
      "iter: 9130, loss: 0.08590176106083103\n",
      "iter: 9140, loss: 0.08588851216908566\n",
      "iter: 9150, loss: 0.08587527187563847\n",
      "iter: 9160, loss: 0.08586204018929929\n",
      "iter: 9170, loss: 0.08584881711896136\n",
      "iter: 9180, loss: 0.08583560267361032\n",
      "iter: 9190, loss: 0.08582239686230758\n",
      "iter: 9200, loss: 0.08580919969420063\n",
      "iter: 9210, loss: 0.08579601117852075\n",
      "iter: 9220, loss: 0.08578283132458094\n",
      "iter: 9230, loss: 0.08576966014177058\n",
      "iter: 9240, loss: 0.08575649763956462\n",
      "iter: 9250, loss: 0.0857433438275137\n",
      "iter: 9260, loss: 0.08573019871524504\n",
      "iter: 9270, loss: 0.0857170623124675\n",
      "iter: 9280, loss: 0.0857039346289594\n",
      "iter: 9290, loss: 0.08569081567457845\n",
      "iter: 9300, loss: 0.08567770545925736\n",
      "iter: 9310, loss: 0.08566460399299775\n",
      "iter: 9320, loss: 0.08565151128587843\n",
      "iter: 9330, loss: 0.08563842734804805\n",
      "iter: 9340, loss: 0.08562535218972592\n",
      "iter: 9350, loss: 0.08561228582119787\n",
      "iter: 9360, loss: 0.0855992282528253\n",
      "iter: 9370, loss: 0.0855861794950325\n",
      "iter: 9380, loss: 0.08557313955831254\n",
      "iter: 9390, loss: 0.08556010845322418\n",
      "iter: 9400, loss: 0.08554708619039095\n",
      "iter: 9410, loss: 0.08553407278050192\n",
      "iter: 9420, loss: 0.0855210682343106\n",
      "iter: 9430, loss: 0.0855080725626307\n",
      "iter: 9440, loss: 0.08549508577633894\n",
      "iter: 9450, loss: 0.08548210788637499\n",
      "iter: 9460, loss: 0.08546913890373449\n",
      "iter: 9470, loss: 0.08545617883947618\n",
      "iter: 9480, loss: 0.08544322770471567\n",
      "iter: 9490, loss: 0.08543028551062284\n",
      "iter: 9500, loss: 0.08541735226843032\n",
      "iter: 9510, loss: 0.08540442798942104\n",
      "iter: 9520, loss: 0.0853915126849364\n",
      "iter: 9530, loss: 0.08537860636636806\n",
      "iter: 9540, loss: 0.08536570904516605\n",
      "iter: 9550, loss: 0.08535282073283001\n",
      "iter: 9560, loss: 0.08533994144090994\n",
      "iter: 9570, loss: 0.08532707118100682\n",
      "iter: 9580, loss: 0.08531420996477258\n",
      "iter: 9590, loss: 0.0853013578039069\n",
      "iter: 9600, loss: 0.08528851471015898\n",
      "iter: 9610, loss: 0.08527568069532392\n",
      "iter: 9620, loss: 0.08526285577124361\n",
      "iter: 9630, loss: 0.08525003994980301\n",
      "iter: 9640, loss: 0.08523723324293608\n",
      "iter: 9650, loss: 0.0852244356626186\n",
      "iter: 9660, loss: 0.08521164722086785\n",
      "iter: 9670, loss: 0.08519886792974632\n",
      "iter: 9680, loss: 0.08518609780135533\n",
      "iter: 9690, loss: 0.08517333684783729\n",
      "iter: 9700, loss: 0.08516058508137424\n",
      "iter: 9710, loss: 0.08514784251418711\n",
      "iter: 9720, loss: 0.08513510915853473\n",
      "iter: 9730, loss: 0.08512238502671549\n",
      "iter: 9740, loss: 0.085109670131061\n",
      "iter: 9750, loss: 0.08509696448393847\n",
      "iter: 9760, loss: 0.08508426809775281\n",
      "iter: 9770, loss: 0.08507158098493933\n",
      "iter: 9780, loss: 0.08505890315796792\n",
      "iter: 9790, loss: 0.08504623462934104\n",
      "iter: 9800, loss: 0.08503357541159118\n",
      "iter: 9810, loss: 0.0850209255172862\n",
      "iter: 9820, loss: 0.08500828495901663\n",
      "iter: 9830, loss: 0.08499565374940692\n",
      "iter: 9840, loss: 0.08498303190110645\n",
      "iter: 9850, loss: 0.0849704194267966\n",
      "iter: 9860, loss: 0.08495781633918198\n",
      "iter: 9870, loss: 0.08494522265099518\n",
      "iter: 9880, loss: 0.08493263837499077\n",
      "iter: 9890, loss: 0.08492006352394843\n",
      "iter: 9900, loss: 0.08490749811067366\n",
      "iter: 9910, loss: 0.0848949421479943\n",
      "iter: 9920, loss: 0.0848823956487572\n",
      "iter: 9930, loss: 0.08486985862583275\n",
      "iter: 9940, loss: 0.08485733109211283\n",
      "iter: 9950, loss: 0.08484481306050515\n",
      "iter: 9960, loss: 0.08483230454394057\n",
      "iter: 9970, loss: 0.08481980555536163\n",
      "iter: 9980, loss: 0.08480731610773465\n",
      "iter: 9990, loss: 0.084794836214039\n",
      "iter: 10000, loss: 0.0847823658872698\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "N = 16\n",
    "K = 8\n",
    "D = 128\n",
    "\n",
    "seed = 0\n",
    "rng  = np.random.default_rng(seed)\n",
    "fm   = FactorizationMachines(N, K, seed=seed)\n",
    "x    = rng.choice((0, 1), size=(D, N))\n",
    "\n",
    "Q    = rng.uniform(-1., 1., (N, N))\n",
    "y    = np.einsum('dn,nm,dm->d', x, Q, x)\n",
    "\n",
    "_, loss_hist = Main.train_fm_als(\n",
    "    fm.params, x.astype(float), y, fm.predict(x), 10000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "# plt.scatter(range(len(loss_hist))[::200], loss_hist[::200], s=50, marker='o', color='none', edgecolors='black')\n",
    "plt.plot(loss_hist, color='black')\n",
    "plt.yscale('log')\n",
    "plt.xlabel(r'Iteration $t$')\n",
    "plt.ylabel(r'$\\dfrac{1}{D} \\sum_{d} (y^{(d)} - f^{(d)})^2$')\n",
    "plt.title('Loss History of FM (ALS)')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
