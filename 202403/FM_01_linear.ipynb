{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gibbs Sampling for Factorization Machine\n",
    "\n",
    "This notebook series demonstrate how to implement the Alternating Least Squares (ALS) and Gibbs Sampler for the Factorization Machine (FM) using Python and Julia -- with a view to various applications, in particular to Thompson sampling (TS).\n",
    "\n",
    "FMs are a type of supervised learning model that can be used for a range of prediction tasks, such as regression, classification, and ranking. They map real-valued features into a low-dimensional latent factor space. This makes them a versatile option for a variety of tasks. ALS is an optimization algorithm for FM, which is used to minimize the loss function of FM. The implementation is based on the paper by Rendle et al. [1, 2].\n",
    "\n",
    "- Written by Keisuke Morita (GSIS Tohoku Univ.)\n",
    "- References:\n",
    "  1. S. Rendle, Z. Gantner, C. Freudenthaler, and L. Schmidt-Thieme, Fast Context-Aware Recommendations with Factorization Machines, in Proceedings of the 34th International ACM SIGIR Conference on Research and Development in Information Retrieval (Association for Computing Machinery, New York, NY, USA, 2011), pp. 635â€“644.\n",
    "  2. S. Rendle, Factorization Machines with libFM, ACM Trans. Intell. Syst. Technol. 3, 57:1 (2012)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01. Factorization Machine as a Linear Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import Optional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Factorization Machine\n",
    "\n",
    "A factorization machine (FM) is given by\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\n",
    "f(x; b, w, v)\n",
    "&= b + \\sum_{i=1}^N w_i x_i + \\sum_{i \\lt j} \\sum_{k=1}^K v_{ik} v_{jk} x_i x_j \\\\\n",
    "\n",
    "&= b + \\sum_{i=1}^N w_i x_i + \\frac{1}{2} \\sum_{k=1}^K \\left( \\sum_{i=1}^N \\sum_{j=1}^N v_{ik} v_{jk} x_i x_j - \\sum_{i=1}^N v_{ik}^2 x_i^2 \\right).\n",
    "\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FactorizationMachines:\n",
    "    def __init__(self,\n",
    "        num_features: int,\n",
    "        num_factors:  int,\n",
    "        sigma_b_init: float=0.,\n",
    "        sigma_w_init: float=1.,\n",
    "        sigma_v_init: float=1.,\n",
    "        seed: Optional[int]=None\n",
    "    ) -> None:\n",
    "        self.rng = np.random.default_rng(seed)\n",
    "        b = self.rng.normal(0, sigma_b_init)\n",
    "        w = self.rng.normal(0, sigma_w_init, num_features)\n",
    "        v = self.rng.normal(0, sigma_v_init, (num_features, num_factors))\n",
    "        self.params = {'b': b, 'w': w, 'v': v}\n",
    "\n",
    "    def predict(self, x: np.ndarray) -> float:\n",
    "        if x.ndim == 1:\n",
    "            x = x.reshape(1, -1) # x: (d, n)\n",
    "        b = self.params['b']     # b: (1)\n",
    "        w = self.params['w']     # w: (d)\n",
    "        v = self.params['v']     # v: (d, k)\n",
    "\n",
    "        bias   = b\n",
    "            # (1)\n",
    "        linear = x[:, :] @ w[:]\n",
    "            # (D, N) @ (N) = (D)\n",
    "        inter  = 0.5 * np.sum((x[:, :] @ v[:, :]) ** 2 - (x[:, :] ** 2) @ (v[:, :] ** 2), axis=1)\n",
    "            # (D, K) -> (D)\n",
    "\n",
    "        result = bias + linear + inter\n",
    "            # (D)\n",
    "\n",
    "        if result.shape[0] == 1:\n",
    "            return float(result[0])\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-13.85583253034785\n",
      "(5,)\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "\n",
    "N = 100\n",
    "K = 10\n",
    "D = 5\n",
    "\n",
    "fm = FactorizationMachines(N, K)\n",
    "\n",
    "# for x: (N,)\n",
    "x = np.random.randn(N)\n",
    "y = fm.predict(x)\n",
    "print(y)       # float\n",
    "\n",
    "# for x: (D, N)\n",
    "x = np.random.choice((0, 1), size=(D, N))\n",
    "y = fm.predict(x)\n",
    "print(y.shape) # (5,)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rewriting an FM as a linear function\n",
    "\n",
    "- **Lemma** [Rendle+ (2011)]\n",
    "\n",
    "    A factorization machine is a linear function with respect to every single model parameter $\\theta \\in \\Theta = \\{b, w_i, v_{ik}\\}$.\n",
    "\n",
    "    $$\n",
    "    \\begin{aligned}\n",
    "    f(x) = \\theta h_\\theta(x) + g_\\theta(x)\n",
    "    \\end{aligned}\n",
    "    $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Differentiating $f$ by $\\theta \\in \\Theta$ yields\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial f}{\\partial \\theta}\n",
    "&=\n",
    "\\left\\{\\begin{aligned}\n",
    "& 1,\n",
    "&& \\theta=b \\\\\n",
    "& x_i,\n",
    "&& \\theta=w_i\\\\\n",
    "& x_i \\left( \\sum_{j=1}^N v_{jk} x_j - v_{ik} x_i \\right).\n",
    "&& \\theta = v_{ik}\n",
    "\\end{aligned}\\right.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "By defining $h_\\theta(x) \\coloneqq \\frac{\\partial f}{\\partial \\theta}$ and $g_\\theta(x) \\coloneqq f(x) - \\theta h_\\theta(x)$ we obtain\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "f(x) = \\theta h_\\theta(x) + g_\\theta(x).\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Remark that the value of $h_\\theta(x)$ and $g_\\theta(x)$ is independent of the value of $\\theta$, that is,\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial g_\\theta(x)}{\\partial \\theta}\n",
    "=\n",
    "\\frac{\\partial h_\\theta(x)}{\\partial \\theta}\n",
    "=\n",
    "0.\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_h_b(x: np.ndarray) -> np.ndarray:\n",
    "    return np.ones(x.shape[0])\n",
    "\n",
    "def calc_h_w(x: np.ndarray, i: int) -> np.ndarray:\n",
    "    return x[:, i]\n",
    "\n",
    "def calc_h_v(x: np.ndarray, v: np.ndarray, i: int, k: int) -> np.ndarray:\n",
    "    return x[:, i] * (x[:, :] @ v[:, k] - x[:, i] * v[i, k])\n",
    "\n",
    "def calc_g(f: np.ndarray, h: np.ndarray, p: float) -> np.ndarray:\n",
    "    return f - h * p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm that the values of g and h are independent of parameter p.\n",
    "\n",
    "# for b\n",
    "h_1 = calc_h_b(x)\n",
    "g_1 = calc_g(fm.predict(x), h_1, fm.params['b'])\n",
    "\n",
    "fm.params['b'] = fm.params['b'] - 100\n",
    "\n",
    "h_2 = calc_h_b(x)\n",
    "g_2 = calc_g(fm.predict(x), h_2, fm.params['b'])\n",
    "\n",
    "fm.params['b'] = fm.params['b'] + 100\n",
    "\n",
    "assert np.isclose(h_1, h_2).all()\n",
    "assert np.isclose(g_1, g_2).all()\n",
    "\n",
    "# for w\n",
    "for i in range(N):\n",
    "    h_1 = calc_h_w(x, i)\n",
    "    g_1 = calc_g(fm.predict(x), h_1, fm.params['w'][i])\n",
    "\n",
    "    fm.params['w'][i] = fm.params['w'][i] - 100\n",
    "\n",
    "    h_2 = calc_h_w(x, i)\n",
    "    g_2 = calc_g(fm.predict(x), h_2, fm.params['w'][i])\n",
    "\n",
    "    fm.params['w'][i] = fm.params['w'][i] + 100\n",
    "\n",
    "    assert np.isclose(h_1, h_2).all()\n",
    "    assert np.isclose(g_1, g_2).all()\n",
    "\n",
    "\n",
    "# for v\n",
    "from itertools import product\n",
    "for i, k in product(range(N), range(K)):\n",
    "    h_1 = calc_h_v(x, fm.params['v'], i, k)\n",
    "    f_1 = fm.predict(x)\n",
    "    g_1 = calc_g(f_1, h_1, fm.params['v'][i, k])\n",
    "\n",
    "    fm.params['v'][i, k] = fm.params['v'][i, k] - 100\n",
    "\n",
    "    h_2 = calc_h_v(x, fm.params['v'], i, k)\n",
    "    f_2 = fm.predict(x)\n",
    "    g_2 = calc_g(f_2, h_2, fm.params['v'][i, k])\n",
    "\n",
    "    fm.params['v'][i, k] = fm.params['v'][i, k] + 100\n",
    "\n",
    "    assert np.isclose(h_1, h_2).all()\n",
    "    assert np.isclose(g_1, g_2).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using FM as a regression model, we assume that the target variable $y$ is given by"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "y^{(d)}\n",
    "&= f(x^{(d)}; b, w, v) + \\varepsilon^{(d)} \\\\\n",
    "&= \\theta h_\\theta(x^{(d)}) + g_\\theta(x^{(d)}) + \\varepsilon^{(d)},\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where\n",
    "\n",
    "- $y^{(d)}$ is the target variable,\n",
    "- $x^{(d)}$ is the feature vector,\n",
    "- $b, w, v$ are the model parameters,\n",
    "- $\\varepsilon^{(d)}$ is the error term,\n",
    "- $d=1,\\ldots,D$ is the index of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make more clear, we can rewrite the FM as"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "y_\\theta^{(d)} = \\theta x_\\theta^{(d)} + \\varepsilon^{(d)},\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "x_\\theta^{(d)} &\\coloneqq \\frac{\\partial f(x^{(d)})}{\\partial \\theta}, \\\\\n",
    "y_\\theta^{(d)} &\\coloneqq y^{(d)} - g_\\theta(x^{(d)}) = y^{(d)} - (f^{(d)} - \\theta x_\\theta^{(d)}).\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These notations are useful for analyzing the FM as a linear model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Speeding up the inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using sampling method like MCMC for learning, $x_\\theta^{(d)}$ and $y_\\theta^{(d)}$ have to be evaluated so many times, which incurs a significant computational cost. Here we show how to reduce this complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make explicit that $f(x^{(d)})$ is a *value*, not a *function*, we denote it by $f^{(d)} \\coloneqq f(x^{(d)})$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main idea is to calculate the new $f^{(d)}$ based on its previous value. We save the value of $f^{(d)}$, and every time we sample $\\theta^{\\rm new}$, we update $f^{(d)}$ using $f^{(d)\\rm new} = f^{(d)} + \\Delta f^{(d)}$, where"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "\\Delta f^{(d)\\rm new}\n",
    "\n",
    "&\\coloneqq\n",
    "f^{(d)\\rm new} - f^{(d)} \\\\\n",
    "\n",
    "&=\n",
    "(\\theta^{\\rm new} - \\theta) \\frac{\\partial f(x^{(d)})}{\\partial \\theta} \\\\\n",
    "\n",
    "&=\n",
    "(\\theta^{\\rm new} - \\theta) x_\\theta^{(d)}.\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_df(\n",
    "    x_theta: np.ndarray,\n",
    "    param_new: float,\n",
    "    param_old: float,\n",
    "):\n",
    "    return (param_new - param_old) * x_theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since $x_\\theta^{(d)}$ is given by"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "x_\\theta^{(d)}\n",
    "&=\n",
    "\\left\\{\\begin{aligned}\n",
    "& 1,\n",
    "&& \\theta=b \\\\\n",
    "& x_i^{(d)},\n",
    "&& \\theta=w_i\\\\\n",
    "& x_i^{(d)} \\left( \\sum_{j=1}^N v_{jk} x_j^{(d)} - v_{ik} x_i^{(d)} \\right),\n",
    "&& \\theta = v_{ik}\n",
    "\\end{aligned}\\right.\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the computational cost of $x_\\theta^{(d)}$ is $\\mathcal O(1)$ for $\\theta = b, w_i$ but for $\\theta = v_{ik}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To reduce the complexity of $x_{v_{ik}}^{(d)}$, we precalculate the value of $q_k^{(d)}$, which is given by"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "q_k^{(d)} \\coloneqq \\sum_{j=1}^N v_{jk} x_j^{(d)},\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and then we can calculate $x_{v_{ik}}^{(d)}$ by"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "x_{v_{ik}}^{(d)} = x_i^{(d)} (q_k^{(d)} - v_{ik} x_i^{(d)})\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in constant time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$q_k^{(d)}$ can be updated using $q_k^{(d) \\rm new} = q_k^{(d)} + \\Delta q_k^{(d)}$, where"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "\\Delta q_k^{(d)}\n",
    "&\\coloneqq q_k^{(d) \\rm new} - q_k^{(d)} \\\\\n",
    "&= x_i^{(d)} (v_{ik}^{\\rm new} - v_{ik}),\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "whose time complexity is also $\\mathcal O(1)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_q_init(\n",
    "    x: np.ndarray,\n",
    "    v: np.ndarray\n",
    ") -> np.ndarray:\n",
    "    # x: (D, N)\n",
    "    # v: (N, K)\n",
    "    return x[:, :] @ v[:, :] # (D, K)\n",
    "\n",
    "def calc_dq(\n",
    "    i: int,\n",
    "    x: np.ndarray,\n",
    "    v_ik_new: float,\n",
    "    v_ik_old: np.ndarray,\n",
    ") -> np.ndarray:\n",
    "    # v_ik_new: float\n",
    "    # v: (N, K)\n",
    "    # x: (D, N)\n",
    "    return (v_ik_new - v_ik_old) * x[:, i] # (D)\n",
    "\n",
    "def calc_xy_b(\n",
    "    f: np.ndarray,\n",
    "    b: float,\n",
    "    x_data: np.ndarray,\n",
    "    y_data: np.ndarray,\n",
    "):\n",
    "    # x_data: (D, N)\n",
    "    # y_data: (D)\n",
    "    x_b = np.ones(x_data.shape[0])\n",
    "    y_b = y_data - (f - b * x_b)\n",
    "    return x_b, y_b\n",
    "\n",
    "def calc_xy_w(\n",
    "    f: np.ndarray,\n",
    "    w: np.ndarray,\n",
    "    x_data: np.ndarray,\n",
    "    y_data: np.ndarray,\n",
    "    i: int\n",
    "):\n",
    "    # x_data: (D, N)\n",
    "    # y_data: (D)\n",
    "    x_w = x_data[:, i]\n",
    "    y_w = y_data - (f - x_w * w[i])\n",
    "    return x_w, y_w\n",
    "\n",
    "def calc_xy_v_naive(\n",
    "    f: np.ndarray,\n",
    "    v: np.ndarray,\n",
    "    x_data: np.ndarray,\n",
    "    y_data: np.ndarray,\n",
    "    i: int,\n",
    "    k: int\n",
    "):\n",
    "    # x_data: (D, N)\n",
    "    # y_data: (D)\n",
    "    # v: (N, K)\n",
    "    x_v = x_data[:, i] * (x_data[:, :] @ v[:, k] - x_data[:, i] * v[i, k])\n",
    "    y_v = y_data - (f - x_v * v[i, k])\n",
    "    return x_v, y_v\n",
    "\n",
    "def calc_xy_v(\n",
    "    f: np.ndarray,\n",
    "    q: np.ndarray,\n",
    "    v: np.ndarray,\n",
    "    x_data: np.ndarray,\n",
    "    y_data: np.ndarray,\n",
    "    i: int,\n",
    "    k: int\n",
    "):\n",
    "    # x_data: (D, N)\n",
    "    # y_data: (D)\n",
    "    x_v = x_data[:, i] * (q[:, k] - x_data[:, i] * v[i, k])\n",
    "    y_v = y_data - (f - x_v * v[i, k])\n",
    "    return x_v, y_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "naive: 3.56743 [s]\n",
      "fast:  0.14770 [s]\n",
      "----------------------------------\n",
      "Both returned the same value: True\n"
     ]
    }
   ],
   "source": [
    "# check how much faster the fast version is\n",
    "import time\n",
    "\n",
    "def naive_updates(iter, fm, x_data, y_data):\n",
    "    f = fm.predict(x_data)\n",
    "    v = fm.params['v']\n",
    "    for _ in range(iter):\n",
    "        x, y = calc_xy_v_naive(f, v, x_data, y_data, 0, 0)\n",
    "        v[0, 0] -= 100\n",
    "    return x\n",
    "\n",
    "def fast_updates(iter, fm, x_data, y_data):\n",
    "    v = fm.params['v']\n",
    "    f = fm.predict(x_data)\n",
    "    q = calc_q_init(x_data, v)\n",
    "    for _ in range(iter):\n",
    "        x, y = calc_xy_v(f, q, v, x_data, y_data, 0, 0)\n",
    "        v_ik_new = v[0, 0] - 100\n",
    "        q[:, 0] += calc_dq(0, x_data, v_ik_new, v[0, 0])\n",
    "        v[0, 0] = v_ik_new\n",
    "    return x\n",
    "\n",
    "iter = 1000\n",
    "\n",
    "seed = 0\n",
    "rng  = np.random.default_rng(seed)\n",
    "\n",
    "N = 100\n",
    "K = 10\n",
    "D = 10000\n",
    "x_data = rng.choice((0, 1), size=(D, N))\n",
    "y_data = rng.normal(size=(D))\n",
    "\n",
    "fm = FactorizationMachines(N, K, seed=seed)\n",
    "\n",
    "time_start = time.time()\n",
    "h_naive    = naive_updates(iter, fm, x_data, y_data)\n",
    "time_end   = time.time()\n",
    "time_naive = time_end - time_start\n",
    "\n",
    "fm = FactorizationMachines(N, K, seed=seed)\n",
    "\n",
    "time_start = time.time()\n",
    "h_fast     = fast_updates(iter, fm, x_data, y_data)\n",
    "time_end   = time.time()\n",
    "time_fast  = time_end - time_start\n",
    "\n",
    "print(f'naive: {time_naive:.5f} [s]')\n",
    "print(f'fast:  {time_fast:.5f} [s]')\n",
    "print('-'*34)\n",
    "print(f'Both returned the same value: {np.isclose(h_fast, h_naive).all()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we obtain the whole update rule as follows:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "q_k^{(d)}\n",
    "&=\n",
    "\\sum_{j=1}^N v_{jk} x_j^{(d)},\n",
    "\\\\\n",
    "\n",
    "\\Delta f^{(d)}\n",
    "&= (\\theta^{\\rm new} - \\theta) x_\\theta^{(d)},\n",
    "\\\\\n",
    "\n",
    "\\Delta q_k^{(d)}\n",
    "&= x_i^{(d)} (v_{ik}^{\\rm new} - v_{ik}),\n",
    "\\\\\n",
    "\n",
    "x_\\theta^{(d)}\n",
    "&=\n",
    "\\left\\{\\begin{aligned}\n",
    "& 1,\n",
    "&& \\theta=b \\\\\n",
    "& x_i^{(d)},\n",
    "&& \\theta=w_i\\\\\n",
    "& x_i^{(d)} \\left( q_k^{(d)} - v_{ik} x_i^{(d)} \\right).\n",
    "&& \\theta = v_{ik}\n",
    "\\end{aligned}\\right.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "The time complexity is $\\mathcal O(1)$ for every $\\theta$."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
