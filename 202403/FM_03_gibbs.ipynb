{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03. Gibbs Sampler for Factorization Machine\n",
    "\n",
    "$\\DeclareMathOperator*{\\argmin}{arg~min}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import Optional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From ALS to Gibbs Sampler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results of ordinary least squares are equivalent to the maximum a posteriori (MAP) estimation of a linear regression model with normally distributed errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is, when the likelihood of $\\theta$ is given by"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "p(y^{(d)} | \\theta, x^{(d)}) &= \\mathcal{N}(y^{(d)} | \\theta x^{(d)}, \\sigma_n^2),\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $\\sigma_n^2$ is the variance of the noise and its prior is uniform, the posterior distribution of $\\theta$ is given by"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "p(\\theta | y, x)\n",
    "&\\propto\n",
    "p(y | \\theta, x) p(\\theta)\n",
    "\\\\\n",
    "\n",
    "&\\propto\n",
    "\\prod_{d=1}^D \\mathcal{N}(y^{(d)} | \\theta x^{(d)}, \\sigma_n^2) \\times 1\n",
    "\\\\\n",
    "\n",
    "&\\propto\n",
    "\\exp\\left(-\\frac{1}{2\\sigma_n^2} \\sum_{d=1}^D (y^{(d)} - \\theta x^{(d)})^2\\right)\n",
    "\\\\\n",
    "\n",
    "&=\n",
    "\\exp\\left(-\\frac{1}{2\\sigma_n^2} \\sum_{d=1}^D x^{(d)2} \\left( \\theta - \\left( \\sum_{d=1}^D x^{(d)2} \\right)^{-1} \\sum_{d=1}^D x^{(d)}y^{(d)} \\right)^2 \\right)\n",
    "\\\\\n",
    "\n",
    "&\\propto\n",
    "\\mathcal{N}\\left(\\theta \\middle| \\mu_\\theta^\\star, \\sigma_\\theta^{\\star 2} \\right),\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "\\mu_\\theta^\\star\n",
    "&=\n",
    "\\left( \\sum_{d=1}^D x^{(d)2} \\right)^{-1} \\sum_{d=1}^D x^{(d)}y^{(d)},\n",
    "\\\\\n",
    "\n",
    "\\sigma_\\theta^{\\star 2}\n",
    "&=\n",
    "\\sigma_n^2 \\left( \\sum_{d=1}^D x^{(d)2} \\right)^{-1}.\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviously, its MAP estimation, a.k.a. maximum likelihood estimation (MLE) --since it is equivalent to maximizing the likelihood function--,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\hat \\theta = \\mu_\\theta^\\star\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "is equivalent to the ordinary least squares solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the similar way, the L2 regularized least squares is equivalent to the MAP estimation of a linear regression model with"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- normal likelihood $p(y^{(d)} | \\theta, x^{(d)}) \\sim \\mathcal{N}(y^{(d)} | \\theta x^{(d)}, \\sigma_n^2)$ and\n",
    "- a Gaussian prior $\\theta \\sim \\mathcal N(\\theta | \\mu_\\theta, \\sigma_\\theta^2)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What then about sampling from the posterior distribution instead of performing MAP estimation for each parameter $\\theta$ at each iteration in the ALS?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we are drawing samples from the posterior distribution of $\\theta$ given $x, y$ and the other parameters $\\Theta \\setminus \\{ \\theta \\}$, that is,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "\\tilde \\theta | x, y, \\Theta \\setminus \\{ \\theta \\} \\sim p(\\theta | x, y, \\Theta \\setminus \\{ \\theta \\}) \\quad \\forall \\theta \\in \\Theta.\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen from its expression, this posterior sampling operation is equivalent to performing **Gibbs sampling**, a type of MCMC method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Factorization Machine as Hierarchical Bayesian Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following [Rendle+ (2012)], we build a hierarchical Bayesian model for FM, which is known as the **Bayesian factorization machine** (BFM)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hiearchical expression of the BFM is given by the graphical model below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](figures/FM_03_hierarchical.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $\\mathcal D = \\{ (x^{(d)}, y^{(d)}) \\}_{d=1, \\dots, D}$ (denoted by $x, y$) is the given dataset,\n",
    "- $\\Theta_H = \\{ \\mu_b, \\sigma_b^2, m_\\theta, \\lambda_\\theta, a_\\theta, b_\\theta, a_n, b_n \\}$ are given hyperparameters, and\n",
    "- $\\Theta = \\{ b, w, v, \\sigma_n^2 \\}$ are the parameters to be tuned through the training process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distributions of the parameters and outputs are given by"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "p(\\{ y_\\theta^{(d)} \\} | \\{ x_\\theta^{(d)} \\}, \\Theta, \\sigma_n^2)\n",
    "&= \\prod_{d=1}^D \\mathcal{N}(y_\\theta^{(d)} | \\theta x_\\theta^{(d)}, \\sigma_n^2),\n",
    "&& \\theta \\in \\{ b, w, v \\}\n",
    "\\\\\n",
    "\n",
    "p(\\sigma_n^2 | a_n, b_n)\n",
    "&= \\mathcal{IG}(\\sigma_n^2 | a_n, b_n),\n",
    "\\\\\n",
    "\n",
    "p(\\theta | \\mu_\\theta, \\sigma_\\theta^2)\n",
    "&= \\mathcal{N}(\\theta | \\mu_\\theta, \\sigma_\\theta^2),\n",
    "&& \\theta \\in \\{ b, w, v \\}\n",
    "\\\\\n",
    "\n",
    "p(\\mu_\\theta | m_\\theta, \\lambda_\\theta^2, \\sigma_\\theta^2)\n",
    "&\\sim \\mathcal{N}(\\mu_\\theta | m_\\theta, \\sigma_\\theta^2 / \\lambda_\\theta^2),\n",
    "&& \\theta \\in \\{ w, v \\}\n",
    "\\\\\n",
    "\n",
    "p(\\sigma_\\theta^2 | a_\\theta, b_\\theta) &\\sim \\mathcal{IG}(\\sigma_w^2 | a_\\theta, b_\\theta).\n",
    "&& \\theta \\in \\{ w, v \\}\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bayes' theorem states that the posterior distribution is proportional to the product of the prior distribution and likelihood for each parameter, that is,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "p(a | b, x) &\\propto p(b | a, x) p(b | x).\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The conjugacy of the assumed priors allows the posterior distribution to be calculated analytically by keeping only the relevant parameter term and discarding the rest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting posterior distributions are"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "p(\\sigma_n^2 | \\mathcal D, \\Theta, \\Theta_H)\n",
    "&= \\mathcal{IG}(\\sigma_n^2 | a_n^\\star, b_n^\\star),\n",
    "\\\\\n",
    "\n",
    "p(\\theta | \\mathcal D, \\Theta \\setminus \\{ \\theta \\}, \\Theta_H)\n",
    "&= \\mathcal{N}(\\theta | \\mu_\\theta^\\star, \\sigma_\\theta^{\\star 2}),\n",
    "\\\\\n",
    "\n",
    "p(\\mu_\\theta | \\Theta \\setminus \\{ \\mu_\\theta \\}, \\Theta_H)\n",
    "&= \\mathcal{N}(\\mu_\\theta | \\mu_\\mu^\\star, \\sigma_\\theta^2 / \\lambda_\\mu^{\\star 2}),\n",
    "\\\\\n",
    "\n",
    "p(\\sigma_\\theta^2 | \\Theta \\setminus \\{ \\sigma_\\theta^2 \\}, \\Theta_H)\n",
    "&= \\mathcal{IG}(\\sigma_\\theta^2 | a_\\theta^\\star, b_\\theta^\\star),\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "a_n^\\star\n",
    "&= a_n + \\frac{D}{2},\n",
    "\\\\\n",
    "\n",
    "b_n^\\star\n",
    "&= b_n + \\frac{1}{2} \\sum_{d=1}^D (y_\\theta^{(d)} - x_\\theta^{(d)} \\theta )^2,\n",
    "\\\\\n",
    "\n",
    "\\mu_\\theta^\\star\n",
    "&= \\sigma_\\theta^{\\star 2}\n",
    "\\left( \\frac{1}{\\sigma_n^2} \\sum_{d=1}^D x_\\theta^{(d)} y_\\theta^{(d)} + \\frac{1}{\\sigma_\\theta^2} \\mu_\\theta \\right),\n",
    "&& \\theta \\in \\{ b, w, v \\},\n",
    "\\\\\n",
    "\n",
    "\\sigma_\\theta^{\\star 2}\n",
    "&= \\left( \\frac{1}{\\sigma_n^2} \\sum_{d=1}^D x_\\theta^{(d)2} + \\frac{1}{\\sigma_\\theta^2} \\right)^{-1},\n",
    "&& \\theta \\in \\{ b, w, v \\}\n",
    "\\\\\n",
    "\n",
    "m_\\theta^\\star\n",
    "&= \\frac{ 1 }{ \\lambda_\\theta^\\star }\n",
    "\\left( \\sum_{i=1}^N \\theta_i + \\lambda_\\theta m_\\theta \\right),\n",
    "&& \\theta \\in \\{ w, v \\}\n",
    "\\\\\n",
    "\n",
    "\\lambda_\\theta^\\star\n",
    "&= N + \\lambda_\\theta,\n",
    "&& \\theta \\in \\{ w, v \\}\n",
    "\\\\\n",
    "\n",
    "a_\\theta^\\star &=\n",
    "a_\\theta + \\frac{N + 1}{2},\n",
    "&& \\theta \\in \\{ w, v \\}\n",
    "\\\\\n",
    "\n",
    "b_\\theta^\\star &=\n",
    "a_\\theta + \\frac{1}{2} \\left( \\sum_{i=1}^N (\\theta_i - \\mu_\\theta)^2 + \\lambda_\\theta(\\mu_\\theta - m_\\theta)^2 \\right).\n",
    "&& \\theta \\in \\{ w, v \\}\n",
    "\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Substituting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "x_\\theta^{(d)} &\\coloneqq h_\\theta^{(d)}, \\\\\n",
    "y_\\theta^{(d)} &\\coloneqq y^{(d)} - g_\\theta^{(d)}\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "into the above distributions gives the Gibbs sampler algorithm for the BFM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in the ALS case, we can speed up the algorithm by pre-computing and retaining the values of $f^{(d)}$ and $q^{(d)}_k$ which are given by"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "f^{(d)} &\\coloneqq f(x^{(d)}),\n",
    "\\\\\n",
    "q^{(d)}_k &\\coloneqq \\sum_{j=1}^D v_{jk} x^{(d)}_j,\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and updating them sequentially in the following rule:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "\\Delta f^{(d)}\n",
    "&= (\\theta^{\\rm new} - \\theta) h_\\theta^{(d)},\n",
    "\\\\\n",
    "\n",
    "\\Delta q_k^{(d)}\n",
    "&= x_i^{(d)} (v_{ik}^{\\rm new} - v_{ik}),\n",
    "\\\\\n",
    "\n",
    "h_\\theta^{(d)}\n",
    "&=\n",
    "\\left\\{\\begin{aligned}\n",
    "& 1,\n",
    "&& \\theta=b \\\\\n",
    "& x_i^{(d)},\n",
    "&& \\theta=w_i\\\\\n",
    "& x_i^{(d)} \\left( q_k^{(d)} - v_{ik} x_i^{(d)} \\right).\n",
    "&& \\theta = v_{ik}\n",
    "\\end{aligned}\\right.\n",
    "\\end{aligned}\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
